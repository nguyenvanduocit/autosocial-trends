---
source: trendshift
title: HKUDS/RAG-Anything
url: https://github.com/HKUDS/RAG-Anything
date: 2025-12-29
---

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FHKUDS%2FRAG-Anything)

Appearance settings

* Platform

  + AI CODE CREATION
    - [GitHub CopilotWrite better code with AI](https://github.com/features/copilot)
    - [GitHub SparkBuild and deploy intelligent apps](https://github.com/features/spark)
    - [GitHub ModelsManage and compare prompts](https://github.com/features/models)
    - [MCP RegistryNewIntegrate external tools](https://github.com/mcp)
  + DEVELOPER WORKFLOWS
    - [ActionsAutomate any workflow](https://github.com/features/actions)
    - [CodespacesInstant dev environments](https://github.com/features/codespaces)
    - [IssuesPlan and track work](https://github.com/features/issues)
    - [Code ReviewManage code changes](https://github.com/features/code-review)
  + APPLICATION SECURITY
    - [GitHub Advanced SecurityFind and fix vulnerabilities](https://github.com/security/advanced-security)
    - [Code securitySecure your code as you build](https://github.com/security/advanced-security/code-security)
    - [Secret protectionStop leaks before they start](https://github.com/security/advanced-security/secret-protection)
  + EXPLORE
    - [Why GitHub](https://github.com/why-github)
    - [Documentation](https://docs.github.com)
    - [Blog](https://github.blog)
    - [Changelog](https://github.blog/changelog)
    - [Marketplace](https://github.com/marketplace)

  [View all features](https://github.com/features)
* Solutions

  + BY COMPANY SIZE
    - [Enterprises](https://github.com/enterprise)
    - [Small and medium teams](https://github.com/team)
    - [Startups](https://github.com/enterprise/startups)
    - [Nonprofits](https://github.com/solutions/industry/nonprofits)
  + BY USE CASE
    - [App Modernization](https://github.com/solutions/use-case/app-modernization)
    - [DevSecOps](https://github.com/solutions/use-case/devsecops)
    - [DevOps](https://github.com/solutions/use-case/devops)
    - [CI/CD](https://github.com/solutions/use-case/ci-cd)
    - [View all use cases](https://github.com/solutions/use-case)
  + BY INDUSTRY
    - [Healthcare](https://github.com/solutions/industry/healthcare)
    - [Financial services](https://github.com/solutions/industry/financial-services)
    - [Manufacturing](https://github.com/solutions/industry/manufacturing)
    - [Government](https://github.com/solutions/industry/government)
    - [View all industries](https://github.com/solutions/industry)

  [View all solutions](https://github.com/solutions)
* Resources

  + EXPLORE BY TOPIC
    - [AI](https://github.com/resources/articles?topic=ai)
    - [Software Development](https://github.com/resources/articles?topic=software-development)
    - [DevOps](https://github.com/resources/articles?topic=devops)
    - [Security](https://github.com/resources/articles?topic=security)
    - [View all topics](https://github.com/resources/articles)
  + EXPLORE BY TYPE
    - [Customer stories](https://github.com/customer-stories)
    - [Events & webinars](https://github.com/resources/events)
    - [Ebooks & reports](https://github.com/resources/whitepapers)
    - [Business insights](https://github.com/solutions/executive-insights)
    - [GitHub Skills](https://skills.github.com)
  + SUPPORT & SERVICES
    - [Documentation](https://docs.github.com)
    - [Customer support](https://support.github.com)
    - [Community forum](https://github.com/orgs/community/discussions)
    - [Trust center](https://github.com/trust-center)
    - [Partners](https://github.com/partners)
* Open Source

  + COMMUNITY
    - [GitHub SponsorsFund open source developers](https://github.com/sponsors)
  + PROGRAMS
    - [Security Lab](https://securitylab.github.com)
    - [Maintainer Community](https://maintainers.github.com)
    - [Accelerator](https://github.com/accelerator)
    - [Archive Program](https://archiveprogram.github.com)
  + REPOSITORIES
    - [Topics](https://github.com/topics)
    - [Trending](https://github.com/trending)
    - [Collections](https://github.com/collections)
* Enterprise

  + ENTERPRISE SOLUTIONS
    - [Enterprise platformAI-powered developer platform](https://github.com/enterprise)
  + AVAILABLE ADD-ONS
    - [GitHub Advanced SecurityEnterprise-grade security features](https://github.com/security/advanced-security)
    - [Copilot for BusinessEnterprise-grade AI features](https://github.com/features/copilot/copilot-business)
    - [Premium SupportEnterprise-grade 24/7 support](https://github.com/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

[ ]
Include my email address so I can be contacted

Cancel
 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Cancel
 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FHKUDS%2FRAG-Anything)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=HKUDS%2FRAG-Anything)

Appearance settings

Resetting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[HKUDS](/HKUDS)
/
**[RAG-Anything](/HKUDS/RAG-Anything)**
Public

* [Notifications](/login?return_to=%2FHKUDS%2FRAG-Anything) You must be signed in to change notification settings
* [Fork
  1.4k](/login?return_to=%2FHKUDS%2FRAG-Anything)
* [Star
   11.6k](/login?return_to=%2FHKUDS%2FRAG-Anything)

"RAG-Anything: All-in-One RAG Framework"

[arxiv.org/abs/2510.12323](http://arxiv.org/abs/2510.12323 "http://arxiv.org/abs/2510.12323")

### License

[MIT license](/HKUDS/RAG-Anything/blob/main/LICENSE)

[11.6k
stars](/HKUDS/RAG-Anything/stargazers) [1.4k
forks](/HKUDS/RAG-Anything/forks) [Branches](/HKUDS/RAG-Anything/branches) [Tags](/HKUDS/RAG-Anything/tags) [Activity](/HKUDS/RAG-Anything/activity)

[Star](/login?return_to=%2FHKUDS%2FRAG-Anything)

[Notifications](/login?return_to=%2FHKUDS%2FRAG-Anything) You must be signed in to change notification settings

* [Code](/HKUDS/RAG-Anything)
* [Issues
  94](/HKUDS/RAG-Anything/issues)
* [Pull requests
  3](/HKUDS/RAG-Anything/pulls)
* [Discussions](/HKUDS/RAG-Anything/discussions)
* [Actions](/HKUDS/RAG-Anything/actions)
* [Projects
  0](/HKUDS/RAG-Anything/projects)
* [Security

  ### Uh oh!

  There was an error while loading. Please reload this page.](/HKUDS/RAG-Anything/security)
* [Insights](/HKUDS/RAG-Anything/pulse)

Additional navigation options

* [Code](/HKUDS/RAG-Anything)
* [Issues](/HKUDS/RAG-Anything/issues)
* [Pull requests](/HKUDS/RAG-Anything/pulls)
* [Discussions](/HKUDS/RAG-Anything/discussions)
* [Actions](/HKUDS/RAG-Anything/actions)
* [Projects](/HKUDS/RAG-Anything/projects)
* [Security](/HKUDS/RAG-Anything/security)
* [Insights](/HKUDS/RAG-Anything/pulse)

# HKUDS/RAG-Anything

main

[Branches](/HKUDS/RAG-Anything/branches)[Tags](/HKUDS/RAG-Anything/tags)

Go to file

Code

Open more actions menu

## Folders and files

| Name | | Name | Last commit message | Last commit date |
| --- | --- | --- | --- | --- |
| Latest commit   History[230 Commits](/HKUDS/RAG-Anything/commits/main/) | | |
| [.github](/HKUDS/RAG-Anything/tree/main/.github ".github") | | [.github](/HKUDS/RAG-Anything/tree/main/.github ".github") |  |  |
| [assets](/HKUDS/RAG-Anything/tree/main/assets "assets") | | [assets](/HKUDS/RAG-Anything/tree/main/assets "assets") |  |  |
| [docs](/HKUDS/RAG-Anything/tree/main/docs "docs") | | [docs](/HKUDS/RAG-Anything/tree/main/docs "docs") |  |  |
| [examples](/HKUDS/RAG-Anything/tree/main/examples "examples") | | [examples](/HKUDS/RAG-Anything/tree/main/examples "examples") |  |  |
| [raganything](/HKUDS/RAG-Anything/tree/main/raganything "raganything") | | [raganything](/HKUDS/RAG-Anything/tree/main/raganything "raganything") |  |  |
| [scripts](/HKUDS/RAG-Anything/tree/main/scripts "scripts") | | [scripts](/HKUDS/RAG-Anything/tree/main/scripts "scripts") |  |  |
| [.gitignore](/HKUDS/RAG-Anything/blob/main/.gitignore ".gitignore") | | [.gitignore](/HKUDS/RAG-Anything/blob/main/.gitignore ".gitignore") |  |  |
| [.pre-commit-config.yaml](/HKUDS/RAG-Anything/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml") | | [.pre-commit-config.yaml](/HKUDS/RAG-Anything/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml") |  |  |
| [LICENSE](/HKUDS/RAG-Anything/blob/main/LICENSE "LICENSE") | | [LICENSE](/HKUDS/RAG-Anything/blob/main/LICENSE "LICENSE") |  |  |
| [MANIFEST.in](/HKUDS/RAG-Anything/blob/main/MANIFEST.in "MANIFEST.in") | | [MANIFEST.in](/HKUDS/RAG-Anything/blob/main/MANIFEST.in "MANIFEST.in") |  |  |
| [README.md](/HKUDS/RAG-Anything/blob/main/README.md "README.md") | | [README.md](/HKUDS/RAG-Anything/blob/main/README.md "README.md") |  |  |
| [README\_zh.md](/HKUDS/RAG-Anything/blob/main/README_zh.md "README_zh.md") | | [README\_zh.md](/HKUDS/RAG-Anything/blob/main/README_zh.md "README_zh.md") |  |  |
| [env.example](/HKUDS/RAG-Anything/blob/main/env.example "env.example") | | [env.example](/HKUDS/RAG-Anything/blob/main/env.example "env.example") |  |  |
| [pyproject.toml](/HKUDS/RAG-Anything/blob/main/pyproject.toml "pyproject.toml") | | [pyproject.toml](/HKUDS/RAG-Anything/blob/main/pyproject.toml "pyproject.toml") |  |  |
| [requirements.txt](/HKUDS/RAG-Anything/blob/main/requirements.txt "requirements.txt") | | [requirements.txt](/HKUDS/RAG-Anything/blob/main/requirements.txt "requirements.txt") |  |  |
| [setup.py](/HKUDS/RAG-Anything/blob/main/setup.py "setup.py") | | [setup.py](/HKUDS/RAG-Anything/blob/main/setup.py "setup.py") |  |  |
| View all files | | |

## Repository files navigation

* README
* MIT license

[![RAG-Anything Logo](/HKUDS/RAG-Anything/raw/main/assets/logo.png)](/HKUDS/RAG-Anything/blob/main/assets/logo.png)

# üöÄ RAG-Anything: All-in-One RAG Framework

[![HKUDS%2FRAG-Anything | Trendshift](https://camo.githubusercontent.com/b7d262712df3f0d8c5822a912b14f75d8879ed98333f5dccb8166f1d301a0946/68747470733a2f2f7472656e6473686966742e696f2f6170692f62616467652f7265706f7369746f726965732f3134393539)](https://trendshift.io/repositories/14959)

[![Typing Animation](https://camo.githubusercontent.com/ee0ffc9f8a8b338b4342f708bc183aa91ef92fc20187fe9e9e583d038b52b32d/68747470733a2f2f726561646d652d747970696e672d7376672e6865726f6b756170702e636f6d3f666f6e743d4f72626974726f6e2673697a653d3234266475726174696f6e3d333030302670617573653d3130303026636f6c6f723d3030443946462663656e7465723d74727565267643656e7465723d747275652677696474683d363030266c696e65733d57656c636f6d652b746f2b5241472d416e797468696e673b4e6578742d47656e2b4d756c74696d6f64616c2b5241472b53797374656d3b506f77657265642b62792b416476616e6365642b41492b546563686e6f6c6f6779)](https://camo.githubusercontent.com/ee0ffc9f8a8b338b4342f708bc183aa91ef92fc20187fe9e9e583d038b52b32d/68747470733a2f2f726561646d652d747970696e672d7376672e6865726f6b756170702e636f6d3f666f6e743d4f72626974726f6e2673697a653d3234266475726174696f6e3d333030302670617573653d3130303026636f6c6f723d3030443946462663656e7465723d74727565267643656e7465723d747275652677696474683d363030266c696e65733d57656c636f6d652b746f2b5241472d416e797468696e673b4e6578742d47656e2b4d756c74696d6f64616c2b5241472b53797374656d3b506f77657265642b62792b416476616e6365642b41492b546563686e6f6c6f6779)

[![](https://camo.githubusercontent.com/f54fa1a4f8de2d145e7f754a4ec7fae92ec7ab67f2e4ab7a61e025826a5d7872/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f94a550726f6a6563742d506167652d3030643966663f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)](https://github.com/HKUDS/RAG-Anything)
[![](https://camo.githubusercontent.com/2fcfddd9a3a9e97cd04a4b18be37d15b52c983282593a6313c4d8dcbec3e0ea4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f938461725869762d323531302e31323332332d6666366236623f7374796c653d666f722d7468652d6261646765266c6f676f3d6172786976266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)](https://arxiv.org/abs/2510.12323)
[![](https://camo.githubusercontent.com/0fb76146c37abfb0a7ea0804d47cf492e4543bd72942b9a1eb2251414348e44a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29aa142617365642532306f6e2d4c696768745241472d3465636463343f7374796c653d666f722d7468652d6261646765266c6f676f3d6c696768746e696e67266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)](https://github.com/HKUDS/LightRAG)

[![](https://camo.githubusercontent.com/3256df7110a98bf05ac7cb8250a0161bb3c52397064cbbd809234b29dc24d489/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f484b5544532f5241472d416e797468696e673f636f6c6f723d303064396666267374796c653d666f722d7468652d6261646765266c6f676f3d73746172266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)](https://github.com/HKUDS/RAG-Anything/stargazers)
[![](https://camo.githubusercontent.com/21597bf364600b5cd1ff1c924de84c2b16d9b5ab0be13b18ccc4a70f7c2b7e77/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f908d507974686f6e2d332e31302d3465636463343f7374796c653d666f722d7468652d6261646765266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)](https://camo.githubusercontent.com/21597bf364600b5cd1ff1c924de84c2b16d9b5ab0be13b18ccc4a70f7c2b7e77/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f908d507974686f6e2d332e31302d3465636463343f7374796c653d666f722d7468652d6261646765266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)
[![](https://camo.githubusercontent.com/b3482471e63b9f6ba28d6ce0dc9605c16d991928005dcf6f6b94a5815c181183/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f726167616e797468696e672e7376673f7374796c653d666f722d7468652d6261646765266c6f676f3d70797069266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d31613161326526636f6c6f723d666636623662)](https://pypi.org/project/raganything/)
[![](https://camo.githubusercontent.com/2af9796c1dcf87bc34bcf204f9c3c9d136d903cba129734dde689fc19dfe05e5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe29aa175762d52656164792d6666366236623f7374796c653d666f722d7468652d6261646765266c6f676f3d707974686f6e266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)](https://github.com/astral-sh/uv)

[![](https://camo.githubusercontent.com/997243778fa5a1c573a6946428cbbba930b66978c116930554c897d656cd7402/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac446973636f72642d436f6d6d756e6974792d3732383964613f7374796c653d666f722d7468652d6261646765266c6f676f3d646973636f7264266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)](https://discord.gg/yF2MmDJyGJ)
[![](https://camo.githubusercontent.com/66747ec1eb2241514dc09e4bc8279b3017c2d617dcea15a693db0cca958240d7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac5765436861742d47726f75702d3037633136303f7374796c653d666f722d7468652d6261646765266c6f676f3d776563686174266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)](https://github.com/HKUDS/RAG-Anything/issues/7)

[![](https://camo.githubusercontent.com/0e023c856e19d217d0f619fe7247a59f829e0046a66b0b16dd26820344f8427b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f87a8f09f87b3e4b8ade69687e789882d3161316132653f7374796c653d666f722d7468652d6261646765)](/HKUDS/RAG-Anything/blob/main/README_zh.md)
[![](https://camo.githubusercontent.com/37d611c70fafbf6df48db46cb5a9959452e75e92f6f73ee10bace1a1567bd42a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f87baf09f87b8456e676c6973682d3161316132653f7374796c653d666f722d7468652d6261646765)](/HKUDS/RAG-Anything/blob/main/README.md)

[![](https://camo.githubusercontent.com/5dc13c91fe6858f8373b0aa709fc6505f1716ffc1c7e35050fcdccd6971376f1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f517569636b25323053746172742d476574253230537461727465642532304e6f772d3030643966663f7374796c653d666f722d7468652d6261646765266c6f676f3d726f636b6574266c6f676f436f6c6f723d7768697465266c6162656c436f6c6f723d316131613265)](#-quick-start)

---

## üéâ News

* [x]  [2025.10]üéØüì¢ üöÄ We have released the technical report of [RAG-Anything](http://arxiv.org/abs/2510.12323). Access it now to explore our latest research findings.
* [x]  [2025.08]üéØüì¢ üîç RAG-Anything now features **VLM-Enhanced Query** mode! When documents include images, the system seamlessly integrates them into VLM for advanced multimodal analysis, combining visual and textual context for deeper insights.
* [x]  [2025.07]üéØüì¢ RAG-Anything now features a [context configuration module](/HKUDS/RAG-Anything/blob/main/docs/context_aware_processing.md), enabling intelligent integration of relevant contextual information to enhance multimodal content processing.
* [x]  [2025.07]üéØüì¢ üöÄ RAG-Anything now supports multimodal query capabilities, enabling enhanced RAG with seamless processing of text, images, tables, and equations.
* [x]  [2025.07]üéØüì¢ üéâ RAG-Anything has reached 1küåü stars on GitHub! Thank you for your incredible support and valuable contributions to the project.

---

## üåü System Overview

*Next-Generation Multimodal Intelligence*

Modern documents increasingly contain diverse multimodal content‚Äîtext, images, tables, equations, charts, and multimedia‚Äîthat traditional text-focused RAG systems cannot effectively process. **RAG-Anything** addresses this challenge as a comprehensive **All-in-One Multimodal Document Processing RAG system** built on [LightRAG](https://github.com/HKUDS/LightRAG).

As a unified solution, RAG-Anything **eliminates the need for multiple specialized tools**. It provides **seamless processing and querying across all content modalities** within a single integrated framework. Unlike conventional RAG approaches that struggle with non-textual elements, our all-in-one system delivers **comprehensive multimodal retrieval capabilities**.

Users can query documents containing **interleaved text**, **visual diagrams**, **structured tables**, and **mathematical formulations** through **one cohesive interface**. This consolidated approach makes RAG-Anything particularly valuable for academic research, technical documentation, financial reports, and enterprise knowledge management where rich, mixed-content documents demand a **unified processing framework**.

[![RAG-Anything](/HKUDS/RAG-Anything/raw/main/assets/rag_anything_framework.png)](/HKUDS/RAG-Anything/blob/main/assets/rag_anything_framework.png)

### üéØ Key Features

* **üîÑ End-to-End Multimodal Pipeline** - Complete workflow from document ingestion and parsing to intelligent multimodal query answering
* **üìÑ Universal Document Support** - Seamless processing of PDFs, Office documents, images, and diverse file formats
* **üß† Specialized Content Analysis** - Dedicated processors for images, tables, mathematical equations, and heterogeneous content types
* **üîó Multimodal Knowledge Graph** - Automatic entity extraction and cross-modal relationship discovery for enhanced understanding
* **‚ö° Adaptive Processing Modes** - Flexible MinerU-based parsing or direct multimodal content injection workflows
* **üìã Direct Content List Insertion** - Bypass document parsing by directly inserting pre-parsed content lists from external sources
* **üéØ Hybrid Intelligent Retrieval** - Advanced search capabilities spanning textual and multimodal content with contextual understanding

---

## üèóÔ∏è Algorithm & Architecture

### Core Algorithm

**RAG-Anything** implements an effective **multi-stage multimodal pipeline** that fundamentally extends traditional RAG architectures to seamlessly handle diverse content modalities through intelligent orchestration and cross-modal understanding.

üìÑ

Document Parsing

‚Üí

üß†

Content Analysis

‚Üí

üîç

Knowledge Graph

‚Üí

üéØ

Intelligent Retrieval

### 1. Document Parsing Stage

The system provides high-fidelity document extraction through adaptive content decomposition. It intelligently segments heterogeneous elements while preserving contextual relationships. Universal format compatibility is achieved via specialized optimized parsers.

**Key Components:**

* **‚öôÔ∏è MinerU Integration**: Leverages [MinerU](https://github.com/opendatalab/MinerU) for high-fidelity document structure extraction and semantic preservation across complex layouts.
* **üß© Adaptive Content Decomposition**: Automatically segments documents into coherent text blocks, visual elements, structured tables, mathematical equations, and specialized content types while preserving contextual relationships.
* **üìÅ Universal Format Support**: Provides comprehensive handling of PDFs, Office documents (DOC/DOCX/PPT/PPTX/XLS/XLSX), images, and emerging formats through specialized parsers with format-specific optimization.

### 2. Multi-Modal Content Understanding & Processing

The system automatically categorizes and routes content through optimized channels. It uses concurrent pipelines for parallel text and multimodal processing. Document hierarchy and relationships are preserved during transformation.

**Key Components:**

* **üéØ Autonomous Content Categorization and Routing**: Automatically identify, categorize, and route different content types through optimized execution channels.
* **‚ö° Concurrent Multi-Pipeline Architecture**: Implements concurrent execution of textual and multimodal content through dedicated processing pipelines. This approach maximizes throughput efficiency while preserving content integrity.
* **üèóÔ∏è Document Hierarchy Extraction**: Extracts and preserves original document hierarchy and inter-element relationships during content transformation.

### 3. Multimodal Analysis Engine

The system deploys modality-aware processing units for heterogeneous data modalities:

**Specialized Analyzers:**

* **üîç Visual Content Analyzer**:

  + Integrate vision model for image analysis.
  + Generates context-aware descriptive captions based on visual semantics.
  + Extracts spatial relationships and hierarchical structures between visual elements.
* **üìä Structured Data Interpreter**:

  + Performs systematic interpretation of tabular and structured data formats.
  + Implements statistical pattern recognition algorithms for data trend analysis.
  + Identifies semantic relationships and dependencies across multiple tabular datasets.
* **üìê Mathematical Expression Parser**:

  + Parses complex mathematical expressions and formulas with high accuracy.
  + Provides native LaTeX format support for seamless integration with academic workflows.
  + Establishes conceptual mappings between mathematical equations and domain-specific knowledge bases.
* **üîß Extensible Modality Handler**:

  + Provides configurable processing framework for custom and emerging content types.
  + Enables dynamic integration of new modality processors through plugin architecture.
  + Supports runtime configuration of processing pipelines for specialized use cases.

### 4. Multimodal Knowledge Graph Index

The multi-modal knowledge graph construction module transforms document content into structured semantic representations. It extracts multimodal entities, establishes cross-modal relationships, and preserves hierarchical organization. The system applies weighted relevance scoring for optimized knowledge retrieval.

**Core Functions:**

* **üîç Multi-Modal Entity Extraction**: Transforms significant multimodal elements into structured knowledge graph entities. The process includes semantic annotations and metadata preservation.
* **üîó Cross-Modal Relationship Mapping**: Establishes semantic connections and dependencies between textual entities and multimodal components. This is achieved through automated relationship inference algorithms.
* **üèóÔ∏è Hierarchical Structure Preservation**: Maintains original document organization through "belongs\_to" relationship chains. These chains preserve logical content hierarchy and sectional dependencies.
* **‚öñÔ∏è Weighted Relationship Scoring**: Assigns quantitative relevance scores to relationship types. Scoring is based on semantic proximity and contextual significance within the document structure.

### 5. Modality-Aware Retrieval

The hybrid retrieval system combines vector similarity search with graph traversal algorithms for comprehensive content retrieval. It implements modality-aware ranking mechanisms and maintains relational coherence between retrieved elements to ensure contextually integrated information delivery.

**Retrieval Mechanisms:**

* **üîÄ Vector-Graph Fusion**: Integrates vector similarity search with graph traversal algorithms. This approach leverages both semantic embeddings and structural relationships for comprehensive content retrieval.
* **üìä Modality-Aware Ranking**: Implements adaptive scoring mechanisms that weight retrieval results based on content type relevance. The system adjusts rankings according to query-specific modality preferences.
* **üîó Relational Coherence Maintenance**: Maintains semantic and structural relationships between retrieved elements. This ensures coherent information delivery and contextual integrity.

---

## üöÄ Quick Start

*Initialize Your AI Journey*

[![](https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif)](https://user-images.githubusercontent.com/74038190/212284158-e840e285-664b-44d7-b79b-e264b5e54825.gif)

### Installation

#### Option 1: Install from PyPI (Recommended)

```
# Basic installation
pip install raganything

# With optional dependencies for extended format support:
pip install 'raganything[all]'              # All optional features
pip install 'raganything[image]'            # Image format conversion (BMP, TIFF, GIF, WebP)
pip install 'raganything[text]'             # Text file processing (TXT, MD)
pip install 'raganything[image,text]'       # Multiple features
```

#### Option 2: Install from Source

```
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone and setup the project with uv
git clone https://github.com/HKUDS/RAG-Anything.git
cd RAG-Anything

# Install the package and dependencies in a virtual environment
uv sync

# If you encounter network timeouts (especially for opencv packages):
# UV_HTTP_TIMEOUT=120 uv sync

# Run commands directly with uv (recommended approach)
uv run python examples/raganything_example.py --help

# Install with optional dependencies
uv sync --extra image --extra text  # Specific extras
uv sync --all-extras                 # All optional features
```

#### Optional Dependencies

* **`[image]`** - Enables processing of BMP, TIFF, GIF, WebP image formats (requires Pillow)
* **`[text]`** - Enables processing of TXT and MD files (requires ReportLab)
* **`[all]`** - Includes all Python optional dependencies

> **‚ö†Ô∏è Office Document Processing Requirements:**
>
> * Office documents (.doc, .docx, .ppt, .pptx, .xls, .xlsx) require **LibreOffice** installation
> * Download from [LibreOffice official website](https://www.libreoffice.org/download/download/)
> * **Windows**: Download installer from official website
> * **macOS**: `brew install --cask libreoffice`
> * **Ubuntu/Debian**: `sudo apt-get install libreoffice`
> * **CentOS/RHEL**: `sudo yum install libreoffice`

**Check MinerU installation:**

```
# Verify installation
mineru --version

# Check if properly configured
python -c "from raganything import RAGAnything; rag = RAGAnything(); print('‚úÖ MinerU installed properly' if rag.check_parser_installation() else '‚ùå MinerU installation issue')"
```

Models are downloaded automatically on first use. For manual download, refer to [MinerU Model Source Configuration](https://github.com/opendatalab/MinerU/blob/master/README.md#22-model-source-configuration).

### Usage Examples

#### 1. End-to-End Document Processing

```
import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def main():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        parser="mineru",  # Parser selection: mineru or docling
        parse_method="auto",  # Parse method: auto, ocr, or txt
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define LLM model function
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Define embedding function
    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Process a document
    await rag.process_document_complete(
        file_path="path/to/your/document.pdf",
        output_dir="./output",
        parse_method="auto"
    )

    # Query the processed content
    # Pure text query - for basic knowledge base search
    text_result = await rag.aquery(
        "What are the main findings shown in the figures and tables?",
        mode="hybrid"
    )
    print("Text query result:", text_result)

    # Multimodal query with specific multimodal content
    multimodal_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
    print("Multimodal query result:", multimodal_result)

if __name__ == "__main__":
    asyncio.run(main())
```

#### 2. Direct Multimodal Content Processing

```
import asyncio
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc
from raganything.modalprocessors import ImageModalProcessor, TableModalProcessor

async def process_multimodal_content():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Initialize LightRAG
    rag = LightRAG(
        working_dir="./rag_storage",
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )
    await rag.initialize_storages()

    # Process an image
    image_processor = ImageModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], image_data=None, **kwargs: openai_complete_if_cache(
            "gpt-4o",
            "",
            system_prompt=None,
            history_messages=[],
            messages=[
                {"role": "system", "content": system_prompt} if system_prompt else None,
                {"role": "user", "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                ]} if image_data else {"role": "user", "content": prompt}
            ],
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ) if image_data else openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    image_content = {
        "img_path": "path/to/image.jpg",
        "image_caption": ["Figure 1: Experimental results"],
        "image_footnote": ["Data collected in 2024"]
    }

    description, entity_info = await image_processor.process_multimodal_content(
        modal_content=image_content,
        content_type="image",
        file_path="research_paper.pdf",
        entity_name="Experimental Results Figure"
    )

    # Process a table
    table_processor = TableModalProcessor(
        lightrag=rag,
        modal_caption_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )
    )

    table_content = {
        "table_body": """
        | Method | Accuracy | F1-Score |
        |--------|----------|----------|
        | RAGAnything | 95.2% | 0.94 |
        | Baseline | 87.3% | 0.85 |
        """,
        "table_caption": ["Performance Comparison"],
        "table_footnote": ["Results on test dataset"]
    }

    description, entity_info = await table_processor.process_multimodal_content(
        modal_content=table_content,
        content_type="table",
        file_path="research_paper.pdf",
        entity_name="Performance Results Table"
    )

if __name__ == "__main__":
    asyncio.run(process_multimodal_content())
```

#### 3. Batch Processing

```
# Process multiple documents
await rag.process_folder_complete(
    folder_path="./documents",
    output_dir="./output",
    file_extensions=[".pdf", ".docx", ".pptx"],
    recursive=True,
    max_workers=4
)
```

#### 4. Custom Modal Processors

```
from raganything.modalprocessors import GenericModalProcessor

class CustomModalProcessor(GenericModalProcessor):
    async def process_multimodal_content(self, modal_content, content_type, file_path, entity_name):
        # Your custom processing logic
        enhanced_description = await self.analyze_custom_content(modal_content)
        entity_info = self.create_custom_entity(enhanced_description, entity_name)
        return await self._create_entity_and_chunk(enhanced_description, entity_info, file_path)
```

#### 5. Query Options

RAG-Anything provides three types of query methods:

**Pure Text Queries** - Direct knowledge base search using LightRAG:

```
# Different query modes for text queries
text_result_hybrid = await rag.aquery("Your question", mode="hybrid")
text_result_local = await rag.aquery("Your question", mode="local")
text_result_global = await rag.aquery("Your question", mode="global")
text_result_naive = await rag.aquery("Your question", mode="naive")

# Synchronous version
sync_text_result = rag.query("Your question", mode="hybrid")
```

**VLM Enhanced Queries** - Automatically analyze images in retrieved context using VLM:

```
# VLM enhanced query (automatically enabled when vision_model_func is provided)
vlm_result = await rag.aquery(
    "Analyze the charts and figures in the document",
    mode="hybrid"
    # vlm_enhanced=True is automatically set when vision_model_func is available
)

# Manually control VLM enhancement
vlm_enabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=True  # Force enable VLM enhancement
)

vlm_disabled = await rag.aquery(
    "What do the images show in this document?",
    mode="hybrid",
    vlm_enhanced=False  # Force disable VLM enhancement
)

# When documents contain images, VLM can see and analyze them directly
# The system will automatically:
# 1. Retrieve relevant context containing image paths
# 2. Load and encode images as base64
# 3. Send both text context and images to VLM for comprehensive analysis
```

**Multimodal Queries** - Enhanced queries with specific multimodal content analysis:

```
# Query with table data
table_result = await rag.aquery_with_multimodal(
    "Compare these performance metrics with the document content",
    multimodal_content=[{
        "type": "table",
        "table_data": """Method,Accuracy,Speed
                        RAGAnything,95.2%,120ms
                        Traditional,87.3%,180ms""",
        "table_caption": "Performance comparison"
    }],
    mode="hybrid"
)

# Query with equation content
equation_result = await rag.aquery_with_multimodal(
    "Explain this formula and its relevance to the document content",
    multimodal_content=[{
        "type": "equation",
        "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
        "equation_caption": "Document relevance probability"
    }],
    mode="hybrid"
)
```

#### 6. Loading Existing LightRAG Instance

```
import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag import LightRAG
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.kg.shared_storage import initialize_pipeline_status
from lightrag.utils import EmbeddingFunc
import os

async def load_existing_lightrag():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # First, create or load existing LightRAG instance
    lightrag_working_dir = "./existing_lightrag_storage"

    # Check if previous LightRAG instance exists
    if os.path.exists(lightrag_working_dir) and os.listdir(lightrag_working_dir):
        print("‚úÖ Found existing LightRAG instance, loading...")
    else:
        print("‚ùå No existing LightRAG instance found, will create new one")

    # Create/load LightRAG instance with your configuration
    lightrag_instance = LightRAG(
        working_dir=lightrag_working_dir,
        llm_model_func=lambda prompt, system_prompt=None, history_messages=[], **kwargs: openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        ),
        embedding_func=EmbeddingFunc(
            embedding_dim=3072,
            max_token_size=8192,
            func=lambda texts: openai_embed(
                texts,
                model="text-embedding-3-large",
                api_key=api_key,
                base_url=base_url,
            ),
        )
    )

    # Initialize storage (this will load existing data if available)
    await lightrag_instance.initialize_storages()
    await initialize_pipeline_status()

    # Define vision model function for image processing
    def vision_model_func(
        prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs
    ):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt}
                    if system_prompt
                    else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_data}"
                                },
                            },
                        ],
                    }
                    if image_data
                    else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return lightrag_instance.llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    # Now use existing LightRAG instance to initialize RAGAnything
    rag = RAGAnything(
        lightrag=lightrag_instance,  # Pass existing LightRAG instance
        vision_model_func=vision_model_func,
        # Note: working_dir, llm_model_func, embedding_func, etc. are inherited from lightrag_instance
    )

    # Query existing knowledge base
    result = await rag.aquery(
        "What data has been processed in this LightRAG instance?",
        mode="hybrid"
    )
    print("Query result:", result)

    # Add new multimodal document to existing LightRAG instance
    await rag.process_document_complete(
        file_path="path/to/new/multimodal_document.pdf",
        output_dir="./output"
    )

if __name__ == "__main__":
    asyncio.run(load_existing_lightrag())
```

#### 7. Direct Content List Insertion

For scenarios where you already have a pre-parsed content list (e.g., from external parsers or previous processing), you can directly insert it into RAGAnything without document parsing:

```
import asyncio
from raganything import RAGAnything, RAGAnythingConfig
from lightrag.llm.openai import openai_complete_if_cache, openai_embed
from lightrag.utils import EmbeddingFunc

async def insert_content_list_example():
    # Set up API configuration
    api_key = "your-api-key"
    base_url = "your-base-url"  # Optional

    # Create RAGAnything configuration
    config = RAGAnythingConfig(
        working_dir="./rag_storage",
        enable_image_processing=True,
        enable_table_processing=True,
        enable_equation_processing=True,
    )

    # Define model functions
    def llm_model_func(prompt, system_prompt=None, history_messages=[], **kwargs):
        return openai_complete_if_cache(
            "gpt-4o-mini",
            prompt,
            system_prompt=system_prompt,
            history_messages=history_messages,
            api_key=api_key,
            base_url=base_url,
            **kwargs,
        )

    def vision_model_func(prompt, system_prompt=None, history_messages=[], image_data=None, messages=None, **kwargs):
        # If messages format is provided (for multimodal VLM enhanced query), use it directly
        if messages:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=messages,
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Traditional single image format
        elif image_data:
            return openai_complete_if_cache(
                "gpt-4o",
                "",
                system_prompt=None,
                history_messages=[],
                messages=[
                    {"role": "system", "content": system_prompt} if system_prompt else None,
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                        ],
                    } if image_data else {"role": "user", "content": prompt},
                ],
                api_key=api_key,
                base_url=base_url,
                **kwargs,
            )
        # Pure text format
        else:
            return llm_model_func(prompt, system_prompt, history_messages, **kwargs)

    embedding_func = EmbeddingFunc(
        embedding_dim=3072,
        max_token_size=8192,
        func=lambda texts: openai_embed(
            texts,
            model="text-embedding-3-large",
            api_key=api_key,
            base_url=base_url,
        ),
    )

    # Initialize RAGAnything
    rag = RAGAnything(
        config=config,
        llm_model_func=llm_model_func,
        vision_model_func=vision_model_func,
        embedding_func=embedding_func,
    )

    # Example: Pre-parsed content list from external source
    content_list = [
        {
            "type": "text",
            "text": "This is the introduction section of our research paper.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "image",
            "img_path": "/absolute/path/to/figure1.jpg",  # IMPORTANT: Use absolute path
            "image_caption": ["Figure 1: System Architecture"],
            "image_footnote": ["Source: Authors' original design"],
            "page_idx": 1  # Page number where this image appears
        },
        {
            "type": "table",
            "table_body": "| Method | Accuracy | F1-Score |\n|--------|----------|----------|\n| Ours | 95.2% | 0.94 |\n| Baseline | 87.3% | 0.85 |",
            "table_caption": ["Table 1: Performance Comparison"],
            "table_footnote": ["Results on test dataset"],
            "page_idx": 2  # Page number where this table appears
        },
        {
            "type": "equation",
            "latex": "P(d|q) = \\frac{P(q|d) \\cdot P(d)}{P(q)}",
            "text": "Document relevance probability formula",
            "page_idx": 3  # Page number where this equation appears
        },
        {
            "type": "text",
            "text": "In conclusion, our method demonstrates superior performance across all metrics.",
            "page_idx": 4  # Page number where this content appears
        }
    ]

    # Insert the content list directly
    await rag.insert_content_list(
        content_list=content_list,
        file_path="research_paper.pdf",  # Reference file name for citation
        split_by_character=None,         # Optional text splitting
        split_by_character_only=False,   # Optional text splitting mode
        doc_id=None,                     # Optional custom document ID (will be auto-generated if not provided)
        display_stats=True               # Show content statistics
    )

    # Query the inserted content
    result = await rag.aquery(
        "What are the key findings and performance metrics mentioned in the research?",
        mode="hybrid"
    )
    print("Query result:", result)

    # You can also insert multiple content lists with different document IDs
    another_content_list = [
        {
            "type": "text",
            "text": "This is content from another document.",
            "page_idx": 0  # Page number where this content appears
        },
        {
            "type": "table",
            "table_body": "| Feature | Value |\n|---------|-------|\n| Speed | Fast |\n| Accuracy | High |",
            "table_caption": ["Feature Comparison"],
            "page_idx": 1  # Page number where this table appears
        }
    ]

    await rag.insert_content_list(
        content_list=another_content_list,
        file_path="another_document.pdf",
        doc_id="custom-doc-id-123"  # Custom document ID
    )

if __name__ == "__main__":
    asyncio.run(insert_content_list_example())
```

**Content List Format:**

The `content_list` should follow the standard format with each item being a dictionary containing:

* **Text content**: `{"type": "text", "text": "content text", "page_idx": 0}`
* **Image content**: `{"type": "image", "img_path": "/absolute/path/to/image.jpg", "image_caption": ["caption"], "image_footnote": ["note"], "page_idx": 1}`
* **Table content**: `{"type": "table", "table_body": "markdown table", "table_caption": ["caption"], "table_footnote": ["note"], "page_idx": 2}`
* **Equation content**: `{"type": "equation", "latex": "LaTeX formula", "text": "description", "page_idx": 3}`
* **Generic content**: `{"type": "custom_type", "content": "any content", "page_idx": 4}`

**Important Notes:**

* **`img_path`**: Must be an absolute path to the image file (e.g., `/home/user/images/chart.jpg` or `C:\Users\user\images\chart.jpg`)
* **`page_idx`**: Represents the page number where the content appears in the original document (0-based indexing)
* **Content ordering**: Items are processed in the order they appear in the list

This method is particularly useful when:

* You have content from external parsers (non-MinerU/Docling)
* You want to process programmatically generated content
* You need to insert content from multiple sources into a single knowledge base
* You have cached parsing results that you want to reuse

---

## üõ†Ô∏è Examples

*Practical Implementation Demos*

[![](https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif)](https://user-images.githubusercontent.com/74038190/212257455-13e3e01e-d6a6-45dc-bb92-3ab87b12dfc1.gif)

The `examples/` directory contains comprehensive usage examples:

* **`raganything_example.py`**: End-to-end document processing with MinerU
* **`modalprocessors_example.py`**: Direct multimodal content processing
* **`office_document_test.py`**: Office document parsing test with MinerU (no API key required)
* **`image_format_test.py`**: Image format parsing test with MinerU (no API key required)
* **`text_format_test.py`**: Text format parsing test with MinerU (no API key required)

**Run examples:**

```
# End-to-end processing with parser selection
python examples/raganything_example.py path/to/document.pdf --api-key YOUR_API_KEY --parser mineru

# Direct modal processing
python examples/modalprocessors_example.py --api-key YOUR_API_KEY

# Office document parsing test (MinerU only)
python examples/office_document_test.py --file path/to/document.docx

# Image format parsing test (MinerU only)
python examples/image_format_test.py --file path/to/image.bmp

# Text format parsing test (MinerU only)
python examples/text_format_test.py --file path/to/document.md

# Check LibreOffice installation
python examples/office_document_test.py --check-libreoffice --file dummy

# Check PIL/Pillow installation
python examples/image_format_test.py --check-pillow --file dummy

# Check ReportLab installation
python examples/text_format_test.py --check-reportlab --file dummy
```

---

## üîß Configuration

*System Optimization Parameters*

### Environment Variables

Create a `.env` file (refer to `.env.example`):

```
OPENAI_API_KEY=your_openai_api_key
OPENAI_BASE_URL=your_base_url  # Optional
OUTPUT_DIR=./output             # Default output directory for parsed documents
PARSER=mineru                   # Parser selection: mineru or docling
PARSE_METHOD=auto              # Parse method: auto, ocr, or txt
```

**Note:** For backward compatibility, legacy environment variable names are still supported:

* `MINERU_PARSE_METHOD` is deprecated, please use `PARSE_METHOD`

> **Note**: API keys are only required for full RAG processing with LLM integration. The parsing test files (`office_document_test.py` and `image_format_test.py`) only test parser functionality and do not require API keys.

### Parser Configuration

RAGAnything now supports multiple parsers, each with specific advantages:

#### MinerU Parser

* Supports PDF, images, Office documents, and more formats
* Powerful OCR and table extraction capabilities
* GPU acceleration support

#### Docling Parser

* Optimized for Office documents and HTML files
* Better document structure preservation
* Native support for multiple Office formats

### MinerU Configuration

```
# MinerU 2.0 uses command-line parameters instead of config files
# Check available options:
mineru --help

# Common configurations:
mineru -p input.pdf -o output_dir -m auto    # Automatic parsing mode
mineru -p input.pdf -o output_dir -m ocr     # OCR-focused parsing
mineru -p input.pdf -o output_dir -b pipeline --device cuda  # GPU acceleration
```

You can also configure parsing through RAGAnything parameters:

```
# Basic parsing configuration with parser selection
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # or "ocr", "txt"
    parser="mineru"               # Optional: "mineru" or "docling"
)

# Advanced parsing configuration with special parameters
await rag.process_document_complete(
    file_path="document.pdf",
    output_dir="./output/",
    parse_method="auto",          # Parsing method: "auto", "ocr", "txt"
    parser="mineru",              # Parser selection: "mineru" or "docling"

    # MinerU special parameters - all supported kwargs:
    lang="ch",                   # Document language for OCR optimization (e.g., "ch", "en", "ja")
    device="cuda:0",             # Inference device: "cpu", "cuda", "cuda:0", "npu", "mps"
    start_page=0,                # Starting page number (0-based, for PDF)
    end_page=10,                 # Ending page number (0-based, for PDF)
    formula=True,                # Enable formula parsing
    table=True,                  # Enable table parsing
    backend="pipeline",          # Parsing backend: pipeline|vlm-transformers|vlm-sglang-engine|vlm-sglang-client.
    source="huggingface",        # Model source: "huggingface", "modelscope", "local"
    # vlm_url="http://127.0.0.1:3000" # Service address when using backend=vlm-sglang-client

    # Standard RAGAnything parameters
    display_stats=True,          # Display content statistics
    split_by_character=None,     # Optional character to split text by
    doc_id=None                  # Optional document ID
)
```

> **Note**: MinerU 2.0 no longer uses the `magic-pdf.json` configuration file. All settings are now passed as command-line parameters or function arguments. RAG-Anything now supports multiple document parsers - you can choose between MinerU and Docling based on your needs.

### Processing Requirements

Different content types require specific optional dependencies:

* **Office Documents** (.doc, .docx, .ppt, .pptx, .xls, .xlsx): Install [LibreOffice](https://www.libreoffice.org/download/download/)
* **Extended Image Formats** (.bmp, .tiff, .gif, .webp): Install with `pip install raganything[image]`
* **Text Files** (.txt, .md): Install with `pip install raganything[text]`

> **üìã Quick Install**: Use `pip install raganything[all]` to enable all format support (Python dependencies only - LibreOffice still needs separate installation)

---

## üß™ Supported Content Types

### Document Formats

* **PDFs** - Research papers, reports, presentations
* **Office Documents** - DOC, DOCX, PPT, PPTX, XLS, XLSX
* **Images** - JPG, PNG, BMP, TIFF, GIF, WebP
* **Text Files** - TXT, MD

### Multimodal Elements

* **Images** - Photographs, diagrams, charts, screenshots
* **Tables** - Data tables, comparison charts, statistical summaries
* **Equations** - Mathematical formulas in LaTeX format
* **Generic Content** - Custom content types via extensible processors

*For installation of format-specific dependencies, see the [Configuration](#-configuration) section.*

---

## üìñ Citation

*Academic Reference*

üìñ

If you find RAG-Anything useful in your research, please cite our paper:

```
@misc{guo2025raganythingallinoneragframework,
      title={RAG-Anything: All-in-One RAG Framework},
      author={Zirui Guo and Xubin Ren and Lingrui Xu and Jiahao Zhang and Chao Huang},
      year={2025},
      eprint={2510.12323},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.12323},
}
```

---

## üîó Related Projects

*Ecosystem & Extensions*

|  |  |  |
| --- | --- | --- |
| [‚ö° **LightRAG**  Simple and Fast RAG](https://github.com/HKUDS/LightRAG) | [üé• **VideoRAG**  Extreme Long-Context Video RAG](https://github.com/HKUDS/VideoRAG) | [‚ú® **MiniRAG**  Extremely Simple RAG](https://github.com/HKUDS/MiniRAG) |

---

## ‚≠ê Star History

*Community Growth Trajectory*

[![Star History Chart](https://camo.githubusercontent.com/ecd2e6786f34337407f35b748bc0513c51f343495659d885798edbdfddacb75d/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d484b5544532f5241472d416e797468696e6726747970653d44617465)](https://star-history.com/#HKUDS/RAG-Anything&Date)

---

## ü§ù Contribution

*Join the Innovation*

We thank all our contributors for their valuable contributions.

[![](https://camo.githubusercontent.com/1680c784f122bc349c4b6258a1bacea1304f3219e8ce71ee9d54388eb5f2238e/68747470733a2f2f636f6e747269622e726f636b732f696d6167653f7265706f3d484b5544532f5241472d416e797468696e67)](https://github.com/HKUDS/RAG-Anything/graphs/contributors)

---

[![](https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif)](https://user-images.githubusercontent.com/74038190/212284100-561aa473-3905-4a80-b561-0d28506553ee.gif)

[![](https://camo.githubusercontent.com/e56115416967f2f5db52d380cd13033047da3a5285d17200626095a624aa3076/68747470733a2f2f696d672e736869656c64732e696f2f62616467652fe2ad902532305374617225323075732532306f6e2532304769744875622d3161316132653f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465)](https://github.com/HKUDS/RAG-Anything)
[![](https://camo.githubusercontent.com/908b8e2bc4777c58983e20ddea95fd0a128375b36e09c50aafdcb20a83b9eda6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f909b2532305265706f72742532304973737565732d6666366236623f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465)](https://github.com/HKUDS/RAG-Anything/issues)
[![](https://camo.githubusercontent.com/d163dcc81671b8e3c87347309af9b05ef697fcac12df65b3d4e8d78ab5c7b6c1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652ff09f92ac25323044697363757373696f6e732d3465636463343f7374796c653d666f722d7468652d6261646765266c6f676f3d676974687562266c6f676f436f6c6f723d7768697465)](https://github.com/HKUDS/RAG-Anything/discussions)

‚≠ê
Thank you for visiting RAG-Anything!
‚≠ê

Building the Future of Multimodal AI

## About

"RAG-Anything: All-in-One RAG Framework"

[arxiv.org/abs/2510.12323](http://arxiv.org/abs/2510.12323 "http://arxiv.org/abs/2510.12323")

### Topics

[retrieval-augmented-generation](/topics/retrieval-augmented-generation "Topic: retrieval-augmented-generation")
[multi-modal-rag](/topics/multi-modal-rag "Topic: multi-modal-rag")

### Resources

[Readme](#readme-ov-file)

### License

[MIT license](#MIT-1-ov-file)

### Uh oh!

There was an error while loading. Please reload this page.

[Activity](/HKUDS/RAG-Anything/activity)

[Custom properties](/HKUDS/RAG-Anything/custom-properties)

### Stars

[**11.6k**
stars](/HKUDS/RAG-Anything/stargazers)

### Watchers

[**76**
watching](/HKUDS/RAG-Anything/watchers)

### Forks

[**1.4k**
forks](/HKUDS/RAG-Anything/forks)

[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FHKUDS%2FRAG-Anything&report=HKUDS+%28user%29)

## [Releases 15](/HKUDS/RAG-Anything/releases)

[v1.2.8

Latest

Sep 22, 2025](/HKUDS/RAG-Anything/releases/tag/v1.2.8)

[+ 14 releases](/HKUDS/RAG-Anything/releases)

## [Packages 0](/orgs/HKUDS/packages?repo_name=RAG-Anything)

No packages published

## [Contributors 21](/HKUDS/RAG-Anything/graphs/contributors)

* [![@LarFii](https://avatars.githubusercontent.com/u/49157727?s=64&v=4)](https://github.com/LarFii)
* [![@chaohuang-ai](https://avatars.githubusercontent.com/u/204865953?s=64&v=4)](https://github.com/chaohuang-ai)
* [![@LaansDole](https://avatars.githubusercontent.com/u/85084360?s=64&v=4)](https://github.com/LaansDole)
* [![@hzywhite](https://avatars.githubusercontent.com/u/53216231?s=64&v=4)](https://github.com/hzywhite)
* [![@Zongwei9888](https://avatars.githubusercontent.com/u/145887958?s=64&v=4)](https://github.com/Zongwei9888)
* [![@xlrrrr](https://avatars.githubusercontent.com/u/151314054?s=64&v=4)](https://github.com/xlrrrr)
* [![@MinalMahalaShorthillsAI](https://avatars.githubusercontent.com/u/180261793?s=64&v=4)](https://github.com/MinalMahalaShorthillsAI)
* [![@Harrywang55666](https://avatars.githubusercontent.com/u/11702572?s=64&v=4)](https://github.com/Harrywang55666)
* [![@zzhtx258](https://avatars.githubusercontent.com/u/175302980?s=64&v=4)](https://github.com/zzhtx258)
* [![@EightyOliveira](https://avatars.githubusercontent.com/u/50482796?s=64&v=4)](https://github.com/EightyOliveira)
* [![@ShorthillsAI](https://avatars.githubusercontent.com/u/141953346?s=64&v=4)](https://github.com/ShorthillsAI)
* [![@didier-durand](https://avatars.githubusercontent.com/u/2927957?s=64&v=4)](https://github.com/didier-durand)
* [![@hanlianlu](https://avatars.githubusercontent.com/u/7419836?s=64&v=4)](https://github.com/hanlianlu)
* [![@ikmak](https://avatars.githubusercontent.com/u/52026540?s=64&v=4)](https://github.com/ikmak)

[+ 7 contributors](/HKUDS/RAG-Anything/graphs/contributors)

## Languages

* [Python
  100.0%](/HKUDS/RAG-Anything/search?l=python)

## Footer

¬© 2025 GitHub,¬†Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Community](https://github.community/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can‚Äôt perform that action at this time.
