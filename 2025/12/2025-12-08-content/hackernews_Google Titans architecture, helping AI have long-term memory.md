---
source: hackernews
title: Google Titans architecture, helping AI have long-term memory
url: https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/
date: 2025-12-08
---

[Jump to Content](#page-content)

[Research](/ "Google Research")

[Research](/ "Google Research")

* Who we are

  Back to
   Who we are
  menu

  ---

  ## Defining the technology of today and tomorrow.

  + ## Philosophy

    We strive to create an environment conducive to many different types of research across many different time scales and levels of risk.

    [Learn more about our Philosophy
    Learn more](https://research.google/philosophy/)

    [Philosophy](https://research.google/philosophy/)
  + ## People

    Our researchers drive advancements in computer science through both fundamental and applied research.

    [Learn more about our People
    Learn more](https://research.google/people/)

    [People](https://research.google/people/)
* Research areas

  Back to
   Research areas
  menu

  ---

  + ## Research areas

    - [Explore all research areas](https://research.google/research-areas/)

    Research areas

    Back to
     Research areas
    menu

    ---

    - [Explore all research areas](https://research.google/research-areas/)
  + ## Foundational ML & Algorithms

    - [Algorithms & Theory](https://research.google/research-areas/algorithms-and-theory/)
    - [Data Management](https://research.google/research-areas/data-management/)
    - [Data Mining & Modeling](https://research.google/research-areas/data-mining-and-modeling/)
    - [Information Retrieval & the Web](https://research.google/research-areas/information-retrieval-and-the-web/)
    - [Machine Intelligence](https://research.google/research-areas/machine-intelligence/)
    - [Machine Perception](https://research.google/research-areas/machine-perception/)
    - [Machine Translation](https://research.google/research-areas/machine-translation/)
    - [Natural Language Processing](https://research.google/research-areas/natural-language-processing/)
    - [Speech Processing](https://research.google/research-areas/speech-processing/)

    Foundational ML & Algorithms

    Back to
     Foundational ML & Algorithms
    menu

    ---

    - [Algorithms & Theory](https://research.google/research-areas/algorithms-and-theory/)
    - [Data Management](https://research.google/research-areas/data-management/)
    - [Data Mining & Modeling](https://research.google/research-areas/data-mining-and-modeling/)
    - [Information Retrieval & the Web](https://research.google/research-areas/information-retrieval-and-the-web/)
    - [Machine Intelligence](https://research.google/research-areas/machine-intelligence/)
    - [Machine Perception](https://research.google/research-areas/machine-perception/)
    - [Machine Translation](https://research.google/research-areas/machine-translation/)
    - [Natural Language Processing](https://research.google/research-areas/natural-language-processing/)
    - [Speech Processing](https://research.google/research-areas/speech-processing/)
  + ## Computing Systems & Quantum AI

    - [Distributed Systems & Parallel Computing](https://research.google/research-areas/distributed-systems-and-parallel-computing/)
    - [Hardware & Architecture](https://research.google/research-areas/hardware-and-architecture/)
    - [Mobile Systems](https://research.google/research-areas/mobile-systems/)
    - [Networking](https://research.google/research-areas/networking/)
    - [Quantum Computing](https://research.google/research-areas/quantum-computing/)
    - [Robotics](https://research.google/research-areas/robotics/)
    - [Security, Privacy, & Abuse Prevention](https://research.google/research-areas/security-privacy-and-abuse-prevention/)
    - [Software Engineering](https://research.google/research-areas/software-engineering/)
    - [Software Systems](https://research.google/research-areas/software-systems/)

    Computing Systems & Quantum AI

    Back to
     Computing Systems & Quantum AI
    menu

    ---

    - [Distributed Systems & Parallel Computing](https://research.google/research-areas/distributed-systems-and-parallel-computing/)
    - [Hardware & Architecture](https://research.google/research-areas/hardware-and-architecture/)
    - [Mobile Systems](https://research.google/research-areas/mobile-systems/)
    - [Networking](https://research.google/research-areas/networking/)
    - [Quantum Computing](https://research.google/research-areas/quantum-computing/)
    - [Robotics](https://research.google/research-areas/robotics/)
    - [Security, Privacy, & Abuse Prevention](https://research.google/research-areas/security-privacy-and-abuse-prevention/)
    - [Software Engineering](https://research.google/research-areas/software-engineering/)
    - [Software Systems](https://research.google/research-areas/software-systems/)
  + ## Science, AI & Society

    - [Climate & Sustainability](https://research.google/research-areas/climate-and-sustainability/)
    - [Economics & Electronic Commerce](https://research.google/research-areas/economics-and-electronic-commerce/)
    - [Education Innovation](https://research.google/research-areas/education-innovation/)
    - [General Science](https://research.google/research-areas/general-science/)
    - [Health & Bioscience](https://research.google/research-areas/health-bioscience/)
    - [Human-Computer Interaction and Visualization](https://research.google/research-areas/human-computer-interaction-and-visualization/)
    - [Responsible AI](https://research.google/research-areas/responsible-ai/)

    Science, AI & Society

    Back to
     Science, AI & Society
    menu

    ---

    - [Climate & Sustainability](https://research.google/research-areas/climate-and-sustainability/)
    - [Economics & Electronic Commerce](https://research.google/research-areas/economics-and-electronic-commerce/)
    - [Education Innovation](https://research.google/research-areas/education-innovation/)
    - [General Science](https://research.google/research-areas/general-science/)
    - [Health & Bioscience](https://research.google/research-areas/health-bioscience/)
    - [Human-Computer Interaction and Visualization](https://research.google/research-areas/human-computer-interaction-and-visualization/)
    - [Responsible AI](https://research.google/research-areas/responsible-ai/)
* Our work

  Back to
   Our work
  menu

  ---

  + ## Projects

    We regularly open-source projects with the broader research community and apply our developments to Google products.

    [Learn more about our Projects
    Learn more](https://research.google/resources/our-projects/)

    [Projects](https://research.google/resources/our-projects/)
  + ## Publications

    Publishing our work allows us to share ideas and work collaboratively to advance the field of computer science.

    [Learn more about our Publications
    Learn more](https://research.google/pubs/)

    [Publications](https://research.google/pubs/)
  + ## Resources

    We make products, tools, and datasets available to everyone with the goal of building a more collaborative ecosystem.

    [Learn more about our Resources
    Learn more](https://research.google/resources/)

    [Resources](https://research.google/resources/)
* Programs & events

  Back to
   Programs & events
  menu

  ---

  ## Shaping the future, together.

  [Collaborate with us](https://research.google/programs-and-events/)

  + ## Student programs

    Supporting the next generation of researchers through a wide range of programming.

    [Learn more about our Student programs
    Learn more](https://research.google/programs-and-events/student-engagement/)

    [Student programs](https://research.google/programs-and-events/student-engagement/)
  + ## Faculty programs

    Participating in the academic research community through meaningful engagement with university faculty.

    [Learn more about our Faculty programs
    Learn more](https://research.google/programs-and-events/faculty-engagement/)

    [Faculty programs](https://research.google/programs-and-events/faculty-engagement/)
  + ## Conferences & events

    Connecting with the broader research community through events is essential for creating progress in every aspect of our work.

    [Learn more about our Conferences & events
    Learn more](https://research.google/conferences-and-events/)

    [Conferences & events](https://research.google/conferences-and-events/)

  [Collaborate with us](https://research.google/programs-and-events/)
* [Careers](https://research.google/careers/)
* [Blog](https://research.google/blog/)

Search

![](https://storage.googleapis.com/gweb-research2023-media/original_images/Titans-0-Hero.png)

1. [Home](/)
2. [Blog](/blog/)

# Titans + MIRAS: Helping AI have long-term memory

December 4, 2025

Ali Behrouz, Student Researcher, Meisam Razaviyayn, Staff Researcher, and Vahab Mirrokni, VP and Google Fellow, Google Research

We introduce the Titans architecture and the MIRAS framework, which allow AI models to work much faster and handle massive contexts by updating their core memory while it's actively running.

## Quick links

* [Titans paper](https://arxiv.org/abs/2501.00663)
* [MIRAS paper](https://arxiv.org/pdf/2504.13173)
* Share

  + Copy link

    ×

The [Transformer architecture](https://en.wikipedia.org/wiki/Transformer_%28deep_learning%29) revolutionized [sequence modeling](https://medium.com/machine-learning-basics/sequence-modelling-b2cdf244c233) with its introduction of [attention](https://en.wikipedia.org/wiki/Attention_%28machine_learning%29), a mechanism by which models look back at earlier inputs to prioritize relevant input data. However, computational cost increases drastically with sequence length, which limits the ability to scale Transformer-based models to extremely long contexts, such as those required for full-document understanding or genomic analysis.

The research community explored various approaches for solutions, such as efficient linear [recurrent neural networks](https://www.d2l.ai/chapter_recurrent-modern/index.html) (RNNs) and [state space models](https://huggingface.co/blog/lbourdois/get-on-the-ssm-train) (SSMs) like [Mamba-2](https://arxiv.org/pdf/2405.21060). These models offer fast, linear scaling by compressing context into a fixed-size. However, this fixed-size compression cannot adequately capture the rich information in very long sequences.

In two new papers, [*Titans*](https://arxiv.org/abs/2501.00663) and [*MIRAS*](https://arxiv.org/pdf/2504.13173), we introduce an architecture and theoretical blueprint that combine the speed of RNNs with the accuracy of transformers. Titans is the specific architecture (the tool), and MIRAS is the theoretical framework (the blueprint) for generalizing these approaches. Together, they advance the concept of test-time memorization, the ability of an AI model to maintain long-term memory by incorporating more powerful “surprise” metrics (i.e., unexpected pieces of information) while the model is running and without dedicated offline retraining.

The MIRAS framework, as demonstrated by Titans, introduces a meaningful shift toward real-time adaptation. Instead of compressing information into a static state, this architecture actively learns and updates its own parameters as data streams in. This crucial mechanism enables the model to incorporate new, specific details into its core knowledge instantly.

## Titans: Learning new context on the fly

An effective learning system requires distinct yet interconnected memory modules, mirroring the [human brain's separation of short-term and long-term memory](https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/).

While attention mechanisms excel for precise, short-term memory, Titans introduces a novel neural [long-term memory module](https://arxiv.org/abs/2306.07174#:~:text=LongMem%20can:%20*%20Memorize%20long%20past%20context,Yan%20*%20Jianfeng%20Gao%20*%20Furu%20Wei), that, unlike the fixed-size vector or matrix memory in traditional RNNs, acts as a deep neural network (specifically, a [multi-layer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)). This memory module provides significantly higher expressive power, allowing the model to summarize large volumes of information without losing important context. The model isn't simply taking notes; it's understanding and synthesizing the entire story.

Crucially, Titans doesn’t just passively store data. It actively learns *how* to recognize and retain important relationships and conceptual themes that connect tokens across the entire input. A key aspect of this ability is what we call the “surprise metric”. In human psychology, we know we quickly and easily forget routine, expected events but remember things that break the pattern — unexpected, surprising, or highly emotional events.

![A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).](https://storage.googleapis.com/gweb-research2023-media/images/Titans-1-Overview.width-1250.png)

Overview of the Titans (MAC) architecture. It uses a long-term memory to compress the past data and then incorporate the summary into the context and pass it to attention. Attention can then decide if it needs to attend to the summary of the past or not.

In the context of Titans, the "surprise metric" is the model detecting a large difference between what it currently remembers and what the new input is telling it.

* *Low surprise*: If the new word is "cat" and the model's memory state already expects an animal word, the gradient (surprise) is low. It can safely skip memorizing the word "cat" in its permanent long-term state.
* *High surprise*: If the model's memory state is summarizing a serious financial report, and the new input is a picture of a banana peel (the unexpected event), the gradient (surprise) will be very high. This signals that the new input is important or anomalous, and it must be prioritized for permanent storage in the long-term memory module.

The model uses this internal error signal (the gradient) as a mathematical equivalent of saying, "This is unexpected and important!" This allows the Titans architecture to selectively update its long-term memory only with the most novel and context-breaking information, keeping the overall process fast and efficient.

Titans refines this mechanism by incorporating two critical elements:

1. *Momentum*: The model considers both "momentary surprise" (the current input) and "past surprise" (the recent context flow). This ensures relevant subsequent information is also captured, even if those tokens are not individually surprising.
2. *Forgetting (weight decay)*: To manage the finite capacity of the memory when dealing with extremely long sequences, Titans employ an adaptive weight decay mechanism. This acts as a forgetting gate, allowing the model to discard information that is no longer needed.

## MIRAS: A unified view of sequence modeling

Every major breakthrough in sequence modeling — from modern transformers to the new, lightning-fast linear RNNs — is essentially the same thing under the hood: a highly complex [associative memory](https://www.geeksforgeeks.org/computer-organization-architecture/associative-memory/) module.

Accordingly, what makes MIRAS both unique and practical is the way it views AI modeling. Instead of seeing diverse architectures, it sees different methods of solving the same problem: efficiently combining new information with old memories without letting the essential concepts be forgotten**.**

MIRAS defines a sequence model through four key design choices:

* *Memory architecture*: The structure that stores information (e.g., a vector, matrix, or a deep multi-layer perceptron, like in Titans).
* *Attentional bias*: The internal learning objective the model optimizes that determines what it prioritizes.
* *Retention gate*: The memory regularizer. MIRAS reinterprets "forgetting mechanisms" as specific forms of [regularization](https://dev.to/nareshnishad/day-27-regularization-techniques-for-large-language-models-llms-4af3) that balance new learning against retaining past knowledge.
* *Memory algorithm*: The optimization algorithm used to update the memory.

[

](https://storage.googleapis.com/gweb-research2023-media/media/MIRAS_Framework_Animation.mp4)

play silent looping video
pause silent looping video

unmute video
mute video

The MIRAS framework overview. In the MIRAS framework, we aim to learn an associative memory, mapping between keys and values. For each token, the memory module internally optimizes its inner attentional bias while using its retention gate to make sure that it does not deviate from its past state. The optimization process is done through gradient-based optimizer.

### Transcending the mean squared error paradigm

Virtually all successful existing sequence models rely on [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE) or [dot-product similarity](https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de) for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.

MIRAS transcends this limitation by providing a generative framework to explore a more rich design space informed by the literature in optimization and statistics. This allows for the creation of novel architectures with [non-Euclidean objectives](https://en.wikipedia.org/wiki/Non-Euclidean_geometry) and regularization.

Using MIRAS, we created three specific attention-free models:

* *YAAD*: We designed this MIRAS variant to be less sensitive to major errors or "outliers" (like a single typo in a large document). It uses a gentler math penalty ([Huber loss](https://en.wikipedia.org/wiki/Huber_loss)) for mistakes, so it doesn't overreact to one-off issues. This makes the model more robust when the input data is messy or inconsistent.
* *MONETA*: This model explores the use of more complex and strict mathematical penalties (called [generalized norms](https://en.wikipedia.org/wiki/Norm_%28mathematics%29)). It investigates whether using these more disciplined rules for both what the model attends to and what it forgets can lead to a more powerful and stable long-term memory system overall.
* *MEMORA*: This model focuses on achieving the best possible memory stability by forcing its memory to act like a strict probability map. By using this constraint, it ensures that every time the memory state is updated, the changes are controlled and balanced. This guarantees a clean, stable process for integrating new information.Virtually all successful existing sequence models rely on [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error) (MSE) or [dot-product similarity](https://medium.com/advanced-deep-learning/understanding-vector-similarity-b9c10f7506de) for both their bias and retention. This reliance can make models sensitive to outliers and limit their expressive power.

## Experiments and results

We rigorously compared Titans along with MIRAS variants (YAAD, MONETA, MEMORA) against leading architectures, including [Transformer++](https://arxiv.org/abs/2003.04974), [Mamba-2](https://arxiv.org/pdf/2405.21060), and [Gated DeltaNet](https://arxiv.org/pdf/2412.06464). We further validated versatility by testing Titans on genomic modeling (DNA) and time-series forecasting, proving the architecture generalizes effectively beyond text.

Across both standard language modeling datasets ([C4](https://c4model.com/), [WikiTex](https://huggingface.co/datasets/Salesforce/wikitext)t) and [zero-shot reasoning tasks](https://medium.com/%40hetzer2807/zero-shot-reasoning-unleashed-the-magic-of-large-language-models-4e877dfe470e) ([HellaSwag](https://arxiv.org/abs/1905.07830), PIQA), our models consistently demonstrated higher accuracy and [perplexity](https://en.wikipedia.org/wiki/Perplexity) (a measure of how surprised an LLM is when looking at a piece of text).

### The power of deep memory

Ablation studies clearly show that the depth of the memory architecture is crucial. When comparing long-term memory modules of the same size but different depths, modules with deeper memories consistently achieve lower perplexity in language modeling. Furthermore, they exhibit better scaling properties, maintaining performance as the sequence length increases significantly.

![Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.](https://storage.googleapis.com/gweb-research2023-media/images/Titans-3-Performance1.width-1250.png)

*The effect of memory depth on the perplexity across 360M and 760M parameter scales.*

### Language modeling and efficiency

In language modeling and commonsense reasoning tasks, Titans architectures outperform state-of-the-art linear recurrent models (such as Mamba-2 and Gated DeltaNet) and Transformer++ baselines of comparable sizes. The novel MIRAS variants (MONETA, YAAD, MEMORA) also achieve improved performance compared to these baselines, validating the benefit of exploring robust, non-MSE optimization mechanisms. Importantly, these models maintain efficient, parallelizable training and fast linear inference speeds.

### Extreme long-context recall

The most significant advantage of these new architectures is their ability to handle extremely long contexts. This is highlighted in the [BABILong benchmark](https://github.com/booydar/babilong), a task requiring reasoning across facts distributed in extremely long documents. In this challenging setting, Titans outperforms all baselines, including extremely large models like GPT-4, despite having many fewer parameters. Titans further demonstrates the capability to scale effectively to context window sizes larger than 2 million tokens.

![Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.](https://storage.googleapis.com/gweb-research2023-media/images/Titans-4-Performance2.width-1250.png)

*Performance of Titans on extreme long-context reasoning.*

## Conclusion

The introduction of Titans and the MIRAS framework marks a significant advancement in sequence modeling. By employing deep neural networks as memory modules that learn to memorize as data is coming in, these approaches overcome the limitations of fixed-size recurrent states. Furthermore, MIRAS provides a powerful theoretical unification, revealing the connection between online optimization, associative memory, and architectural design. By moving beyond the standard Euclidean paradigm, this research opens the door to a new generation of sequence models that combine the efficiency of RNNs with the expressive power needed for the era of long-context AI.

Labels:* [Generative AI](/blog/label/generative-ai)
* [Machine Intelligence](/blog/label/machine-intelligence)
* [Natural Language Processing](/blog/label/natural-language-processing)

## Quick links

* [Titans paper](https://arxiv.org/abs/2501.00663)
* [MIRAS paper](https://arxiv.org/pdf/2504.13173)
* Share

  + Copy link

    ×

### Other posts of interest

* [![](https://storage.googleapis.com/gweb-research2023-media/original_images/MSEB_Hero.png)

  December 3, 2025

  From Waveforms to Wisdom: The New Benchmark for Auditory Intelligence

  + Machine Intelligence
    ·
  + Sound & Accoustics
    ·
  + Speech Processing](/blog/from-waveforms-to-wisdom-the-new-benchmark-for-auditory-intelligence/)
* [![](https://storage.googleapis.com/gweb-research2023-media/original_images/GenUI-0c-Hero.png)

  November 18, 2025

  Generative UI: A rich, custom, visual interactive user experience for any prompt

  + Generative AI
    ·
  + Human-Computer Interaction and Visualization
    ·
  + Product](/blog/generative-ui-a-rich-custom-visual-interactive-user-experience-for-any-prompt/)
* [![](https://storage.googleapis.com/gweb-research2023-media/original_images/NestedLearning-0-Hero.png)

  November 7, 2025

  Introducing Nested Learning: A new ML paradigm for continual learning

  + Algorithms & Theory
    ·
  + Generative AI
    ·
  + Machine Intelligence](/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/)

×
❮
❯

![Titans-1-Overview](https://storage.googleapis.com/gweb-research2023-media/original_images/Titans-1-Overview.png)

A diagram illustrating a neural architecture with three layers: Contextual Memory (learning), Core (in-context learning), and Persistent Memory (fixed weights).

![Titans-4-Performance2](https://storage.googleapis.com/gweb-research2023-media/original_images/Titans-4-Performance2.png)

Line graph showing Titans (MAC)-FT maintains improved accuracy over increasing sequence lengths compared to GPT-4, Mamba-FT, and other models.

![Titans-3-Performance1](https://storage.googleapis.com/gweb-research2023-media/original_images/Titans-3-Performance1.png)

Two line charts showing that LMM and MM models maintain lower perplexity than Mamba as sequence length increases across 360M and 760M parameter scales.

Follow us

* [About Google](https://about.google/)
* [Google Products](https://about.google/intl/en/products/)
* [Privacy](https://policies.google.com/privacy)
* [Terms](https://policies.google.com/terms)

* [Help](https://support.google.com/?hl=en)
* Submit feedback

×

![]()
