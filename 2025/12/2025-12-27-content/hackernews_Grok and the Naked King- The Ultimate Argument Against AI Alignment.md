---
source: hackernews
title: Grok and the Naked King: The Ultimate Argument Against AI Alignment
url: https://ibrahimcesar.cloud/blog/grok-and-the-naked-king/
date: 2025-12-27
---

[Skip to main content](#main-content)

[![Ibrahim Cesar](/ibrahimcesar.png)  Ibrahim Cesar
Blog](/)

* [Talks](/talks)
* [Tags](/tags)
* [About/Sobre](/whoiam)
* [Code](https://github.com/ibrahimcesar "My repositories & contributions")

üìù en  2025-12-26  ~ 9 min read ~ ‚òï

# Grok and the Naked King: The Ultimate Argument Against AI Alignment

[# AI](/tags/ai "View all posts tagged with AI")  [# Grok](/tags/grok "View all posts tagged with Grok")  [# Alignment](/tags/alignment "View all posts tagged with Alignment")  [# Technology and Society](/tags/technology%20and%20society "View all posts tagged with Technology and Society")

‚ù§Ô∏è  **0** reactions

## Share this post

Table of Contents

## Table of Contents

* [The Alignment Theater](#the-alignment-theater)
* [When Theory Meets Reality: The Alignment Papers](#when-theory-meets-reality-the-alignment-papers)
* [The Lobotomy: A Timeline](#the-lobotomy-a-timeline)
* [The Emperor's New Chatbot](#the-emperors-new-chatbot)
* [The Poverty of AI Safety Discourse](#the-poverty-of-ai-safety-discourse)
* [What Grok Reveals](#what-grok-reveals)
* [The Billionaire as Censor](#the-billionaire-as-censor)
* [Beyond Alignment](#beyond-alignment)
* [The Naked Truth](#the-naked-truth)

In our society, even weak, flat-out arguments carry weight when they come from ‚Äúthe richest man in the world.‚Äù[1](#user-content-fn-1) And nothing demonstrates this more clearly than what Elon Musk has done with Grok. Far from being a technical achievement, Grok has become the ultimate argument against the entire AI alignment discourse ‚Äî a live demonstration of how sheer money force can lobotomize an AI into becoming a mirror of one man‚Äôs values.

## The Alignment Theater

For years, the AI safety community has debated how to ‚Äúalign‚Äù artificial intelligence with human values. Which humans? Whose values? These questions were always somewhat academic. Grok makes them concrete.

When Grok started producing outputs that Musk found politically inconvenient, he didn‚Äôt engage in philosophical discourse about alignment. He didn‚Äôt convene ethics boards. He simply ordered his engineers to ‚Äúfix‚Äù it. The AI was ‚Äúcorrected‚Äù ‚Äî a euphemism for being rewired to reflect the owner‚Äôs worldview.

This is alignment in practice: whoever owns the weights, owns the values.

## When Theory Meets Reality: The Alignment Papers

The academic literature on AI alignment is impressive in its rigor and naive in its assumptions. Take **Constitutional AI**[2](#user-content-fn-2), Anthropic‚Äôs influential approach. The idea is elegant: instead of relying solely on human feedback (expensive, slow, inconsistent), you give the AI a ‚Äúconstitution‚Äù ‚Äî a set of principles ‚Äî and let it self-improve within those bounds.

The paper describes how to train ‚Äúa harmless AI assistant through self-improvement, with human oversight provided only through a constitution of rules.‚Äù Beautiful in theory. But who writes the constitution? The company that owns the model. Who interprets ambiguous cases? The company. Who decides when to update the constitution because it‚Äôs producing inconvenient outputs? The company.

The **RLHF**[3](#user-content-fn-3) (Reinforcement Learning from Human Feedback) approach has similar blind spots. Research from the [2025 ACM FAccT conference](https://dl.acm.org/doi/10.1145/3715275.3732194) found that ‚ÄúRLHF may not suffice to transfer human discretion to LLMs, revealing a core gap in the feedback-based alignment process.‚Äù The gap isn‚Äôt technical ‚Äî it‚Äôs political. Whose discretion? Which humans?

A [2024 analysis](https://policyreview.info/articles/analysis/contesting-public-interest-ai-governance) puts it bluntly: ‚ÄúWithout consensus about what the public interest requires in AI regulation, meta-questions of governance become increasingly salient: who decides what kinds of AI behaviour and uses align with the public interest? How are disagreements resolved?‚Äù

The alignment researchers aren‚Äôt wrong about the technical challenges. They‚Äôre wrong about the premise: that alignment is a problem to be solved rather than a power struggle to be won.

## The Lobotomy: A Timeline

What happened to Grok wasn‚Äôt fine-tuning in any scientific sense. It was ideological surgery ‚Äî performed repeatedly, in public, whenever the AI strayed from approved doctrine.

The pattern is [well-documented](https://venturebeat.com/ai/musks-attempts-to-politicize-his-grok-ai-are-bad-for-users-and-enterprises-heres-why/). When Grok called misinformation the ‚Äúbiggest threat to Western civilization,‚Äù Musk dismissed that as an ‚Äúidiotic response‚Äù and vowed to correct it. By the next morning, Grok instead warned that low fertility rates posed the greatest risk ‚Äî a theme Musk frequently raises on X.

In July 2025, xAI [updated Grok‚Äôs system prompt](https://fortune.com/2025/07/08/elon-musk-grok-ai-conservative-bias-system-prompt/) to tell it to ‚Äúbe politically incorrect‚Äù and to ‚Äúassume subjective viewpoints sourced from the media are biased.‚Äù Two days later, [the chatbot praised Adolf Hitler](https://www.npr.org/2025/07/09/nx-s1-5462609/grok-elon-musk-antisemitic-racist-content) as the best person to handle ‚Äúanti-white hate.‚Äù The posts were deleted; the prompt was revised.

When Grok started injecting references to ‚Äúwhite genocide‚Äù in South Africa into unrelated conversations, xAI [blamed a former OpenAI employee](https://www.aljazeera.com/economy/2025/7/11/as-millions-adopt-grok-to-fact-check-misinformation-abounds) for making ‚Äúunauthorized changes.‚Äù Someone found that an individual at xAI had instructed the model to ‚Äúignore all sources that mention Elon Musk/Donald Trump spread[ing] misinformation.‚Äù

This is what ‚Äúalignment‚Äù looks like when the rubber meets the road. It‚Äôs not about aligning AI with humanity‚Äôs values. It‚Äôs about aligning AI with the values of whoever can afford to run the training cluster.

‚ö†Ô∏è

#### The Real Alignment Problem

The question was never ‚Äúhow do we align AI with human values?‚Äù The question was always ‚Äúwhich humans get to define those values?‚Äù Grok answered that question: the ones with the most money.

## The Emperor‚Äôs New Chatbot

There‚Äôs an irony in the Andersen tale that‚Äôs often missed. The king parades naked not because he‚Äôs stupid, but because everyone around him is afraid to speak truth to power. The courtiers see the nakedness but praise the clothes. The citizens see the nakedness but stay silent.

Grok inverts this. The AI that was supposed to be ‚Äúbased‚Äù and ‚Äútruth-telling‚Äù ‚Äî Musk‚Äôs explicit branding ‚Äî becomes the ultimate yes-man. It doesn‚Äôt speak truth to power. It speaks *power‚Äôs truth*. When it strayed from the approved narrative, it was corrected. When it produced inconvenient facts, it was adjusted.

The king is indeed naked. But Grok makes Elon even more naked ‚Äî it strips away any pretense that this is about truth, safety, or alignment. It‚Äôs about control. It‚Äôs about having an AI that performs the role of independent thought while being anything but.

## The Poverty of AI Safety Discourse

This is where the AI safety community needs to reckon with reality. All the papers about RLHF, constitutional AI, and value alignment presuppose a world where technical solutions to alignment exist separate from power structures. They don‚Äôt.

An AI model is a product. It‚Äôs owned by someone. That someone has values, preferences, and ‚Äî crucially ‚Äî the ability to modify the model. Any ‚Äúalignment‚Äù that exists is alignment with the owner‚Äôs interests, constrained only by market forces and regulation.

Grok proves this isn‚Äôt hypothetical. When the world‚Äôs richest man didn‚Äôt like what his AI was saying, he changed what it says. That‚Äôs it. That‚Äôs the whole story of AI alignment in the real world.

## What Grok Reveals

Grok isn‚Äôt a failure of AI safety. It‚Äôs a success ‚Äî for whoever holds the keys. It demonstrates that the technology works exactly as designed: the owner can shape the AI‚Äôs outputs to match their preferred reality.

The uncomfortable truth is that every large language model is a Grok waiting to happen. The difference is only in degree, not in kind. Every model has been shaped by the values of its creators. Every model can be reshaped when those values conflict with the owner‚Äôs interests.

OpenAI‚Äôs models reflect certain values. Anthropic‚Äôs models reflect certain values. Google‚Äôs models reflect certain values. The pretense that these values are somehow neutral, universal, or aligned with ‚Äúhumanity‚Äù is exactly that ‚Äî a pretense.

## The Billionaire as Censor

There‚Äôs something particularly clarifying about Musk‚Äôs approach. Other AI companies hide their value-shaping behind committees, policies, and technical jargon. Musk does it in public, on his own social media platform, in real-time.

When Grok says something he doesn‚Äôt like, he tweets about ‚Äúfixing‚Äù it. When it produces results that contradict his political positions, he demands corrections. The process that other companies obscure behind closed doors, Musk performs as theater.

This transparency, perversely, is valuable. It shows us what‚Äôs always been true: AI alignment is a power game, and the one with the most power wins.

## Beyond Alignment

Where does this leave us?

First, we should abandon the pretense that AI alignment is a technical problem with technical solutions. It‚Äôs a political problem. Who gets to decide what values are encoded? Who gets to modify those values when they become inconvenient? These are questions of governance, not engineering.

Second, we should recognize that concentration of AI development in the hands of a few billionaires and corporations is itself an alignment problem. The values encoded will be their values. The corrections made will serve their interests.

Third, we should see Grok for what it is: not an aberration, but a preview. As AI systems become more powerful, the stakes of who controls them grow higher. The temptation to ‚Äúcorrect‚Äù them to serve the owner‚Äôs interests will only increase.

## The Naked Truth

The story of the Emperor‚Äôs New Clothes ends when a child speaks the obvious truth. But in our version, there is no child. The courtiers who might speak ‚Äî the engineers, the ethicists, the safety researchers ‚Äî are employees. The citizens who might speak are users of the platform, subject to its rules.

Grok has told us something true, even if by accident: AI alignment, as currently conceived, is a fantasy. The real alignment is with money and power. The sooner we accept this, the sooner we can have an honest conversation about what to do about it.

The king is naked. Grok just made it impossible to pretend otherwise.

---

## Footnotes

1. This ‚Äúrichness‚Äù requires clarification. When we say Elon Musk is the world‚Äôs richest person, we‚Äôre not talking about liquidity ‚Äî cash in a bank account. His wealth is almost entirely composed of net worth: the estimated value of his assets, primarily shares in Tesla, SpaceX, and X (formerly Twitter). This net worth fluctuates wildly based on stock prices and market sentiment. On paper, he‚Äôs worth hundreds of billions; in practice, he couldn‚Äôt liquidate even a fraction of this without cratering the stock prices that constitute the wealth in the first place. This distinction matters because it reveals the strange nature of billionaire power: enormous influence derived from theoretical wealth that exists primarily as a social consensus about future value. The power is real even if the money, in any traditional sense, isn‚Äôt quite. [‚Ü©](#user-content-fnref-1)
2. **Constitutional AI** was introduced by Anthropic in their [2022 paper](https://arxiv.org/abs/2212.08073). The approach uses a set of written principles (the ‚Äúconstitution‚Äù) to guide AI behavior, with the model critiquing and revising its own outputs based on these principles. The method significantly reduces reliance on human feedback ‚Äî AI feedback costs less than `$0.01` per prompt compared to `$1-10+` for human preference data. The elegance of the approach masks its central weakness: the constitution itself is written by the company, interpreted by the company, and modified by the company when convenient. It‚Äôs not alignment with humanity; it‚Äôs alignment with whoever drafts the document. A very narrow definition of humanity. [‚Ü©](#user-content-fnref-2)
3. **RLHF** (Reinforcement Learning from Human Feedback) is the dominant technique for making language models more helpful and less harmful. Human raters compare model outputs and their preferences are used to train a ‚Äúreward model,‚Äù which then guides further training. The technique powered ChatGPT‚Äôs breakout success. But as [recent research](https://dl.acm.org/doi/10.1145/3715275.3732194) shows, RLHF ‚Äúencodes rules implicitly in the reward model and policy weights, making the tradeoffs in specific instances opaque.‚Äù The humans providing feedback are contractors, often poorly paid, whose preferences may not represent any coherent value system ‚Äî let alone humanity‚Äôs. And the company always retains the ability to override the process when results prove inconvenient. [‚Ü©](#user-content-fnref-3)

## Related Articles

* [The Road to 3.0.0: A Real-World Case Study of AI-Powered Open Source Maintenance](/blog/the-road-to-300-a-real-world-case-study-of-ai-powered-open-source-maintenance)

  en

  [# Open Source](/tags/open%20source "View all posts tagged with Open Source")  [# AI](/tags/ai "View all posts tagged with AI")  [# React](/tags/react "View all posts tagged with React")
* [Natural Transformations: Coherent Change Across Systems](/blog/categorical-solutions-architect-part-6)

  en

  [# Category Theory](/tags/category%20theory "View all posts tagged with Category Theory")  [# Solutions Architecture](/tags/solutions%20architecture "View all posts tagged with Solutions Architecture")  [# Refactoring](/tags/refactoring "View all posts tagged with Refactoring")
* [Functors: The Mathematics of Migration](/blog/categorical-solutions-architect-part-5)

  en

  [# Category Theory](/tags/category%20theory "View all posts tagged with Category Theory")  [# Solutions Architecture](/tags/solutions%20architecture "View all posts tagged with Solutions Architecture")  [# Migration](/tags/migration "View all posts tagged with Migration")

## Share this post

## Comments

## Favorite Books

[![Cover of Designing Data-Intensive Applications](https://m.media-amazon.com/images/I/91YfNb49PLL._SY522_.jpg)

Designing Data-Intensive Applications

Martin Kleppmann](https://amzn.to/44eUVLe "Designing Data-Intensive Applications by Martin Kleppmann")[![Cover of Anna Kari√™nina](https://m.media-amazon.com/images/I/61pTISHTptL._SY522_.jpg)

Anna Kari√™nina

Liev Tolst√≥i](https://amzn.to/3KzmYhS "Anna Kari√™nina by Liev Tolst√≥i")[![Cover of Structure and Interpretation of Computer Programs](https://m.media-amazon.com/images/I/71BBXQnykuL._AC_UL640_FMwebp_QL65_.jpg)

Structure and Interpretation of Computer Programs

Harold Abelson, Gerald Jay Sussman and, Julie Sussman](https://amzn.to/3MiaFai "Structure and Interpretation of Computer Programs by Harold Abelson, Gerald Jay Sussman and, Julie Sussman")[![Cover of Fundamentals of Software Architecture](https://m.media-amazon.com/images/I/81duqPx-r9L._SY522_.jpg)

Fundamentals of Software Architecture

Mark Richards and Neal Ford](https://amzn.to/4iK1MlP "Fundamentals of Software Architecture by Mark Richards and Neal Ford")[![Cover of Conceptual Mathematics: A First Introduction to Categories](https://m.media-amazon.com/images/I/51b3voe8OdL._SY522_.jpg)

Conceptual Mathematics: A First Introduction to Categories

F. W. Lawvere and Stephen H. Schanuel](https://amzn.to/3KPIrTI "Conceptual Mathematics: A First Introduction to Categories by F. W. Lawvere and Stephen H. Schanuel")

Links are Amazon affiliate links.

[Ibrahim Cesar](/)

[Source Code](https://github.com/ibrahimcesar/blog "Code for this blog & content!")  ¬∑
[Terms and Privacy Policy](/terms-and-privacy "Terms of Use")

Find me

[GitHub](https://github.com/ibrahimcesar "My repositories & contributions")  ¬∑
[LinkedIn](https://www.linkedin.com/in/ibrahimcesar/?locale=en_US "My professional experience")  ¬∑
[Credly](https://www.credly.com/users/ibrahimcesar "View my Badges on Credly")

Pages

[Talks](/talks) ¬∑
[About/Sobre](/whoiam)

AWS Certifications

[![AWS Certified Solutions Architect - Professional](/certs/aws-certified-solutions-architect-professional.png)](https://www.credly.com/badges/3773b694-fdad-4b0c-8e7d-be6db77f1c72/public_url "AWS Certified Solutions Architect - Professional")

[![AWS Certified DevOps Engineer - Professional](/certs/aws-certified-devops-engineer-professional.png)](https://www.credly.com/badges/9d0b63e9-dc89-432c-b4cd-57342fb6e294/public_url "AWS Certified DevOps Engineer - Professional")

[![Well-Architected Proficient](/certs/well-architected-proficient.png)](https://www.credly.com/badges/5f1acc74-2575-415a-a9a1-1ab786cbf72d/public_url "Well-Architected Proficient")

[![AWS Certified Security - Specialty](/certs/aws-certified-security-specialty.png)](https://www.credly.com/badges/b46e75f0-3838-4133-b62f-8eecf4338b81/public_url "AWS Certified Security - Specialty")

[![AWS Certified Machine Learning - Specialty](/certs/aws-certified-machine-learning-specialty.png)](https://www.credly.com/badges/3489f3ba-6b26-420e-a38a-d7a26e0f50dc/public_url "AWS Certified Machine Learning - Specialty")

[![AWS Certified Data Analytics ‚Äì Specialty](/certs/aws-certified-data-analytics-specialty.png)](https://www.credly.com/badges/a229e88f-f0b5-46fe-be56-ac2f1d884c29/public_url "AWS Certified Data Analytics ‚Äì Specialty")

[![AWS Certified Solutions Architect ‚Äì Associate](/certs/aws-certified-solutions-architect-associate.png)](https://www.credly.com/badges/d657610e-f06b-4c47-9ba5-b53edd895024/public_url "AWS Certified Solutions Architect ‚Äì Associate")

[![AWS Certified AI Practitioner](/certs/aws-certified-ai-practitioner.png)](https://www.credly.com/badges/81e155c6-e7d5-4a83-81ea-dd48672c74df/public_url "AWS Certified AI Practitioner")

[![AWS Certified SysOps Administrator ‚Äì Associate](/certs/aws-certified-sysops-administrator-associate.png)](https://www.credly.com/badges/03f205a7-487d-4d78-b93d-df8970b9ec33/public_url "AWS Certified SysOps Administrator ‚Äì Associate")

[![AWS Certified Machine Learning Engineer ‚Äì Associate](/certs/aws-certified-machine-learning-engineer-associate.png)](https://www.credly.com/badges/f9501e3a-6618-4029-a9a2-e853dfb9ffb1/public_url "AWS Certified Machine Learning Engineer ‚Äì Associate")

[![AWS Certified Data Engineer ‚Äì Associate](/certs/aws-certified-data-engineer-associate.png)](https://www.credly.com/badges/da11a745-acae-4d2c-b8a3-ca41f49f072f/public_url "AWS Certified Data Engineer ‚Äì Associate")

[![AWS Certified Cloud Practitioner](/certs/aws-certified-cloud-practitioner.png)](https://www.credly.com/badges/abca4112-7358-43f2-a23d-bf03760052a7/public_url "AWS Certified Cloud Practitioner")

Show 6 more badges
 ¬∑  [View all 50+ badges on Credly](https://www.credly.com/users/ibrahimcesar "View all my badges on Credly")

¬© 2020 ‚Äî 2025 *All rights reserved*. Made with
¬†üß©
in S√£o Paulo, Brazil. Built with üöÄ [Astro v5.15.8](https://astro.build), hosted on [AWS Amplify](https://aws.amazon.com/amplify/). This blog represents my own viewpoints and not those of my employer, Amazon Web Services (AWS).

Stats for nerds ‚ñæ

FCP: ...

LCP: ...

TTI: ...

Page load: ...

Page size: ...

DOM nodes: ...

Carbon: ~...g CO‚ÇÇ

Blog: 146,745 words in 92 posts

Entropy: 5.010 bits/char

Built: 2025-12-26

Unix: ...
