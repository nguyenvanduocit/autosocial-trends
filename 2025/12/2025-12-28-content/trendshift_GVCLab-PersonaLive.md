---
source: trendshift
title: GVCLab/PersonaLive
url: https://github.com/GVCLab/PersonaLive
date: 2025-12-28
---

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FGVCLab%2FPersonaLive)

Appearance settings

* Platform

  + AI CODE CREATION
    - [GitHub CopilotWrite better code with AI](https://github.com/features/copilot)
    - [GitHub SparkBuild and deploy intelligent apps](https://github.com/features/spark)
    - [GitHub ModelsManage and compare prompts](https://github.com/features/models)
    - [MCP RegistryNewIntegrate external tools](https://github.com/mcp)
  + DEVELOPER WORKFLOWS
    - [ActionsAutomate any workflow](https://github.com/features/actions)
    - [CodespacesInstant dev environments](https://github.com/features/codespaces)
    - [IssuesPlan and track work](https://github.com/features/issues)
    - [Code ReviewManage code changes](https://github.com/features/code-review)
  + APPLICATION SECURITY
    - [GitHub Advanced SecurityFind and fix vulnerabilities](https://github.com/security/advanced-security)
    - [Code securitySecure your code as you build](https://github.com/security/advanced-security/code-security)
    - [Secret protectionStop leaks before they start](https://github.com/security/advanced-security/secret-protection)
  + EXPLORE
    - [Why GitHub](https://github.com/why-github)
    - [Documentation](https://docs.github.com)
    - [Blog](https://github.blog)
    - [Changelog](https://github.blog/changelog)
    - [Marketplace](https://github.com/marketplace)

  [View all features](https://github.com/features)
* Solutions

  + BY COMPANY SIZE
    - [Enterprises](https://github.com/enterprise)
    - [Small and medium teams](https://github.com/team)
    - [Startups](https://github.com/enterprise/startups)
    - [Nonprofits](https://github.com/solutions/industry/nonprofits)
  + BY USE CASE
    - [App Modernization](https://github.com/solutions/use-case/app-modernization)
    - [DevSecOps](https://github.com/solutions/use-case/devsecops)
    - [DevOps](https://github.com/solutions/use-case/devops)
    - [CI/CD](https://github.com/solutions/use-case/ci-cd)
    - [View all use cases](https://github.com/solutions/use-case)
  + BY INDUSTRY
    - [Healthcare](https://github.com/solutions/industry/healthcare)
    - [Financial services](https://github.com/solutions/industry/financial-services)
    - [Manufacturing](https://github.com/solutions/industry/manufacturing)
    - [Government](https://github.com/solutions/industry/government)
    - [View all industries](https://github.com/solutions/industry)

  [View all solutions](https://github.com/solutions)
* Resources

  + EXPLORE BY TOPIC
    - [AI](https://github.com/resources/articles?topic=ai)
    - [Software Development](https://github.com/resources/articles?topic=software-development)
    - [DevOps](https://github.com/resources/articles?topic=devops)
    - [Security](https://github.com/resources/articles?topic=security)
    - [View all topics](https://github.com/resources/articles)
  + EXPLORE BY TYPE
    - [Customer stories](https://github.com/customer-stories)
    - [Events & webinars](https://github.com/resources/events)
    - [Ebooks & reports](https://github.com/resources/whitepapers)
    - [Business insights](https://github.com/solutions/executive-insights)
    - [GitHub Skills](https://skills.github.com)
  + SUPPORT & SERVICES
    - [Documentation](https://docs.github.com)
    - [Customer support](https://support.github.com)
    - [Community forum](https://github.com/orgs/community/discussions)
    - [Trust center](https://github.com/trust-center)
    - [Partners](https://github.com/partners)
* Open Source

  + COMMUNITY
    - [GitHub SponsorsFund open source developers](https://github.com/sponsors)
  + PROGRAMS
    - [Security Lab](https://securitylab.github.com)
    - [Maintainer Community](https://maintainers.github.com)
    - [Accelerator](https://github.com/accelerator)
    - [Archive Program](https://archiveprogram.github.com)
  + REPOSITORIES
    - [Topics](https://github.com/topics)
    - [Trending](https://github.com/trending)
    - [Collections](https://github.com/collections)
* Enterprise

  + ENTERPRISE SOLUTIONS
    - [Enterprise platformAI-powered developer platform](https://github.com/enterprise)
  + AVAILABLE ADD-ONS
    - [GitHub Advanced SecurityEnterprise-grade security features](https://github.com/security/advanced-security)
    - [Copilot for BusinessEnterprise-grade AI features](https://github.com/features/copilot/copilot-business)
    - [Premium SupportEnterprise-grade 24/7 support](https://github.com/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

[ ]
Include my email address so I can be contacted

Cancel
 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Cancel
 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FGVCLab%2FPersonaLive)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=GVCLab%2FPersonaLive)

Appearance settings

Resetting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[GVCLab](/GVCLab)
/
**[PersonaLive](/GVCLab/PersonaLive)**
Public

* [Notifications](/login?return_to=%2FGVCLab%2FPersonaLive) You must be signed in to change notification settings
* [Fork
  115](/login?return_to=%2FGVCLab%2FPersonaLive)
* [Star
   980](/login?return_to=%2FGVCLab%2FPersonaLive)

PersonaLive! : Expressive Portrait Image Animation for Live Streaming

[arxiv.org/abs/2512.11253](https://arxiv.org/abs/2512.11253 "https://arxiv.org/abs/2512.11253")

### License

[Apache-2.0 license](/GVCLab/PersonaLive/blob/main/LICENSE)

[980
stars](/GVCLab/PersonaLive/stargazers) [115
forks](/GVCLab/PersonaLive/forks) [Branches](/GVCLab/PersonaLive/branches) [Tags](/GVCLab/PersonaLive/tags) [Activity](/GVCLab/PersonaLive/activity)

[Star](/login?return_to=%2FGVCLab%2FPersonaLive)

[Notifications](/login?return_to=%2FGVCLab%2FPersonaLive) You must be signed in to change notification settings

* [Code](/GVCLab/PersonaLive)
* [Issues
  15](/GVCLab/PersonaLive/issues)
* [Pull requests
  2](/GVCLab/PersonaLive/pulls)
* [Actions](/GVCLab/PersonaLive/actions)
* [Projects
  0](/GVCLab/PersonaLive/projects)
* [Security

  ### Uh oh!

  There was an error while loading. Please reload this page.](/GVCLab/PersonaLive/security)
* [Insights](/GVCLab/PersonaLive/pulse)

Additional navigation options

* [Code](/GVCLab/PersonaLive)
* [Issues](/GVCLab/PersonaLive/issues)
* [Pull requests](/GVCLab/PersonaLive/pulls)
* [Actions](/GVCLab/PersonaLive/actions)
* [Projects](/GVCLab/PersonaLive/projects)
* [Security](/GVCLab/PersonaLive/security)
* [Insights](/GVCLab/PersonaLive/pulse)

# GVCLab/PersonaLive

main

[Branches](/GVCLab/PersonaLive/branches)[Tags](/GVCLab/PersonaLive/tags)

Go to file

Code

Open more actions menu

## Folders and files

| Name | | Name | Last commit message | Last commit date |
| --- | --- | --- | --- | --- |
| Latest commit   History[49 Commits](/GVCLab/PersonaLive/commits/main/) | | |
| [assets](/GVCLab/PersonaLive/tree/main/assets "assets") | | [assets](/GVCLab/PersonaLive/tree/main/assets "assets") |  |  |
| [configs](/GVCLab/PersonaLive/tree/main/configs "configs") | | [configs](/GVCLab/PersonaLive/tree/main/configs "configs") |  |  |
| [demo](/GVCLab/PersonaLive/tree/main/demo "demo") | | [demo](/GVCLab/PersonaLive/tree/main/demo "demo") |  |  |
| [pretrained\_weights](/GVCLab/PersonaLive/tree/main/pretrained_weights "pretrained_weights") | | [pretrained\_weights](/GVCLab/PersonaLive/tree/main/pretrained_weights "pretrained_weights") |  |  |
| [src](/GVCLab/PersonaLive/tree/main/src "src") | | [src](/GVCLab/PersonaLive/tree/main/src "src") |  |  |
| [tools](/GVCLab/PersonaLive/tree/main/tools "tools") | | [tools](/GVCLab/PersonaLive/tree/main/tools "tools") |  |  |
| [webcam](/GVCLab/PersonaLive/tree/main/webcam "webcam") | | [webcam](/GVCLab/PersonaLive/tree/main/webcam "webcam") |  |  |
| [LICENSE](/GVCLab/PersonaLive/blob/main/LICENSE "LICENSE") | | [LICENSE](/GVCLab/PersonaLive/blob/main/LICENSE "LICENSE") |  |  |
| [README.md](/GVCLab/PersonaLive/blob/main/README.md "README.md") | | [README.md](/GVCLab/PersonaLive/blob/main/README.md "README.md") |  |  |
| [inference\_offline.py](/GVCLab/PersonaLive/blob/main/inference_offline.py "inference_offline.py") | | [inference\_offline.py](/GVCLab/PersonaLive/blob/main/inference_offline.py "inference_offline.py") |  |  |
| [inference\_online.py](/GVCLab/PersonaLive/blob/main/inference_online.py "inference_online.py") | | [inference\_online.py](/GVCLab/PersonaLive/blob/main/inference_online.py "inference_online.py") |  |  |
| [requirements\_base.txt](/GVCLab/PersonaLive/blob/main/requirements_base.txt "requirements_base.txt") | | [requirements\_base.txt](/GVCLab/PersonaLive/blob/main/requirements_base.txt "requirements_base.txt") |  |  |
| [requirements\_trt.txt](/GVCLab/PersonaLive/blob/main/requirements_trt.txt "requirements_trt.txt") | | [requirements\_trt.txt](/GVCLab/PersonaLive/blob/main/requirements_trt.txt "requirements_trt.txt") |  |  |
| [torch2trt.py](/GVCLab/PersonaLive/blob/main/torch2trt.py "torch2trt.py") | | [torch2trt.py](/GVCLab/PersonaLive/blob/main/torch2trt.py "torch2trt.py") |  |  |
| View all files | | |

## Repository files navigation

* README
* Apache-2.0 license

[![PersonaLive](/GVCLab/PersonaLive/raw/main/assets/header.svg)](/GVCLab/PersonaLive/blob/main/assets/header.svg)

## Expressive Portrait Image Animation for Live Streaming

#### [Zhiyuan Li1,2,3](https://huai-chang.github.io/) ¬∑ [Chi-Man Pun1,üì™](https://cmpun.github.io/) ¬∑ [Chen Fang2](http://fangchen.org/) ¬∑ [Jue Wang2](https://scholar.google.com/citations?user=Bt4uDWMAAAAJ&hl=en) ¬∑ [Xiaodong Cun3,üì™](https://vinthony.github.io/academic/)

1 University of Macau ¬†¬† 2 [Dzine.ai](https://www.dzine.ai/) ¬†¬† 3 [GVC Lab, Great Bay University](https://gvclab.github.io/)

[![](https://camo.githubusercontent.com/bc425a691fd4f450485ee313bed9124d69c9a6a5d4d53ea2151a5fc8af9c6da0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f41725869762d323531322e31313235332d726564)](https://arxiv.org/abs/2512.11253) [![](https://camo.githubusercontent.com/a90baacba1b458fcce1243526e49de61e28f8202acc303124342c1c6cf077882/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f25463025394625413425393725323048756767696e67253230466163652d4d6f64656c2d666663313037)](https://huggingface.co/huaichang/PersonaLive) [![](https://camo.githubusercontent.com/e50330d5049faa9ca6823384ebd7e4e36e37c76632c1b3c97e82334837a83d6d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f64656c53636f70652d4d6f64656c2d363234414646)](https://modelscope.cn/models/huaichang/PersonaLive) [![GitHub](https://camo.githubusercontent.com/0f30946c6d86c360418fc190a9b86817cc4490b05ac419203159bd5981801fb0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f4756434c61622f506572736f6e614c6976653f7374796c653d736f6369616c)](https://github.com/GVCLab/PersonaLive)

[![highlight](/GVCLab/PersonaLive/raw/main/assets/highlight.svg)](/GVCLab/PersonaLive/blob/main/assets/highlight.svg)

[![](/GVCLab/PersonaLive/raw/main/assets/demo_3.gif)](/GVCLab/PersonaLive/blob/main/assets/demo_3.gif) ¬†¬† [![](/GVCLab/PersonaLive/raw/main/assets/demo_2.gif)](/GVCLab/PersonaLive/blob/main/assets/demo_2.gif)

## üìã TODO

* [ ]  If you find PersonaLive useful or interesting, please give us a Starüåü! Your support drives us to keep improving.
* [ ]  Fix bugs (If you encounter any issues, please feel free to open an issue or contact me! üôè)
* [ ]  Enhance WebUI (Support reference image replacement).
* [x]  **[2025.12.22]** üî• Supported streaming strategy in offline inference to generate long videos on 12GB VRAM!
* [x]  **[2025.12.17]** üî• [ComfyUI-PersonaLive](https://github.com/okdalto/ComfyUI-PersonaLive) is now supported! (Thanks to [@okdalto](https://github.com/okdalto))
* [x]  **[2025.12.15]** üî• Release `paper`!
* [x]  **[2025.12.12]** üî• Release `inference code`, `config`, and `pretrained weights`!

## ‚öôÔ∏è Framework

[![Image 1](/GVCLab/PersonaLive/raw/main/assets/overview.png)](/GVCLab/PersonaLive/blob/main/assets/overview.png)

We present PersonaLive, a `real-time` and `streamable` diffusion framework capable of generating `infinite-length` portrait animations on a single `12GB GPU`.

## üöÄ Getting Started

### üõ† Installation

```
# clone this repo
git clone https://github.com/GVCLab/PersonaLive
cd PersonaLive

# Create conda environment
conda create -n personalive python=3.10
conda activate personalive

# Install packages with pip
pip install -r requirements_base.txt
```

### ‚è¨ Download weights

Option 1: Download pre-trained weights of base models and other components ([sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers) and [sd-vae-ft-mse](https://huggingface.co/stabilityai/sd-vae-ft-mse)). You can run the following command to download weights automatically:

```
python tools/download_weights.py
```

Option 2: Download pre-trained weights into the `./pretrained_weights` folder from one of the below URLs:

[![](https://camo.githubusercontent.com/c679bbe6325703d96975da92d55e8caefd5353c0520dd34f0c4c50ade7d47e91/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f476f6f676c6525323044726976652d3542384445463f7374796c653d666f722d7468652d6261646765266c6f676f3d676f6f676c656472697665266c6f676f436f6c6f723d7768697465)](https://drive.google.com/drive/folders/1GOhDBKIeowkMpBnKhGB8jgEhJt_--vbT?usp=drive_link) [![](https://camo.githubusercontent.com/de67faea98961fcdaaf2c9b44efbedf6d222da4e06ca6f4679739fbed44a5537/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f42616964752532304e65746469736b2d3345344138393f7374796c653d666f722d7468652d6261646765266c6f676f3d6261696475266c6f676f436f6c6f723d7768697465)](https://pan.baidu.com/s/1DCv4NvUy_z7Gj2xCGqRMkQ?pwd=gj64) [![](https://camo.githubusercontent.com/7ef50183781a34919a882ad45c3cd263dbe1d142c95d5ce507ab94816e31e129/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d6f64656c53636f70652d3632344146463f7374796c653d666f722d7468652d6261646765266c6f676f3d616c6962616261636c6f7564266c6f676f436f6c6f723d7768697465)](https://modelscope.cn/models/huaichang/PersonaLive) [![](https://camo.githubusercontent.com/c6464eaffe903a035ae7191d04e45e7f042bebb4ed92782b6968123e111b97cf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f48756767696e67466163652d4536374532323f7374796c653d666f722d7468652d6261646765266c6f676f3d68756767696e6766616365266c6f676f436f6c6f723d7768697465)](https://huggingface.co/huaichang/PersonaLive)

Finally, these weights should be organized as follows:

```
pretrained_weights
‚îú‚îÄ‚îÄ onnx
‚îÇ   ‚îú‚îÄ‚îÄ unet_opt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unet_opt.onnx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ unet_opt.onnx.data
‚îÇ   ‚îî‚îÄ‚îÄ unet
‚îú‚îÄ‚îÄ personalive
‚îÇ   ‚îú‚îÄ‚îÄ denoising_unet.pth
‚îÇ   ‚îú‚îÄ‚îÄ motion_encoder.pth
‚îÇ   ‚îú‚îÄ‚îÄ motion_extractor.pth
‚îÇ   ‚îú‚îÄ‚îÄ pose_guider.pth
‚îÇ   ‚îú‚îÄ‚îÄ reference_unet.pth
‚îÇ   ‚îî‚îÄ‚îÄ temporal_module.pth
‚îú‚îÄ‚îÄ sd-vae-ft-mse
‚îÇ   ‚îú‚îÄ‚îÄ diffusion_pytorch_model.bin
‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îú‚îÄ‚îÄ sd-image-variations-diffusers
‚îÇ   ‚îú‚îÄ‚îÄ image_encoder
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pytorch_model.bin
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ unet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ diffusion_pytorch_model.bin
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.json
‚îÇ   ‚îî‚îÄ‚îÄ model_index.json
‚îî‚îÄ‚îÄ tensorrt
    ‚îî‚îÄ‚îÄ unet_work.engine
```

### üéûÔ∏è Offline Inference

Run offline inference with the default configuration:

```
python inference_offline.py
```

* `-L`: Max number of frames to generate. (Default: 100)
* `--use_xformers`: Enable xFormers memory efficient attention. (Default: True)
* `--stream_gen`: Enable streaming generation strategy. (Default: True)
* `--reference_image`: Path to a specific reference image. Overrides settings in config.
* `--driving_video`: Path to a specific driving video. Overrides settings in config.

‚ö†Ô∏è Note for RTX 50-Series (Blackwell) Users: xformers is not yet fully compatible with the new architecture. To avoid crashes, please disable it by running:

```
python inference_offline.py --use_xformers False
```

### üì∏ Online Inference

#### üì¶ Setup Web UI

```
# install Node.js 18+
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
nvm install 18

cd webcam
source start.sh
```

#### üèéÔ∏è Acceleration (Optional)

Converting the model to TensorRT can significantly speed up inference (~ 2x ‚ö°Ô∏è). Building the engine may take about `20 minutes` depending on your device. Note that TensorRT optimizations may lead to slight variations or a small drop in output quality.

```
# Install packages with pip
pip install -r requirements_trt.txt

# Converting the model to TensorRT
python torch2trt.py
```

‚ö†Ô∏è The provided TensorRT model is from an `H100`. We recommend `ALL users` (including H100 users) re-run `python torch2trt.py` locally to ensure best compatibility.

#### ‚ñ∂Ô∏è Start Streaming

```
python inference_online.py --acceleration none (for RTX 50-Series) or xformers or tensorrt
```

Then open `http://0.0.0.0:7860` in your browser. (\*If `http://0.0.0.0:7860` does not work well, try `http://localhost:7860`)

**How to use**: Upload Image ‚û°Ô∏è Fuse Reference ‚û°Ô∏è Start Animation ‚û°Ô∏è Enjoy! üéâ

[![PersonaLive](/GVCLab/PersonaLive/raw/main/assets/guide.png)](/GVCLab/PersonaLive/blob/main/assets/guide.png)

**Regarding Latency**: Latency varies depending on your device's computing power. You can try the following methods to optimize it:

1. Lower the "Driving FPS" setting in the WebUI to reduce the computational workload.
2. You can increase the multiplier (e.g., set to `num_frames_needed * 4` or higher) to better match your device's inference speed.

   [PersonaLive/webcam/util.py](https://github.com/GVCLab/PersonaLive/blob/6953d1a8b409f360a3ee1d7325093622b29f1e22/webcam/util.py#L73)

   Line 73
   in
   [6953d1a](/GVCLab/PersonaLive/commit/6953d1a8b409f360a3ee1d7325093622b29f1e22)

   |  |  |
   | --- | --- |
   |  | read\_size = min(queue.qsize(), num\_frames\_needed \* 3) |

## üìö Community Contribution

Special thanks to the community for providing helpful setups! ü•Ç

* **Windows + RTX 50-Series Guide**: Thanks to [@dknos](https://github.com/dknos) for providing a [detailed guide](https://github.com/GVCLab/PersonaLive/issues/10#issuecomment-3662785532) on running this project on Windows with Blackwell GPUs.
* **TensorRT on Windows**: If you are trying to convert TensorRT models on Windows, [this discussion](https://github.com/GVCLab/PersonaLive/issues/8) might be helpful. Special thanks to [@MaraScott](https://github.com/MaraScott) and [@Jeremy8776](https://github.com/Jeremy8776) for their insights.
* **ComfyUI**: Thanks to [@okdalto](https://github.com/okdalto) for helping implement the [ComfyUI-PersonaLive](https://github.com/okdalto/ComfyUI-PersonaLive) support.
* **Useful Scripts**: Thanks to [@suruoxi](https://github.com/suruoxi) for implementing `download_weights.py`, and to [@andchir](https://github.com/andchir) for adding audio merging functionality.

## üé¨ More Results

#### üëÄ Visualization results

|  |  |
| --- | --- |
| demo\_1.mp4 | demo\_2.mp4 |

|  |  |  |  |
| --- | --- | --- | --- |
| demo\_3.mp4 | demo\_4.mp4 | demo\_5.mp4 | demo\_6.mp4 |
| demo\_7.mp4 | demo\_8.mp4 | demo\_9.mp4 | demo\_0.mp4 |

#### ü§∫ Comparisons

|  |
| --- |
| same\_id.mp4 |
| cross\_id\_1.mp4 |
| cross\_id\_2.mp4 |

## ‚≠ê Citation

If you find PersonaLive useful for your research, welcome to cite our work using the following BibTeX:

```
@article{li2025personalive,
  title={PersonaLive! Expressive Portrait Image Animation for Live Streaming},
  author={Li, Zhiyuan and Pun, Chi-Man and Fang, Chen and Wang, Jue and Cun, Xiaodong},
  journal={arXiv preprint arXiv:2512.11253},
  year={2025}
}
```

## ‚ù§Ô∏è Acknowledgement

This code is mainly built upon [Moore-AnimateAnyone](https://github.com/MooreThreads/Moore-AnimateAnyone), [X-NeMo](https://byteaigc.github.io/X-Portrait2/), [StreamDiffusion](https://github.com/cumulo-autumn/StreamDiffusion), [RAIN](https://pscgylotti.github.io/pages/RAIN/) and [LivePortrait](https://github.com/KlingTeam/LivePortrait), thanks to their invaluable contributions.

## About

PersonaLive! : Expressive Portrait Image Animation for Live Streaming

[arxiv.org/abs/2512.11253](https://arxiv.org/abs/2512.11253 "https://arxiv.org/abs/2512.11253")

### Resources

[Readme](#readme-ov-file)

### License

[Apache-2.0 license](#Apache-2.0-1-ov-file)

### Uh oh!

There was an error while loading. Please reload this page.

[Activity](/GVCLab/PersonaLive/activity)

[Custom properties](/GVCLab/PersonaLive/custom-properties)

### Stars

[**980**
stars](/GVCLab/PersonaLive/stargazers)

### Watchers

[**11**
watching](/GVCLab/PersonaLive/watchers)

### Forks

[**115**
forks](/GVCLab/PersonaLive/forks)

[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FGVCLab%2FPersonaLive&report=GVCLab+%28user%29)

## [Releases](/GVCLab/PersonaLive/releases)

No releases published

## [Packages 0](/orgs/GVCLab/packages?repo_name=PersonaLive)

No packages published

## [Contributors 4](/GVCLab/PersonaLive/graphs/contributors)

* [![@huai-chang](https://avatars.githubusercontent.com/u/71637017?s=64&v=4)](https://github.com/huai-chang)

  [**huai-chang**
  Zhiyuan Li](https://github.com/huai-chang)
* [![@vinthony](https://avatars.githubusercontent.com/u/4397546?s=64&v=4)](https://github.com/vinthony)

  [**vinthony**
  Xiaodong Cun](https://github.com/vinthony)
* [![@MaraScott](https://avatars.githubusercontent.com/u/482210?s=64&v=4)](https://github.com/MaraScott)

  [**MaraScott**
  MaraScott](https://github.com/MaraScott)
* [![@suruoxi](https://avatars.githubusercontent.com/u/7591860?s=64&v=4)](https://github.com/suruoxi)

  [**suruoxi**
  Peng Zhang](https://github.com/suruoxi)

## Languages

* [Python
  93.0%](/GVCLab/PersonaLive/search?l=python)
* [Svelte
  4.5%](/GVCLab/PersonaLive/search?l=svelte)
* [TypeScript
  2.2%](/GVCLab/PersonaLive/search?l=typescript)
* [JavaScript
  0.2%](/GVCLab/PersonaLive/search?l=javascript)
* [HTML
  0.1%](/GVCLab/PersonaLive/search?l=html)
* [Shell
  0.0%](/GVCLab/PersonaLive/search?l=shell)

## Footer

¬© 2025 GitHub,¬†Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Community](https://github.community/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You can‚Äôt perform that action at this time.
