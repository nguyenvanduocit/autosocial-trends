---
source: hackernews
title: Local AI is driving the biggest change in laptops in decades
url: https://spectrum.ieee.org/ai-models-locally
date: 2025-12-24
---

[IEEE.org](https://www.ieee.org/)[IEEE Xplore Digital Library](https://ieeexplore.ieee.org/Xplore/home.jsp)[IEEE Standards](https://standards.ieee.org/)[More Sites](https://www.ieee.org/sitemap.html)

[Sign In](https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&sourceCode=spectrum&signinurl=https://spectrum.ieee.org/core/saml/main/login&url=https://spectrum.ieee.org/&autoSignin=Y&car=IEEE-Spectrum)[Join IEEE](/join)

Your Laptop Isn’t Ready for LLMs. That’s About to Change

Share

FOR THE TECHNOLOGY INSIDER

Search:

Explore by topic

[Aerospace](/topic/aerospace/)[AI](/topic/artificial-intelligence/)[Biomedical](/topic/biomedical/)[Climate Tech](/topic/climate-tech/)[Computing](/topic/computing/)[Consumer Electronics](/topic/consumer-electronics/)[Energy](/topic/energy/)[History of Technology](/topic/tech-history/)[Robotics](/topic/robotics/)[Semiconductors](/topic/semiconductors/)[Telecommunications](/topic/telecommunications/)[Transportation](/topic/transportation/)

[IEEE Spectrum](/)

FOR THE TECHNOLOGY INSIDER

### Topics

[Aerospace](/topic/aerospace/)[AI](/topic/artificial-intelligence/)[Biomedical](/topic/biomedical/)[Climate Tech](/topic/climate-tech/)[Computing](/topic/computing/)[Consumer Electronics](/topic/consumer-electronics/)[Energy](/topic/energy/)[History of Technology](/topic/tech-history/)[Robotics](/topic/robotics/)[Semiconductors](/topic/semiconductors/)[Telecommunications](/topic/telecommunications/)[Transportation](/topic/transportation/)

### Sections

[Features](/type/feature/)[News](/type/news/)[Opinion](/type/opinion/)[Careers](/topic/careers/)[DIY](/topic/diy/)[Engineering Resources](/engineering-resources/)

### More

[Newsletters](/newsletters/)[Special Reports](/special-reports/)[Collections](/collections/)[Explainers](/type/explainer/)[Top Programming Languages](/top-programming-languages)[Robots Guide ↗](https://robotsguide.com)[IEEE Job Site ↗](https://jobs.ieee.org/)

### For IEEE Members

[Current Issue](/magazine/current-issue)[Magazine Archive](/magazine/)[The Institute](/the-institute/)[The Institute Archive](/the-institute/ti-archive/)

### For IEEE Members

[Current Issue](/magazine/current-issue)[Magazine Archive](/magazine/)[The Institute](/the-institute/)[The Institute Archive](/the-institute/ti-archive/)

### IEEE Spectrum

[About Us](/about)[Contact Us](/contact)[Reprints & Permissions ↗](https://www.parsintl.com/publications/ieee-media/)[Advertising ↗](https://advertise.ieee.org/%20)

### Follow IEEE Spectrum

### Support IEEE Spectrum

*IEEE Spectrum* is the flagship publication of the IEEE — the world’s largest professional organization devoted to engineering and applied sciences. Our articles, videos, and infographics inform our readers about developments in technology, engineering, and science.

[Subscribe](https://ieee.dragonforms.com/spectrum_subscribe?utm_source=ieee-spectrum&utm_medium=nav&utm_content=hugemenu)

[About IEEE](https://www.ieee.org/about/)[Contact & Support](https://www.ieee.org/about/contact.html)[Accessibility](https://www.ieee.org/accessibility-statement.html)[Nondiscrimination Policy](https://www.ieee.org/about/corporate/governance/p9-26.html)[Terms](https://www.ieee.org/about/help/site-terms-conditions.html)[IEEE Privacy Policy](https://www.ieee.org/security-privacy.html)Cookie Preferences[Ad Privacy Options](https://spectrum.ieee.org/st/ppid-info)

© Copyright 2025 IEEE — All rights reserved. A public charity, IEEE is the world's largest technical professional organization dedicated to advancing technology for the benefit of humanity.

## Enjoy more free content and benefits by creating an account

## Saving articles to read later requires an IEEE Spectrum account

## The Institute content is only available for members

## Downloading full PDF issues is exclusive for IEEE Members

## Downloading this e-book is exclusive for IEEE Members

## Access to *Spectrum* 's Digital Edition is exclusive for IEEE Members

## Following topics is a feature exclusive for IEEE Members

## Adding your response to an article requires an IEEE Spectrum account

## Create an account to access more content and features on *IEEE Spectrum* , including the ability to save articles to read later, download Spectrum Collections, and participate in conversations with readers and editors. For more exclusive content and features, consider [Joining IEEE](/join) .

## Join the world’s largest professional organization devoted to engineering and applied sciences and get access to all of Spectrum’s articles, archives, PDF downloads, and other benefits. [Learn more about IEEE →](/join)

## Join the world’s largest professional organization devoted to engineering and applied sciences and get access to this e-book plus all of *IEEE Spectrum’s* articles, archives, PDF downloads, and other benefits. [Learn more about IEEE →](/join)

[CREATE AN ACCOUNT](https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&sourceCode=spectrum&signinurl=https%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fsaml%2Fmain%2Flogin%3Fnext_url%3Dhttps%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fintegrations%2Fieee%2Fchanges%0A&url=https://spectrum.ieee.org/&autoSignin=Y&car=IEEE-Spectrum)[SIGN IN](/core/saml/main/login?next_url=https://spectrum.ieee.org/core/integrations/ieee/changes)

[JOIN IEEE](https://www.ieee.org/membership-application/public/join.html?promo=JOINLITE&style=SPECTRUM&joinlite=TRUE)[SIGN IN](/core/saml/main/login?next_url=https://spectrum.ieee.org/core/integrations/ieee/changes)

Close

## Access Thousands of Articles — Completely Free

## Create an account and get exclusive content and features: **Save articles, download collections,** and **post comments** — all free! For full access and benefits, [subscribe](https://ieee.dragonforms.com/spectrum_subscribe) to *Spectrum*.

[CREATE AN ACCOUNT](https://www.ieee.org/profile/public/createwebaccount/showCreateAccount.html?ShowMGAMarkeatbilityOptIn=true&sourceCode=spectrum3c&signinurl=https%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fsaml%2Fmain%2Flogin%3Fnext_url%3Dhttps%3A%2F%2Fspectrum.ieee.org%2Fcore%2Fintegrations%2Fieee%2Fchanges%0A&url=https://spectrum.ieee.org/&autoSignin=Y&car=IEEE-Spectrum)[SIGN IN](/core/saml/main/login?next_url=https://spectrum.ieee.org/core/integrations/ieee/changes)

[Computing](https://spectrum.ieee.org/topic/computing/)[AI](https://spectrum.ieee.org/topic/artificial-intelligence/)[Magazine](https://spectrum.ieee.org/magazine/)[Feature](https://spectrum.ieee.org/type/feature/)[December 2025](https://spectrum.ieee.org/magazine/2025/december/)

# Your Laptop Isn’t Ready for LLMs. That’s About to Change

## Local AI is driving the biggest change in laptops in decades

[Matthew S. Smith](https://spectrum.ieee.org/u/matthew-s-smith)

17 Nov 2025

10 min read

Vertical

![Hands typing on laptop with a building scaffold shaped like a face on the screen.](https://spectrum.ieee.org/media-library/hands-typing-on-laptop-with-a-building-scaffold-shaped-like-a-face-on-the-screen.jpg?id=62173854&width=1200&height=1587)

Dan Page

Blue

**Odds are the PC in** your office today isn’t ready to run AI [large language models](https://spectrum.ieee.org/large-language-model-performance) (LLMs).

Today, most users interact with LLMs via an online, browser-based interface. The more technically inclined might use an application [programming](https://spectrum.ieee.org/tag/programming) interface or command line interface. In either case, the queries are sent to a [data center](https://spectrum.ieee.org/dcflex-data-center-flexibility), where the model is hosted and run. It works well, until it doesn’t; a data-center outage can take a model offline for hours. Plus, some users might be unwilling to send [personal data](https://spectrum.ieee.org/tag/personal-data) to an anonymous entity.

Running a model locally on your computer could offer significant benefits: lower latency, better understanding of your personal needs, and the privacy that comes with keeping your data on your own machine.

However, for the average laptop that’s over a year old, the number of useful [AI models](https://spectrum.ieee.org/tag/ai-models) you can run locally on your PC is close to zero. This laptop might have a four- to eight-core processor ([CPU](https://en.wikipedia.org/wiki/Central_processing_unit)), no dedicated [graphics](https://spectrum.ieee.org/tag/graphics) chip ([GPU](https://en.wikipedia.org/wiki/Graphics_processing_unit)) or neural-processing unit ([NPU](https://en.wikipedia.org/wiki/Neural_processing_unit)), and 16 gigabytes of [RAM](https://en.wikipedia.org/wiki/Random-access_memory), leaving it underpowered for LLMs.

Even new, high-end PC [laptops](https://spectrum.ieee.org/tag/laptops), which often include an NPU and a GPU, can struggle. The largest AI models have over a trillion parameters, which requires memory in [the hundreds of gigabytes](https://snowkylin.github.io/blogs/a-note-on-deepseek-r1.html#:~:text=Note-,Models,Studio%20(%245.6k)!). Smaller versions of these models are available, even prolific, but they often lack the intelligence of larger models, which only dedicated AI [data centers](https://spectrum.ieee.org/tag/data-centers) can handle.

The situation is even worse when other AI features aimed at making the model more capable are considered. [Small language models (SLMs)](https://huggingface.co/blog/jjokah/small-language-model) that run on local hardware either scale back these features or omit them entirely. Image and video generation are difficult to run locally on laptops, too, and until recently they were reserved for high-end tower desktop PCs.

That’s a problem for AI adoption.

To make running AI models locally possible, the hardware found inside laptops and the software that runs on it will need an upgrade. This is the beginning of a shift in laptop design that will give engineers the opportunity to abandon the last vestiges of the past and reinvent the PC from the ground up.

## NPUs enter the chat

The most obvious way to boost a PC’s AI performance is to place a powerful NPU alongside the CPU.

An NPU is a specialized chip [designed for the matrix multiplication calculations](https://penntoday.upenn.edu/what-is-an-NPU-in-computing) that most AI models rely on. These matrix operations are highly parallelized, which is why [GPUs](https://spectrum.ieee.org/tag/gpus) (which were already better at highly parallelized tasks than CPUs) became the go-to option for AI data centers.

However, because NPUs are designed specifically to handle these matrix operations—and not other tasks, like 3D graphics—[they’re more power efficient than GPUs](https://www.ibm.com/think/topics/npu-vs-gpu). That’s important for accelerating AI on portable consumer technology. NPUs also tend to provide better support for low-precision arithmetic than laptop GPUs. AI models often use low-precision arithmetic to reduce computational and memory needs on portable hardware, such as laptops.

### Laptops Are Being Rebuilt to Run LLMs

![Open laptop showing numbered internal components and hands at edges, with a screwdriver nearby.](data:image/svg+xml...)

Your laptop today is probably not equipped to run large language models. But future laptops might. Chasing the dream of locally run LLMs, laptop architects are rethinking many aspects of current designs, leading to changes that are only now starting to take hold.

iStockphoto

1. **Addition of NPUs.** Neural processing units (NPUs)—specialized accelerator chips that can run large language models (LLMs) and other AI models faster than CPUs and GPUs can—are being incorporated into laptops.

2. **Addition of more—and faster—memory.** The largest language models take up hundreds of gigabytes of memory. To host these models, and serve them quickly to the number-crunching processing units, laptops are increasing their memory capacity and speed.

3. **Consolidation of memory.** Most laptops today have a divided memory architecture, with a separate pool of memory to serve the GPUs. This made sense when the design first came out: GPUs needed faster memory access than could be supplied by the common bus. Now, to feed AI’s data appetite, laptop architects are rethinking this decision, and now they’re pooling memory together with faster [interconnects](https://spectrum.ieee.org/tag/interconnects).

4. **Combination of chips on the same silicon.** To help shorten the path to pooled memory, all the processing units—CPUs, GPUs, and NPUs—are now being integrated into the same silicon chip. This helps them connect to one another and to memory, but it will make maintenance more challenging.

5. **[Power management](https://spectrum.ieee.org/tag/power-management).** AI models can see heavy use when they power always-on features like Microsoft’s Windows Recall, or the AI-powered Windows Search. Power-sipping NPUs help laptops run these models without excessive battery drain.

“With the NPU, the entire structure is really designed around the data type of tensors [a multidimensional array of numbers],” said [Steven Bathiche](https://www.microsoft.com/applied-sciences/people/steven-bathiche), technical fellow at [Microsoft](https://spectrum.ieee.org/tag/microsoft). “NPUs are much more specialized for that workload. And so we go from a CPU that can handle three [trillion] operations per second (TOPS), to an NPU” in [Qualcomm’s](https://www.qualcomm.com) [Snapdragon](https://spectrum.ieee.org/tag/snapdragon) X chip, which can power [Microsoft’s](https://www.microsoft.com/en-us/) [Copilot+](https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/) features. This includes [Windows Recall](https://support.microsoft.com/en-us/windows/retrace-your-steps-with-recall-aa03f8a0-a78b-4b3e-b0a1-2eb8ac48701c), which uses AI to create a searchable timeline of a user’s usage history by analyzing screenshots, and [Windows Photos’ Generative erase](https://blogs.windows.com/windows-insider/2024/02/22/windows-photos-gets-generative-erase-and-recent-ai-editing-features-now-available-on-arm64-devices-and-windows-10/), which can remove the background or specific objects from an image.

While [Qualcomm](https://qualcomm.com) was arguably the first to provide an NPU for Windows laptops, it kickstarted an NPU TOPS arms race that also includes [AMD](https://www.amd.com/en.html) and [Intel](https://www.intel.com/content/www/us/en/homepage.html), and the competition is already pushing NPU performance upward.

In 2023, prior to Qualcomm’s Snapdragon X, [AMD](https://spectrum.ieee.org/tag/amd) chips with NPUs were uncommon, and those that existed delivered about 10 TOPS. Today, AMD and [Intel](https://spectrum.ieee.org/tag/intel) have NPUs that are competitive with Snapdragon, [providing 40 to 50 TOPS](https://www.pcworld.com/article/2806864/intel-vs-amd-vs-qualcomm-which-copilot-pc-cpu-is-best-for-you.html).

[Dell’s upcoming Pro Max Plus AI PC](https://www.lifewire.com/dell-pro-max-plus-ai-laptop-11739880) will up the ante with a [Qualcomm](https://spectrum.ieee.org/tag/qualcomm) AI 100 NPU that promises up to 350 TOPS, improving performance by a staggering 35 times compared with that of the best available NPUs just a few years ago. Drawing that line up and to the right implies that NPUs capable of thousands of TOPS are just a couple of years away.

How many TOPS do you need to run state-of-the-art models with hundreds of millions of parameters? No one knows exactly. It’s not possible to run these models on today’s consumer hardware, so real-world tests just can’t be done. But it stands to reason that we’re within throwing distance of those capabilities. It’s also worth noting that LLMs are not the only use case for NPUs. [Vinesh Sukumar](https://www.qualcomm.com/news/onq/2024/05/from-olympic-table-tennis-to-ai-product-manager-meet-dr-vinesh-sukumar), Qualcomm’s head of AI and [machine learning](https://spectrum.ieee.org/tag/machine-learning) product management, says [AI image generation](https://spectrum.ieee.org/tag/ai-image-generation) and manipulation is an example of a task that’s difficult without an NPU or high-end GPU.

## Building balanced chips for better AI

Faster NPUs will handle more tokens per second, which in turn will deliver a faster, more fluid experience when using AI models. Yet there’s more to running AI on local hardware than throwing a bigger, better NPU at the problem.

[Mike Clark](https://ieeexplore.ieee.org/author/37089001134), corporate fellow design engineer at AMD, says that companies that design chips to accelerate AI on the PC can’t put all their bets on the NPU. That’s in part because AI isn’t a replacement for, but rather an addition to, the tasks a PC is expected to handle.

“We must be good at low latency, at handling smaller data types, at branching code—traditional workloads. We can’t give that up, but we still want to be good at AI,” says Clark. He also noted that “the CPU is used to prepare data” for AI workloads, which means an inadequate CPU could become a bottleneck.

NPUs must also compete or cooperate with GPUs. On the PC, that often means a high-end AMD or [Nvidia](https://spectrum.ieee.org/tag/nvidia) GPU with large amounts of built-in memory. The [Nvidia GeForce RTX 5090](https://signal65.com/research/ai/nvidia-geforce-rtx-5090-founders-edition-performance-analysis/)’s specifications quote an AI performance up to 3,352 TOPS, which leaves even the Qualcomm AI 100 in the dust.

That comes with a big caveat, however: power. Though extremely capable, the RTX 5090 is designed to draw up to [575 watts](https://signal65.com/research/ai/nvidia-geforce-rtx-5090-founders-edition-performance-analysis/) on its own. Mobile versions for laptops are more miserly but still draw up to [175 W](https://www.notebookcheck.net/Nvidia-GeForce-RTX-5090-Laptop-Benchmarks-and-Specs.934947.0.html), which can quickly drain a laptop battery.

[Simon Ng](https://www.linkedin.com/in/simonng39/?originalSubdomain=ca), client AI product manager at Intel, says the company is “seeing that the NPU will just do things much more efficiently at lower power.” [Rakesh Anigundi](https://www.linkedin.com/in/rakesh-s-anigundi/), AMD’s director of product management for Ryzen AI, agrees. He adds that [low-power](https://spectrum.ieee.org/tag/low-power) operation is particularly important because AI workloads tend to take longer to run than other demanding tasks, like encoding a video or rendering graphics. “You’ll want to be running this for a longer period of time, such as an AI personal assistant, which could be always active and listening for your command,” he says.

These competing priorities mean chip architects and system designers will need to make tough calls about how to allocate silicon and power in AI PCs, especially those that often rely on battery power, such as laptops.

“We have to be very deliberate in how we design our [system-on-a-chip](https://spectrum.ieee.org/tag/system-on-a-chip) to ensure that a larger [SoC](https://spectrum.ieee.org/tag/soc) can perform to our requirements in a thin and light form factor,” said [Mahesh Subramony](https://www.linkedin.com/in/mahesh-subramony-a0ba60/), senior fellow design engineer at AMD.

## When it comes to AI, memory matters

Squeezing an NPU alongside a CPU and GPU will improve the average PC’s performance in AI tasks, but it’s not the only revolutionary change AI will force on PC architecture. There’s another that’s perhaps even more fundamental: memory.

Most modern PCs have a divided memory architecture [rooted in decisions made over 25 years ago](https://www.electronicdesign.com/technologies/embedded/article/55300900/jon-peddie-research-what-came-before-pcie-the-evolution-of-pc-graphics-buses). Limitations in bus bandwidth led GPUs (and other add-in cards that might require high-bandwidth memory) to move away from accessing a PC’s system memory and instead rely on the GPU’s own dedicated memory. As a result, powerful PCs typically have two pools of memory, system memory and graphics memory, which operate independently.

That’s a problem for AI. Models require large amounts of memory, and the entire model must load into memory at once. The legacy PC architecture, which splits memory between the system and the GPU, is at odds with that requirement.

“When I have a discrete GPU, I have a separate memory subsystem hanging off it,” explained [Joe Macri,](https://www.linkedin.com/in/joseph-macri-9a288a55/)  vice president and chief technology officer at AMD. “When I want to share data between our [CPU] and GPU, I’ve got to take the data out of my memory, slide it across the PCI Express bus, put it in the GPU memory, do my processing, then move it all back.” Macri said this increases power draw and leads to a sluggish [user experience](https://spectrum.ieee.org/tag/user-experience).

The solution is a unified memory architecture that provides all system resources access to the same pool of memory over a fast, interconnected memory bus. Apple’s in-house silicon is perhaps the most well-known recent example of a chip with a unified memory architecture. However, unified memory is otherwise rare in modern PCs.

AMD is following suit in the laptop space. The company announced a new line of APUs targeted at high-end laptops, [Ryzen AI Max](https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html), at [CES](https://spectrum.ieee.org/tag/ces) ([Consumer Electronics](https://spectrum.ieee.org/topic/consumer-electronics/) Show) 2025.

Ryzen AI Max places the company’s Ryzen CPU cores on the same silicon as Radeon-branded GPU cores, plus an NPU rated at 50 TOPS, on a single piece of silicon with a unified memory architecture. Because of this, the CPU, GPU, and NPU can all access up to a maximum of [128 GB of system memory](https://www.amd.com/en/developer/resources/technical-articles/2025/amd-ryzen-ai-max-395--a-leap-forward-in-generative-ai-performanc.html), which is shared among all three. AMD believes this strategy is ideal for memory and performance management in consumer PCs. “By bringing it all under a single thermal head, the entire power envelope becomes something that we can manage,” said Subramony.

The Ryzen AI Max is already available in several laptops, including [the HP Zbook Ultra G1a](https://www.pcworld.com/article/2650073/hands-on-the-hp-zbook-ultra-g1a-smashes-the-work-laptop-paradigm.html) and the [Asus ROG Flow Z13](https://rog.asus.com/laptops/rog-flow/rog-flow-z13-2025/). It also powers the [Framework Desktop](https://frame.work/marketplace/desktops) and several mini desktops from less well-known brands, such as the [GMKtec EVO-X2 AI mini PC](https://www.gmktec.com/products/amd-ryzen%E2%84%A2-ai-max-395-evo-x2-ai-mini-pc?srsltid=AfmBOooME4uCrsnIh5mOf98eGHteIzsi-DAPl6E5xhNrTzG94qr3Tjf6).

Intel and Nvidia will also join this party, though in an unexpected way. In September, the former rivals announced an alliance to sell chips that pair Intel CPU cores with Nvidia GPU cores. While the details are still under wraps, the chip architecture will likely include unified memory and an Intel NPU.

Chips like these stand to drastically change PC architecture if they catch on. They’ll offer access to much larger pools of memory than before and integrate the CPU, GPU, and NPU into one piece of silicon that can be closely monitored and controlled. These factors should make it easier to shuffle an AI workload to the hardware best suited to execute it at a given moment.

Unfortunately, they’ll also make PC upgrades and repairs more difficult, as chips with a unified memory architecture typically bundle the CPU, GPU, NPU, and memory into a single, physically inseparable package on a PC mainboard. That’s in contrast with traditional PCs, where the CPU, GPU, and memory can be replaced individually.

## Microsoft’s bullish take on AI is rewriting Windows

MacOS is well regarded for its attractive, intuitive [user interface](https://spectrum.ieee.org/tag/user-interface), and Apple Silicon chips have a unified memory architecture that can prove useful for AI. However, Apple’s GPUs aren’t as capable as the best ones used in PCs, and its AI tools for developers are less widely adopted.

[Chrissie Cremers](https://www.linkedin.com/in/chrissiecremers/?originalSubdomain=nl), cofounder of the AI-focused marketing firm Aigency Amsterdam, told me earlier this year that although she prefers macOS, her agency doesn’t use Mac computers for AI work. “The GPU in my Mac desktop can hardly manage [our AI workflow], and it’s not an old computer,” she said. “I’d love for them to catch up here, because they used to be the creative tool.”

![Laptop beneath glass dome shaped like human head on striped orange and blue background.](data:image/svg+xml...)  Dan Page

That leaves an opening for competitors to become the go-to choice for AI on the PC—and Microsoft knows it.

Microsoft launched [Copilot+ PCs](https://blogs.microsoft.com/blog/2024/05/20/introducing-copilot-pcs/) at the company’s 2024 Build developer conference. The launch had problems, most notably the [botched](https://www.theverge.com/2024/6/13/24178144/microsoft-windows-ai-recall-feature-delay) release of its key feature, [Windows Recall](https://spectrum.ieee.org/microsoft-copilot), which uses AI to help users search through anything they’ve seen or heard on their PC. Still, the launch was successful in pushing the PC industry toward NPUs, as AMD and Intel both introduced new laptop chips with upgraded NPUs in late 2024.

At Build 2025, Microsoft also revealed [Windows’ AI Foundry Local](https://devblogs.microsoft.com/foundry/foundry-local-a-new-era-of-edge-ai/), a “runtime stack” that includes a catalog of popular open-source [large language models](https://spectrum.ieee.org/tag/large-language-models). While Microsoft’s own models are available, [the catalog includes thousands of open-source models](https://azure.microsoft.com/en-us/products/ai-model-catalog#tabs-pill-bar-oc92d8_tab0) from [Alibaba](https://spectrum.ieee.org/tag/alibaba), DeepSeek, [Meta](https://spectrum.ieee.org/tag/meta), Mistral AI, Nvidia, [OpenAI](https://spectrum.ieee.org/tag/openai), Stability AI, xAI, and more.

Once a model is selected and implemented into an app, Windows executes AI tasks on local hardware through the Windows ML runtime, which automatically directs AI tasks to the CPU, GPU, or NPU hardware best suited for the job.

AI [Foundry](https://spectrum.ieee.org/tag/foundry) also provides APIs for local knowledge retrieval and low-rank adaptation (LoRA), advanced features that let developers customize the data an AI model can reference and how it responds. Microsoft also announced support for on-device semantic search and retrieval-augmented generation, features that help developers build AI tools that reference specific on-device information.

“[AI Foundry] is about being smart. It’s about using all the [processors](https://spectrum.ieee.org/tag/processors) at hand, being efficient, and prioritizing workloads across the CPU, the NPU, and so on. There’s a lot of opportunity and runway to improve,” said Bathiche.

### Toward [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) on PCs

The rapid evolution of AI-capable PC hardware represents more than just an incremental upgrade. It signals a coming shift in the PC industry that’s likely to wipe away the last vestiges of the PC architectures designed in the ’80s, ’90s, and early 2000s.

The combination of increasingly powerful NPUs, unified memory architectures, and sophisticated software-optimization techniques is closing the performance gap between local and cloud-based AI at a pace that has surprised even industry insiders, such as Bathiche.

It will also nudge chip designers toward ever-more-integrated chips that have a unified memory subsystem and to bring the CPU, GPU, and NPU onto a single piece of silicon—even in high-end laptops and desktops. AMD’s Subramony said the goal is to have users “carrying a mini workstation in your hand, whether it’s for AI workloads, or for high compute. You won’t have to go to the cloud.”

A change that massive won’t happen overnight. Still, it’s clear that many in the PC industry are committed to reinventing the computers we use every day in a way that optimizes for AI. Qualcomm’s Vinesh Sukumar even believes affordable consumer laptops, much like data centers, should aim for [AGI](https://spectrum.ieee.org/tag/agi).

“I want a complete [artificial general intelligence](https://spectrum.ieee.org/tag/artificial-general-intelligence) running on Qualcomm devices,” he said. “That’s what we’re trying to push for.”

*This article appears in the December 2025 print issue.*

From Your Site Articles

* [When AI Unplugs, All Bets Are Off ›](https://spectrum.ieee.org/personal-ai-assistant)
* [Opera Includes AI Agents in Latest Web Browser ›](https://spectrum.ieee.org/agentic-ai-opera-mini)

Related Articles Around the Web

* [A Starter Guide for Playing with Your Own Local AI! : r/LocalLLaMA ›](https://www.reddit.com/r/LocalLLaMA/comments/16y95hk/a_starter_guide_for_playing_with_your_own_local_ai/)
* [Why You Should Use Local Models. When building Gen AI ... ›](https://medium.com/%40springrod/why-you-should-use-local-models-a3fce1124c94)

[large language models](https://spectrum.ieee.org/tag/large-language-models)[laptops](https://spectrum.ieee.org/tag/laptops)[amd](https://spectrum.ieee.org/tag/amd)[apple](https://spectrum.ieee.org/tag/apple)[microsoft](https://spectrum.ieee.org/tag/microsoft)

[Matthew S. Smith](https://spectrum.ieee.org/u/matthew-s-smith)

[Matthew S. Smith](https://mattontech.me/) is a freelance consumer technology journalist with 17 years of experience and the former Lead Reviews Editor at Digital Trends. An *IEEE Spectrum* Contributing Editor, he covers consumer tech with a focus on display innovations, artificial intelligence, and augmented reality. A vintage computing enthusiast, Matthew covers retro computers and computer games on his YouTube channel, [Computer Gaming Yesterday](https://www.youtube.com/channel/UCmPXXix0AucxBo-r8LZbQ_Q).

The Conversation (11)

![John Fitzpatrick](https://spectrum.ieee.org/res/avatars/default "John Fitzpatrick")

John Fitzpatrick23 Dec, 2025

M

The author assumes that people want AI; I see no evidence to support that. Anecdotes suggest that people view AI as forced upon them. If true, then we can expect few to upgrade their computers (laptop or otherwise) to handle AI. Let's watch the sales figures.

0
RepliesHide replies

Show More Replies

![Rakesh Agrawal](https://spectrum.ieee.org/res/avatars/default "Rakesh Agrawal")

Rakesh Agrawal23 Dec, 2025

SM

The real story isn’t that laptops couldn’t run LLMs—it’s that they were never designed to. With NPUs and AI-optimized hardware becoming standard, we’re entering an era where AI is a native capability of the PC, not a cloud add-on. This shift will unlock private, responsive, and always-available AI at the edge.

0
RepliesHide replies

Show More Replies

![lynn brielmaier](https://spectrum.ieee.org/res/avatars/default "lynn brielmaier")

lynn brielmaier20 Dec, 2025

M

Aaaand just in time : The Rampocolis.

0
RepliesHide replies

Show More Replies

[![Illustration of an illuminated flowing road.](data:image/svg+xml...)](https://spectrum.ieee.org/top-ev-charging-stories-2025)

[Transportation](https://spectrum.ieee.org/topic/transportation/)[News](https://spectrum.ieee.org/type/news/)

## [IEEE Spectrum's Top Transportation Stories of 2025](https://spectrum.ieee.org/top-ev-charging-stories-2025)

9h

4 min read

[![A laptop with atoms and lasers instead of a keyboard](data:image/svg+xml...)](https://spectrum.ieee.org/neutral-atom-quantum-computing)

[Computing](https://spectrum.ieee.org/topic/computing/)[Magazine](https://spectrum.ieee.org/magazine/)[Feature](https://spectrum.ieee.org/type/feature/)

## [Next-Level Quantum Computers Will Almost Be Useful](https://spectrum.ieee.org/neutral-atom-quantum-computing)

10h

6 min read

[![Several portraits collaged against a blurred scenic background.](data:image/svg+xml...)](https://spectrum.ieee.org/ieee-stem-summit-2025)

[The Institute](https://spectrum.ieee.org/topic/the-institute/)[Careers](https://spectrum.ieee.org/topic/careers/)[Article](https://spectrum.ieee.org/type/article/)

## [Virtual IEEE Summit Tackles STEM Challenges](https://spectrum.ieee.org/ieee-stem-summit-2025)

22 Dec 2025

2 min read
