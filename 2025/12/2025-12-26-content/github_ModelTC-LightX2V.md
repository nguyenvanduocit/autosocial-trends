---
source: github
title: ModelTC/LightX2V
url: https://github.com/ModelTC/LightX2V
date: 2025-12-26
---

[Skip to content](#start-of-content)

## Navigation Menu

Toggle navigation

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FModelTC%2FLightX2V)

Appearance settings

* Platform

  + AI CODE CREATION
    - [GitHub CopilotWrite better code with AI](https://github.com/features/copilot)
    - [GitHub SparkBuild and deploy intelligent apps](https://github.com/features/spark)
    - [GitHub ModelsManage and compare prompts](https://github.com/features/models)
    - [MCP RegistryNewIntegrate external tools](https://github.com/mcp)
  + DEVELOPER WORKFLOWS
    - [ActionsAutomate any workflow](https://github.com/features/actions)
    - [CodespacesInstant dev environments](https://github.com/features/codespaces)
    - [IssuesPlan and track work](https://github.com/features/issues)
    - [Code ReviewManage code changes](https://github.com/features/code-review)
  + APPLICATION SECURITY
    - [GitHub Advanced SecurityFind and fix vulnerabilities](https://github.com/security/advanced-security)
    - [Code securitySecure your code as you build](https://github.com/security/advanced-security/code-security)
    - [Secret protectionStop leaks before they start](https://github.com/security/advanced-security/secret-protection)
  + EXPLORE
    - [Why GitHub](https://github.com/why-github)
    - [Documentation](https://docs.github.com)
    - [Blog](https://github.blog)
    - [Changelog](https://github.blog/changelog)
    - [Marketplace](https://github.com/marketplace)

  [View all features](https://github.com/features)
* Solutions

  + BY COMPANY SIZE
    - [Enterprises](https://github.com/enterprise)
    - [Small and medium teams](https://github.com/team)
    - [Startups](https://github.com/enterprise/startups)
    - [Nonprofits](https://github.com/solutions/industry/nonprofits)
  + BY USE CASE
    - [App Modernization](https://github.com/solutions/use-case/app-modernization)
    - [DevSecOps](https://github.com/solutions/use-case/devsecops)
    - [DevOps](https://github.com/solutions/use-case/devops)
    - [CI/CD](https://github.com/solutions/use-case/ci-cd)
    - [View all use cases](https://github.com/solutions/use-case)
  + BY INDUSTRY
    - [Healthcare](https://github.com/solutions/industry/healthcare)
    - [Financial services](https://github.com/solutions/industry/financial-services)
    - [Manufacturing](https://github.com/solutions/industry/manufacturing)
    - [Government](https://github.com/solutions/industry/government)
    - [View all industries](https://github.com/solutions/industry)

  [View all solutions](https://github.com/solutions)
* Resources

  + EXPLORE BY TOPIC
    - [AI](https://github.com/resources/articles?topic=ai)
    - [Software Development](https://github.com/resources/articles?topic=software-development)
    - [DevOps](https://github.com/resources/articles?topic=devops)
    - [Security](https://github.com/resources/articles?topic=security)
    - [View all topics](https://github.com/resources/articles)
  + EXPLORE BY TYPE
    - [Customer stories](https://github.com/customer-stories)
    - [Events & webinars](https://github.com/resources/events)
    - [Ebooks & reports](https://github.com/resources/whitepapers)
    - [Business insights](https://github.com/solutions/executive-insights)
    - [GitHub Skills](https://skills.github.com)
  + SUPPORT & SERVICES
    - [Documentation](https://docs.github.com)
    - [Customer support](https://support.github.com)
    - [Community forum](https://github.com/orgs/community/discussions)
    - [Trust center](https://github.com/trust-center)
    - [Partners](https://github.com/partners)
* Open Source

  + COMMUNITY
    - [GitHub SponsorsFund open source developers](https://github.com/sponsors)
  + PROGRAMS
    - [Security Lab](https://securitylab.github.com)
    - [Maintainer Community](https://maintainers.github.com)
    - [Accelerator](https://github.com/accelerator)
    - [Archive Program](https://archiveprogram.github.com)
  + REPOSITORIES
    - [Topics](https://github.com/topics)
    - [Trending](https://github.com/trending)
    - [Collections](https://github.com/collections)
* Enterprise

  + ENTERPRISE SOLUTIONS
    - [Enterprise platformAI-powered developer platform](https://github.com/enterprise)
  + AVAILABLE ADD-ONS
    - [GitHub Advanced SecurityEnterprise-grade security features](https://github.com/security/advanced-security)
    - [Copilot for BusinessEnterprise-grade AI features](https://github.com/features/copilot/copilot-business)
    - [Premium SupportEnterprise-grade 24/7 support](https://github.com/premium-support)
* [Pricing](https://github.com/pricing)

Search or jump to...

# Search code, repositories, users, issues, pull requests...

Search

Clear

[Search syntax tips](https://docs.github.com/search-github/github-code-search/understanding-github-code-search-syntax)

# Provide feedback

We read every piece of feedback, and take your input very seriously.

[ ]
Include my email address so I can be contacted

Cancel
 Submit feedback

# Saved searches

## Use saved searches to filter your results more quickly

Cancel
 Create saved search

[Sign in](/login?return_to=https%3A%2F%2Fgithub.com%2FModelTC%2FLightX2V)

[Sign up](/signup?ref_cta=Sign+up&ref_loc=header+logged+out&ref_page=%2F%3Cuser-name%3E%2F%3Crepo-name%3E&source=header-repo&source_repo=ModelTC%2FLightX2V)

Appearance settings

Resetting focus

You signed in with another tab or window. Reload to refresh your session.
You signed out in another tab or window. Reload to refresh your session.
You switched accounts on another tab or window. Reload to refresh your session.

Dismiss alert

{{ message }}

[ModelTC](/ModelTC)
/
**[LightX2V](/ModelTC/LightX2V)**
Public

* [Notifications](/login?return_to=%2FModelTC%2FLightX2V) You must be signed in to change notification settings
* [Fork
  93](/login?return_to=%2FModelTC%2FLightX2V)
* [Star
   1.4k](/login?return_to=%2FModelTC%2FLightX2V)

Light Video Generation Inference Framework

### License

[Apache-2.0 license](/ModelTC/LightX2V/blob/main/LICENSE)

[1.4k
stars](/ModelTC/LightX2V/stargazers) [93
forks](/ModelTC/LightX2V/forks) [Branches](/ModelTC/LightX2V/branches) [Tags](/ModelTC/LightX2V/tags) [Activity](/ModelTC/LightX2V/activity)

[Star](/login?return_to=%2FModelTC%2FLightX2V)

[Notifications](/login?return_to=%2FModelTC%2FLightX2V) You must be signed in to change notification settings

* [Code](/ModelTC/LightX2V)
* [Issues
  76](/ModelTC/LightX2V/issues)
* [Pull requests
  2](/ModelTC/LightX2V/pulls)
* [Discussions](/ModelTC/LightX2V/discussions)
* [Actions](/ModelTC/LightX2V/actions)
* [Projects
  0](/ModelTC/LightX2V/projects)
* [Security

  ### Uh oh!

  There was an error while loading. Please reload this page.](/ModelTC/LightX2V/security)
* [Insights](/ModelTC/LightX2V/pulse)

Additional navigation options

* [Code](/ModelTC/LightX2V)
* [Issues](/ModelTC/LightX2V/issues)
* [Pull requests](/ModelTC/LightX2V/pulls)
* [Discussions](/ModelTC/LightX2V/discussions)
* [Actions](/ModelTC/LightX2V/actions)
* [Projects](/ModelTC/LightX2V/projects)
* [Security](/ModelTC/LightX2V/security)
* [Insights](/ModelTC/LightX2V/pulse)

# ModelTC/LightX2V

main

[Branches](/ModelTC/LightX2V/branches)[Tags](/ModelTC/LightX2V/tags)

Go to file

Code

Open more actions menu

## Folders and files

| Name | | Name | Last commit message | Last commit date |
| --- | --- | --- | --- | --- |
| Latest commit   History[884 Commits](/ModelTC/LightX2V/commits/main/) | | |
| [.github](/ModelTC/LightX2V/tree/main/.github ".github") | | [.github](/ModelTC/LightX2V/tree/main/.github ".github") |  |  |
| [app](/ModelTC/LightX2V/tree/main/app "app") | | [app](/ModelTC/LightX2V/tree/main/app "app") |  |  |
| [assets](/ModelTC/LightX2V/tree/main/assets "assets") | | [assets](/ModelTC/LightX2V/tree/main/assets "assets") |  |  |
| [configs](/ModelTC/LightX2V/tree/main/configs "configs") | | [configs](/ModelTC/LightX2V/tree/main/configs "configs") |  |  |
| [dockerfiles](/ModelTC/LightX2V/tree/main/dockerfiles "dockerfiles") | | [dockerfiles](/ModelTC/LightX2V/tree/main/dockerfiles "dockerfiles") |  |  |
| [docs](/ModelTC/LightX2V/tree/main/docs "docs") | | [docs](/ModelTC/LightX2V/tree/main/docs "docs") |  |  |
| [examples](/ModelTC/LightX2V/tree/main/examples "examples") | | [examples](/ModelTC/LightX2V/tree/main/examples "examples") |  |  |
| [lightx2v](/ModelTC/LightX2V/tree/main/lightx2v "lightx2v") | | [lightx2v](/ModelTC/LightX2V/tree/main/lightx2v "lightx2v") |  |  |
| [lightx2v\_kernel](/ModelTC/LightX2V/tree/main/lightx2v_kernel "lightx2v_kernel") | | [lightx2v\_kernel](/ModelTC/LightX2V/tree/main/lightx2v_kernel "lightx2v_kernel") |  |  |
| [lightx2v\_platform](/ModelTC/LightX2V/tree/main/lightx2v_platform "lightx2v_platform") | | [lightx2v\_platform](/ModelTC/LightX2V/tree/main/lightx2v_platform "lightx2v_platform") |  |  |
| [save\_results](/ModelTC/LightX2V/tree/main/save_results "save_results") | | [save\_results](/ModelTC/LightX2V/tree/main/save_results "save_results") |  |  |
| [scripts](/ModelTC/LightX2V/tree/main/scripts "scripts") | | [scripts](/ModelTC/LightX2V/tree/main/scripts "scripts") |  |  |
| [test\_cases](/ModelTC/LightX2V/tree/main/test_cases "test_cases") | | [test\_cases](/ModelTC/LightX2V/tree/main/test_cases "test_cases") |  |  |
| [tools](/ModelTC/LightX2V/tree/main/tools "tools") | | [tools](/ModelTC/LightX2V/tree/main/tools "tools") |  |  |
| [.gitignore](/ModelTC/LightX2V/blob/main/.gitignore ".gitignore") | | [.gitignore](/ModelTC/LightX2V/blob/main/.gitignore ".gitignore") |  |  |
| [.pre-commit-config.yaml](/ModelTC/LightX2V/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml") | | [.pre-commit-config.yaml](/ModelTC/LightX2V/blob/main/.pre-commit-config.yaml ".pre-commit-config.yaml") |  |  |
| [LICENSE](/ModelTC/LightX2V/blob/main/LICENSE "LICENSE") | | [LICENSE](/ModelTC/LightX2V/blob/main/LICENSE "LICENSE") |  |  |
| [README.md](/ModelTC/LightX2V/blob/main/README.md "README.md") | | [README.md](/ModelTC/LightX2V/blob/main/README.md "README.md") |  |  |
| [README\_zh.md](/ModelTC/LightX2V/blob/main/README_zh.md "README_zh.md") | | [README\_zh.md](/ModelTC/LightX2V/blob/main/README_zh.md "README_zh.md") |  |  |
| [pyproject.toml](/ModelTC/LightX2V/blob/main/pyproject.toml "pyproject.toml") | | [pyproject.toml](/ModelTC/LightX2V/blob/main/pyproject.toml "pyproject.toml") |  |  |
| [requirements-docs.txt](/ModelTC/LightX2V/blob/main/requirements-docs.txt "requirements-docs.txt") | | [requirements-docs.txt](/ModelTC/LightX2V/blob/main/requirements-docs.txt "requirements-docs.txt") |  |  |
| [requirements.txt](/ModelTC/LightX2V/blob/main/requirements.txt "requirements.txt") | | [requirements.txt](/ModelTC/LightX2V/blob/main/requirements.txt "requirements.txt") |  |  |
| [requirements\_animate.txt](/ModelTC/LightX2V/blob/main/requirements_animate.txt "requirements_animate.txt") | | [requirements\_animate.txt](/ModelTC/LightX2V/blob/main/requirements_animate.txt "requirements_animate.txt") |  |  |
| [requirements\_win.txt](/ModelTC/LightX2V/blob/main/requirements_win.txt "requirements_win.txt") | | [requirements\_win.txt](/ModelTC/LightX2V/blob/main/requirements_win.txt "requirements_win.txt") |  |  |
| [setup\_vae.py](/ModelTC/LightX2V/blob/main/setup_vae.py "setup_vae.py") | | [setup\_vae.py](/ModelTC/LightX2V/blob/main/setup_vae.py "setup_vae.py") |  |  |
| View all files | | |

## Repository files navigation

* README
* Apache-2.0 license

# âš¡ï¸ LightX2V: Light Video Generation Inference Framework

[![logo](/ModelTC/LightX2V/raw/main/assets/img_lightx2v.png)](/ModelTC/LightX2V/blob/main/assets/img_lightx2v.png)

[![License](https://camo.githubusercontent.com/5b60841bea9e11d9d0b0950d690c9bc554e06385634056a7d5d62a15d1a4eabe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4170616368655f322e302d626c75652e737667)](https://opensource.org/licenses/Apache-2.0)
[![Ask DeepWiki](https://camo.githubusercontent.com/0f5ae213ac378635adeb5d7f13cef055ad2f7d9a47b36de7b1c67dbe09f609ca/68747470733a2f2f6465657077696b692e636f6d2f62616467652e737667)](https://deepwiki.com/ModelTC/lightx2v)
[![Doc](https://camo.githubusercontent.com/cbc04220a36924dd7a6c48a055064ee3715ecfaf8b1ceb19750cd7a2f2988429/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d456e676c6973682d3939636332)](https://lightx2v-en.readthedocs.io/en/latest)
[![Doc](https://camo.githubusercontent.com/eedcf8b7e327bbb2a615b7ade6bdf8679ae222fb579261ffea169c4a1934c8bc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545362539362538372545362541312541332d2545342542382541442545362539362538372d3939636332)](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest)
[![Papers](https://camo.githubusercontent.com/b3f5f424fc84076eb27929b5eb4f63832c7c149a7403d9f0cd38ac4751726fa5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f2545382541452542412545362539362538372545392539422538362d2545342542382541442545362539362538372d3939636332)](https://lightx2v-papers-zhcn.readthedocs.io/zh-cn/latest)
[![Docker](https://camo.githubusercontent.com/8709250bbf63030139d0fba80ac65f9449a9ef3915dcd818a841afae0dad96f8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f636b65722d3234393645443f7374796c653d666c6174266c6f676f3d646f636b6572266c6f676f436f6c6f723d7768697465)](https://hub.docker.com/r/lightx2v/lightx2v/tags)

**[ English | [ä¸­æ–‡](/ModelTC/LightX2V/blob/main/README_zh.md) ]**

---

**LightX2V** is an advanced lightweight video generation inference framework engineered to deliver efficient, high-performance video synthesis solutions. This unified platform integrates multiple state-of-the-art video generation techniques, supporting diverse generation tasks including text-to-video (T2V) and image-to-video (I2V). **X2V represents the transformation of different input modalities (X, such as text or images) into video output (V)**.

> ğŸŒ **Try it online now!** Experience LightX2V without installation: **[LightX2V Online Service](https://x2v.light-ai.top/login)** - Free, lightweight, and fast AI digital human video generation platform.

> ğŸ‘‹ **Join us on [WeChat](https://light-ai.top/community.html).**

## ğŸ”¥ Latest News

* **December 25, 2025:** ğŸš€ Supported deployment on AMD ROCm and Ascend 910B.
* **December 23, 2025:** ğŸš€ We support the [Qwen-Image-Edit-2511](https://huggingface.co/Qwen/Qwen-Image-Edit-2511) image editing model since Day 0. On a single H100 GPU, LightX2V delivers approximately 1.4Ã— speedup. We support for CFG parallelism, Ulysses parallelism, and efficient offloading technologies. Our [HuggingFace](https://huggingface.co/lightx2v/Qwen-Image-Edit-2511-Lightning) has been updated with CFG / step-distilled LoRA and FP8 weights. Usage examples can be found in the [Python scripts](https://github.com/ModelTC/LightX2V/tree/main/examples/qwen_image). Combined with LightX2V, 4-step CFG / step distillation, and the FP8 model, the maximum acceleration can reach up to approximately 42Ã—. Feel free to try [LightX2V Online Service](https://x2v.light-ai.top/login) with *Image to Image* and *Qwen-Image-Edit-2511* model.
* **December 22, 2025:** ğŸš€ Added **Wan2.1 NVFP4 quantization-aware 4-step distilled models**; weights are available on HuggingFace: [Wan-NVFP4](https://huggingface.co/lightx2v/Wan-NVFP4).
* **December 15, 2025:** ğŸš€ Supported deployment on Hygon DCU.
* **December 4, 2025:** ğŸš€ Supported GGUF format model inference & deployment on Cambricon MLU590/MetaX C500.
* **November 24, 2025:** ğŸš€ We released 4-step distilled models for HunyuanVideo-1.5! These models enable **ultra-fast 4-step inference** without CFG requirements, achieving approximately **25x speedup** compared to standard 50-step inference. Both base and FP8 quantized versions are now available: [Hy1.5-Distill-Models](https://huggingface.co/lightx2v/Hy1.5-Distill-Models).
* **November 21, 2025:** ğŸš€ We support the [HunyuanVideo-1.5](https://huggingface.co/tencent/HunyuanVideo-1.5) video generation model since Day 0. With the same number of GPUs, LightX2V can achieve a speed improvement of over 2 times and supports deployment on GPUs with lower memory (such as the 24GB RTX 4090). It also supports CFG/Ulysses parallelism, efficient offloading, TeaCache/MagCache technologies, and more. We will soon update more models on our [HuggingFace page](https://huggingface.co/lightx2v), including step distillation, VAE distillation, and other related models. Quantized models and lightweight VAE models are now available: [Hy1.5-Quantized-Models](https://huggingface.co/lightx2v/Hy1.5-Quantized-Models) for quantized inference, and [LightTAE for HunyuanVideo-1.5](https://huggingface.co/lightx2v/Autoencoders/blob/main/lighttaehy1_5.safetensors) for fast VAE decoding. Refer to [this](https://github.com/ModelTC/LightX2V/tree/main/scripts/hunyuan_video_15) for usage tutorials, or check out the [examples directory](https://github.com/ModelTC/LightX2V/tree/main/examples) for code examples.

## ğŸ† Performance Benchmarks (Updated on 2025.12.01)

### ğŸ“Š Cross-Framework Performance Comparison (H100)

| Framework | GPUs | Step Time | Speedup |
| --- | --- | --- | --- |
| Diffusers | 1 | 9.77s/it | 1x |
| xDiT | 1 | 8.93s/it | 1.1x |
| FastVideo | 1 | 7.35s/it | 1.3x |
| SGL-Diffusion | 1 | 6.13s/it | 1.6x |
| **LightX2V** | 1 | **5.18s/it** | **1.9x** ğŸš€ |
| FastVideo | 8 | 2.94s/it | 1x |
| xDiT | 8 | 2.70s/it | 1.1x |
| SGL-Diffusion | 8 | 1.19s/it | 2.5x |
| **LightX2V** | 8 | **0.75s/it** | **3.9x** ğŸš€ |

### ğŸ“Š Cross-Framework Performance Comparison (RTX 4090D)

| Framework | GPUs | Step Time | Speedup |
| --- | --- | --- | --- |
| Diffusers | 1 | 30.50s/it | 1x |
| FastVideo | 1 | 22.66s/it | 1.3x |
| xDiT | 1 | OOM | OOM |
| SGL-Diffusion | 1 | OOM | OOM |
| **LightX2V** | 1 | **20.26s/it** | **1.5x** ğŸš€ |
| FastVideo | 8 | 15.48s/it | 1x |
| xDiT | 8 | OOM | OOM |
| SGL-Diffusion | 8 | OOM | OOM |
| **LightX2V** | 8 | **4.75s/it** | **3.3x** ğŸš€ |

### ğŸ“Š LightX2V Performance Comparison

| Framework | GPU | Configuration | Step Time | Speedup |
| --- | --- | --- | --- | --- |
| **LightX2V** | H100 | 8 GPUs + cfg | 0.75s/it | 1x |
| **LightX2V** | H100 | 8 GPUs + no cfg | 0.39s/it | 1.9x |
| **LightX2V** | H100 | **8 GPUs + no cfg + fp8** | **0.35s/it** | **2.1x** ğŸš€ |
| **LightX2V** | 4090D | 8 GPUs + cfg | 4.75s/it | 1x |
| **LightX2V** | 4090D | 8 GPUs + no cfg | 3.13s/it | 1.5x |
| **LightX2V** | 4090D | **8 GPUs + no cfg + fp8** | **2.35s/it** | **2.0x** ğŸš€ |

**Note**: All the above performance data were tested on Wan2.1-I2V-14B-480P(40 steps, 81 frames). In addition, we also provide 4-step distilled models on the [HuggingFace page](https://huggingface.co/lightx2v).

## ğŸ’¡ Quick Start

For comprehensive usage instructions, please refer to our documentation: **[English Docs](https://lightx2v-en.readthedocs.io/en/latest/) | [ä¸­æ–‡æ–‡æ¡£](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest/)**

**We highly recommend using the Docker environment, as it is the simplest and fastest way to set up the environment. For details, please refer to the Quick Start section in the documentation.**

### Installation from Git

```
pip install -v git+https://github.com/ModelTC/LightX2V.git
```

### Building from Source

```
git clone https://github.com/ModelTC/LightX2V.git
cd LightX2V
uv pip install -v . # pip install -v .
```

### (Optional) Install Attention/Quantize Operators

For attention operators installation, please refer to our documentation: **[English Docs](https://lightx2v-en.readthedocs.io/en/latest/getting_started/quickstart.html#step-4-install-attention-operators) | [ä¸­æ–‡æ–‡æ¡£](https://lightx2v-zhcn.readthedocs.io/zh-cn/latest/getting_started/quickstart.html#id9)**

### Usage Example

```
# examples/wan/wan_i2v.py
"""
Wan2.2 image-to-video generation example.
This example demonstrates how to use LightX2V with Wan2.2 model for I2V generation.
"""

from lightx2v import LightX2VPipeline

# Initialize pipeline for Wan2.2 I2V task
# For wan2.1, use model_cls="wan2.1"
pipe = LightX2VPipeline(
    model_path="/path/to/Wan2.2-I2V-A14B",
    model_cls="wan2.2_moe",
    task="i2v",
)

# Alternative: create generator from config JSON file
# pipe.create_generator(
#     config_json="configs/wan22/wan_moe_i2v.json"
# )

# Enable offloading to significantly reduce VRAM usage with minimal speed impact
# Suitable for RTX 30/40/50 consumer GPUs
pipe.enable_offload(
    cpu_offload=True,
    offload_granularity="block",  # For Wan models, supports both "block" and "phase"
    text_encoder_offload=True,
    image_encoder_offload=False,
    vae_offload=False,
)

# Create generator manually with specified parameters
pipe.create_generator(
    attn_mode="sage_attn2",
    infer_steps=40,
    height=480,  # Can be set to 720 for higher resolution
    width=832,  # Can be set to 1280 for higher resolution
    num_frames=81,
    guidance_scale=[3.5, 3.5],  # For wan2.1, guidance_scale is a scalar (e.g., 5.0)
    sample_shift=5.0,
)

# Generation parameters
seed = 42
prompt = "Summer beach vacation style, a white cat wearing sunglasses sits on a surfboard. The fluffy-furred feline gazes directly at the camera with a relaxed expression. Blurred beach scenery forms the background featuring crystal-clear waters, distant green hills, and a blue sky dotted with white clouds. The cat assumes a naturally relaxed posture, as if savoring the sea breeze and warm sunlight. A close-up shot highlights the feline's intricate details and the refreshing atmosphere of the seaside."
negative_prompt = "é•œå¤´æ™ƒåŠ¨ï¼Œè‰²è°ƒè‰³ä¸½ï¼Œè¿‡æ›ï¼Œé™æ€ï¼Œç»†èŠ‚æ¨¡ç³Šä¸æ¸…ï¼Œå­—å¹•ï¼Œé£æ ¼ï¼Œä½œå“ï¼Œç”»ä½œï¼Œç”»é¢ï¼Œé™æ­¢ï¼Œæ•´ä½“å‘ç°ï¼Œæœ€å·®è´¨é‡ï¼Œä½è´¨é‡ï¼ŒJPEGå‹ç¼©æ®‹ç•™ï¼Œä¸‘é™‹çš„ï¼Œæ®‹ç¼ºçš„ï¼Œå¤šä½™çš„æ‰‹æŒ‡ï¼Œç”»å¾—ä¸å¥½çš„æ‰‹éƒ¨ï¼Œç”»å¾—ä¸å¥½çš„è„¸éƒ¨ï¼Œç•¸å½¢çš„ï¼Œæ¯å®¹çš„ï¼Œå½¢æ€ç•¸å½¢çš„è‚¢ä½“ï¼Œæ‰‹æŒ‡èåˆï¼Œé™æ­¢ä¸åŠ¨çš„ç”»é¢ï¼Œæ‚ä¹±çš„èƒŒæ™¯ï¼Œä¸‰æ¡è…¿ï¼ŒèƒŒæ™¯äººå¾ˆå¤šï¼Œå€’ç€èµ°"
image_path="/path/to/img_0.jpg"
save_result_path = "/path/to/save_results/output.mp4"

# Generate video
pipe.generate(
    seed=seed,
    image_path=image_path,
    prompt=prompt,
    negative_prompt=negative_prompt,
    save_result_path=save_result_path,
)
```

**NVFP4 (quantization-aware 4-step) resources**

* Inference examples: `examples/wan/wan_i2v_nvfp4.py` (I2V) and `examples/wan/wan_t2v_nvfp4.py` (T2V).
* NVFP4 operator build/install guide: see `lightx2v_kernel/README.md`.

> ğŸ’¡ **More Examples**: For more usage examples including quantization, offloading, caching, and other advanced configurations, please refer to the [examples directory](https://github.com/ModelTC/LightX2V/tree/main/examples).

## ğŸ¤– Supported Model Ecosystem

### Official Open-Source Models

* âœ… [HunyuanVideo-1.5](https://huggingface.co/tencent/HunyuanVideo-1.5)
* âœ… [Wan2.1 & Wan2.2](https://huggingface.co/Wan-AI/)
* âœ… [Qwen-Image](https://huggingface.co/Qwen/Qwen-Image)
* âœ… [Qwen-Image-Edit](https://huggingface.co/spaces/Qwen/Qwen-Image-Edit)
* âœ… [Qwen-Image-Edit-2509](https://huggingface.co/Qwen/Qwen-Image-Edit-2509)
* âœ… [Qwen-Image-Edit-2511](https://huggingface.co/Qwen/Qwen-Image-Edit-2511)

### Quantized and Distilled Models/LoRAs (**ğŸš€ Recommended: 4-step inference**)

* âœ… [Wan2.1-Distill-Models](https://huggingface.co/lightx2v/Wan2.1-Distill-Models)
* âœ… [Wan2.2-Distill-Models](https://huggingface.co/lightx2v/Wan2.2-Distill-Models)
* âœ… [Wan2.1-Distill-Loras](https://huggingface.co/lightx2v/Wan2.1-Distill-Loras)
* âœ… [Wan2.2-Distill-Loras](https://huggingface.co/lightx2v/Wan2.2-Distill-Loras)
* âœ… [Wan2.1-Distill-NVFP4](https://huggingface.co/lightx2v/Wan-NVFP4)
* âœ… [Qwen-Image-Edit-2511-Lightning](https://huggingface.co/lightx2v/Qwen-Image-Edit-2511-Lightning)

### Lightweight Autoencoder Models (**ğŸš€ Recommended: fast inference & low memory usage**)

* âœ… [Autoencoders](https://huggingface.co/lightx2v/Autoencoders)

### Autoregressive Models

* âœ… [Wan2.1-T2V-CausVid](https://huggingface.co/lightx2v/Wan2.1-T2V-14B-CausVid)
* âœ… [Self-Forcing](https://github.com/guandeh17/Self-Forcing)
* âœ… [Matrix-Game-2.0](https://huggingface.co/Skywork/Matrix-Game-2.0)

ğŸ”” Follow our [HuggingFace page](https://huggingface.co/lightx2v) for the latest model releases from our team.

ğŸ’¡ Refer to the [Model Structure Documentation](https://lightx2v-en.readthedocs.io/en/latest/getting_started/model_structure.html) to quickly get started with LightX2V

## ğŸš€ Frontend Interfaces

We provide multiple frontend interface deployment options:

* **ğŸ¨ Gradio Interface**: Clean and user-friendly web interface, perfect for quick experience and prototyping
  + ğŸ“– [Gradio Deployment Guide](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_gradio.html)
* **ğŸ¯ ComfyUI Interface**: Powerful node-based workflow interface, supporting complex video generation tasks
  + ğŸ“– [ComfyUI Deployment Guide](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_comfyui.html)
* **ğŸš€ Windows One-Click Deployment**: Convenient deployment solution designed for Windows users, featuring automatic environment configuration and intelligent parameter optimization
  + ğŸ“– [Windows One-Click Deployment Guide](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_local_windows.html)

**ğŸ’¡ Recommended Solutions**:

* **First-time Users**: We recommend the Windows one-click deployment solution
* **Advanced Users**: We recommend the ComfyUI interface for more customization options
* **Quick Experience**: The Gradio interface provides the most intuitive operation experience

## ğŸš€ Core Features

### ğŸ¯ **Ultimate Performance Optimization**

* **ğŸ”¥ SOTA Inference Speed**: Achieve **~20x** acceleration via step distillation and system optimization (single GPU)
* **âš¡ï¸ Revolutionary 4-Step Distillation**: Compress original 40-50 step inference to just 4 steps without CFG requirements
* **ğŸ› ï¸ Advanced Operator Support**: Integrated with cutting-edge operators including [Sage Attention](https://github.com/thu-ml/SageAttention), [Flash Attention](https://github.com/Dao-AILab/flash-attention), [Radial Attention](https://github.com/mit-han-lab/radial-attention), [q8-kernel](https://github.com/KONAKONA666/q8_kernels), [sgl-kernel](https://github.com/sgl-project/sglang/tree/main/sgl-kernel), [vllm](https://github.com/vllm-project/vllm)

### ğŸ’¾ **Resource-Efficient Deployment**

* **ğŸ’¡ Breaking Hardware Barriers**: Run 14B models for 480P/720P video generation with only **8GB VRAM + 16GB RAM**
* **ğŸ”§ Intelligent Parameter Offloading**: Advanced disk-CPU-GPU three-tier offloading architecture with phase/block-level granular management
* **âš™ï¸ Comprehensive Quantization**: Support for `w8a8-int8`, `w8a8-fp8`, `w4a4-nvfp4` and other quantization strategies

### ğŸ¨ **Rich Feature Ecosystem**

* **ğŸ“ˆ Smart Feature Caching**: Intelligent caching mechanisms to eliminate redundant computations
* **ğŸ”„ Parallel Inference**: Multi-GPU parallel processing for enhanced performance
* **ğŸ“± Flexible Deployment Options**: Support for Gradio, service deployment, ComfyUI and other deployment methods
* **ğŸ›ï¸ Dynamic Resolution Inference**: Adaptive resolution adjustment for optimal generation quality
* **ğŸï¸ Video Frame Interpolation**: RIFE-based frame interpolation for smooth frame rate enhancement

## ğŸ“š Technical Documentation

### ğŸ“– **Method Tutorials**

* [Model Quantization](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/quantization.html) - Comprehensive guide to quantization strategies
* [Feature Caching](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/cache.html) - Intelligent caching mechanisms
* [Attention Mechanisms](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/attention.html) - State-of-the-art attention operators
* [Parameter Offloading](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/offload.html) - Three-tier storage architecture
* [Parallel Inference](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/parallel.html) - Multi-GPU acceleration strategies
* [Changing Resolution Inference](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/changing_resolution.html) - U-shaped resolution strategy
* [Step Distillation](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/step_distill.html) - 4-step inference technology
* [Video Frame Interpolation](https://lightx2v-en.readthedocs.io/en/latest/method_tutorials/video_frame_interpolation.html) - Base on the RIFE technology

### ğŸ› ï¸ **Deployment Guides**

* [Low-Resource Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/for_low_resource.html) - Optimized 8GB VRAM solutions
* [Low-Latency Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/for_low_latency.html) - Ultra-fast inference optimization
* [Gradio Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_gradio.html) - Web interface setup
* [Service Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/deploy_service.html) - Production API service deployment
* [Lora Model Deployment](https://lightx2v-en.readthedocs.io/en/latest/deploy_guides/lora_deploy.html) - Flexible Lora deployment

## ğŸ§¾ Contributing Guidelines

We maintain code quality through automated pre-commit hooks to ensure consistent formatting across the project.

Tip

**Setup Instructions:**

1. Install required dependencies:

```
pip install ruff pre-commit
```

2. Run before committing:

```
pre-commit run --all-files
```

We appreciate your contributions to making LightX2V better!

## ğŸ¤ Acknowledgments

We extend our gratitude to all the model repositories and research communities that inspired and contributed to the development of LightX2V. This framework builds upon the collective efforts of the open-source community.

## ğŸŒŸ Star History

[![Star History Chart](https://camo.githubusercontent.com/0a61b9b6704b83e2dcbaef9414d0242197955a4d1346cf5b2c13f2e8b4d1226a/68747470733a2f2f6170692e737461722d686973746f72792e636f6d2f7376673f7265706f733d4d6f64656c54432f6c6967687478327626747970653d54696d656c696e65)](https://star-history.com/#ModelTC/lightx2v&Timeline)

## âœï¸ Citation

If you find LightX2V useful in your research, please consider citing our work:

```
@misc{lightx2v,
 author = {LightX2V Contributors},
 title = {LightX2V: Light Video Generation Inference Framework},
 year = {2025},
 publisher = {GitHub},
 journal = {GitHub repository},
 howpublished = {\url{https://github.com/ModelTC/lightx2v}},
}
```

## ğŸ“ Contact & Support

For questions, suggestions, or support, please feel free to reach out through:

* ğŸ› [GitHub Issues](https://github.com/ModelTC/lightx2v/issues) - Bug reports and feature requests

---

Built with â¤ï¸ by the LightX2V team

## About

Light Video Generation Inference Framework

### Topics

[video-generation](/topics/video-generation "Topic: video-generation")
[diffusion-models](/topics/diffusion-models "Topic: diffusion-models")
[wan-video](/topics/wan-video "Topic: wan-video")
[auto-regressive-diffusion-model](/topics/auto-regressive-diffusion-model "Topic: auto-regressive-diffusion-model")

### Resources

[Readme](#readme-ov-file)

### License

[Apache-2.0 license](#Apache-2.0-1-ov-file)

### Uh oh!

There was an error while loading. Please reload this page.

[Activity](/ModelTC/LightX2V/activity)

[Custom properties](/ModelTC/LightX2V/custom-properties)

### Stars

[**1.4k**
stars](/ModelTC/LightX2V/stargazers)

### Watchers

[**15**
watching](/ModelTC/LightX2V/watchers)

### Forks

[**93**
forks](/ModelTC/LightX2V/forks)

[Report repository](/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FModelTC%2FLightX2V&report=ModelTC+%28user%29)

## [Releases](/ModelTC/LightX2V/releases)

No releases published

## [Packages 0](/orgs/ModelTC/packages?repo_name=LightX2V)

No packages published

### Uh oh!

There was an error while loading. Please reload this page.

## [Contributors 29](/ModelTC/LightX2V/graphs/contributors)

* [![@helloyongyang](https://avatars.githubusercontent.com/u/37302878?s=64&v=4)](https://github.com/helloyongyang)
* [![@gushiqiao](https://avatars.githubusercontent.com/u/77222802?s=64&v=4)](https://github.com/gushiqiao)
* [![@wangshankun](https://avatars.githubusercontent.com/u/3970271?s=64&v=4)](https://github.com/wangshankun)
* [![@GoatWu](https://avatars.githubusercontent.com/u/48351820?s=64&v=4)](https://github.com/GoatWu)
* [![@GACLove](https://avatars.githubusercontent.com/u/8307193?s=64&v=4)](https://github.com/GACLove)
* [![@zhiwei-dong](https://avatars.githubusercontent.com/u/40209280?s=64&v=4)](https://github.com/zhiwei-dong)
* [![@de1star](https://avatars.githubusercontent.com/u/37529028?s=64&v=4)](https://github.com/de1star)
* [![@Linboyan-trc](https://avatars.githubusercontent.com/u/101640332?s=64&v=4)](https://github.com/Linboyan-trc)
* [![@Watebear](https://avatars.githubusercontent.com/u/30264389?s=64&v=4)](https://github.com/Watebear)
* [![@Wq-dd](https://avatars.githubusercontent.com/u/62586707?s=64&v=4)](https://github.com/Wq-dd)
* [![@theNiemand](https://avatars.githubusercontent.com/u/130448801?s=64&v=4)](https://github.com/theNiemand)
* [![@huochaitiantang](https://avatars.githubusercontent.com/u/17795979?s=64&v=4)](https://github.com/huochaitiantang)
* [![@gemini-code-assist[bot]](https://avatars.githubusercontent.com/in/956858?s=64&v=4)](https://github.com/apps/gemini-code-assist)
* [![@Musisoul](https://avatars.githubusercontent.com/u/106440666?s=64&v=4)](https://github.com/Musisoul)

[+ 15 contributors](/ModelTC/LightX2V/graphs/contributors)

## Languages

* [Python
  62.1%](/ModelTC/LightX2V/search?l=python)
* [Vue
  20.7%](/ModelTC/LightX2V/search?l=vue)
* [JavaScript
  8.0%](/ModelTC/LightX2V/search?l=javascript)
* [Shell
  3.6%](/ModelTC/LightX2V/search?l=shell)
* [Cuda
  2.0%](/ModelTC/LightX2V/search?l=cuda)
* [HTML
  1.4%](/ModelTC/LightX2V/search?l=html)
* Other
  2.2%

## Footer

Â© 2025 GitHub,Â Inc.

### Footer navigation

* [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
* [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
* [Security](https://github.com/security)
* [Status](https://www.githubstatus.com/)
* [Community](https://github.community/)
* [Docs](https://docs.github.com/)
* [Contact](https://support.github.com?tags=dotcom-footer)
* Manage cookies
* Do not share my personal information

You canâ€™t perform that action at this time.
