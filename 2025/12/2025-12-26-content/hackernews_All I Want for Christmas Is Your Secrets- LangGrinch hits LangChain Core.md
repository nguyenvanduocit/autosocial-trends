---
source: hackernews
title: All I Want for Christmas Is Your Secrets: LangGrinch hits LangChain Core
url: https://cyata.ai/blog/langgrinch-langchain-core-cve-2025-68664/
date: 2025-12-26
---

[![](https://cyata.ai/wp-content/uploads/2025/06/cyata.svg)](https://cyata.ai/)

[Book a Demo](https://cyata.ai/book-a-demo/)

* [About Us](https://cyata.ai/about-us/)
* [Partners](https://cyata.ai/partners/)
* [Blog](https://cyata.ai/blog/)
* [Careers](https://cyata.ai/careers/)

[Book a Demo](https://cyata.ai/book-a-demo/)

[See Cyata in Action](https://cyata.ai/book-a-demo/)

[Join Our Newsletter](#newsletter)

* [Privacy Policy](https://cyata.ai/privacy-policy/)
* [Terms of Use](https://cyata.ai/terms-of-use/)

Â© 2025 Cyata. All rights reserved.

[All insights](https://cyata.ai/blog/)

December 25, 2025

â€¢
14 min read

# All I Want for Christmas Is Your Secrets: LangGrinch hits LangChain Core (CVE-2025-68664)

![](https://cyata.ai/wp-content/uploads/2025/08/yarden_new-212x300.webp)

Written by
Yarden Porat

![](https://cyata.ai/wp-content/uploads/2025/12/LangGrinch-Article.png)

Cyata Research: LangGrinch Vulnerability in LangChain

Yesterday, LangChain published a [critical advisory](https://github.com/advisories/GHSA-c67j-w6g6-q2cm) for a vulnerability I reported in langchain-core: **CVE-2025-68664 / GHSA-c67j-w6g6-q2cm**.

Earlier this year, my research focused on breaking secret managers in our â€œ[Vault Fault](https://cyata.ai/vault-fault/)â€ work â€“ systems that are explicitly designed to be the security boundary around your most sensitive credentials. One takeaway kept repeating: when a platform accidentally treats attacker-shaped data as trusted structure, that boundary collapses fast. **This time, the system that â€œbreaksâ€ isnâ€™t your secret manager. Itâ€™s the agent framework that may use them.**

Why this vulnerability deserves extra attention:

1. **Itâ€™s in Core.** This is not a specific tool bug, not an integration edge-case, and not a â€œsome community package did something weird.â€ The vulnerable APIs (dumps() / dumpd()) live in **langchain-core itself**.
2. **The blast radius is scale.** By download volume, langchain is one of the most widely deployed AI framework components globally today. As of late December 2025, public package telemetry shows **hundreds of millions of installs**, with pepy.tech reporting **~847M total downloads** and pypistats showing **~98M downloads in the last month**.
3. **One prompt can trigger a lot of machinery.** The most common real-world path here is not â€œattacker sends you a serialized blob and you call load().â€ Itâ€™s subtler: LLM outputs can influence fields like additional\_kwargs or response\_metadata, and those fields can be serialized and later deserialized through normal framework features like streaming logs/events. Plainly, this means **an exploit can be triggered by a single text prompt** that cascades into a surprisingly complex internal pipeline.

Before you read further, patches are now released in versions 1.2.5 and 0.3.81. If youâ€™re running LangChain in production, this one is trickier than it may seem; please update ASAP.

## **The short version of the bug**

LangChain uses a special internal serialization format where dictionaries containing an â€˜lcâ€™ marker represent LangChain objects. The vulnerability was that dumps() and dumpd() **did not properly escape user-controlled dictionaries** that happened to include the reserved â€˜lcâ€™ key.

So once an attacker is able to make a LangChain orchestration loop serialize and later deserialize content including an â€˜lcâ€™ key, they would instantiate an unsafe arbitrary object, potentially triggering many attacker-friendly paths.

The [advisory](https://github.com/advisories/GHSA-c67j-w6g6-q2cm) lists **12 distinct vulnerable flows**, which are extremely common use cases, such as standard event streaming, logging, message history/memory or caches:

![](https://cyata.ai/wp-content/uploads/2025/12/image-16-1024x221.png)

The most damaging outcomes include:

* **Secret extraction** from environment variables. The advisory notes this happens when deserialization is performed with secrets\_from\_env=True. Notably, **this was the default until yesterday**. ğŸ™‚
* **Object instantiation** within pre-approved namespaces (including *langchain\_core*, *langchain\_openai, langchain\_aws, langchain\_anthropicâ€¦*), potentially triggering side effects in constructors (network calls, file operations, etc.).
* Under certain conditions LangChain object instantiation may lead to arbitrary code execution.

This is categorized under **CWE-502: Deserialization of Untrusted Data**, with a CNA CVSS score of **9.3 (Critical)**.

![](https://cyata.ai/wp-content/uploads/2025/12/banner-langgrinch.png)

## **My research story: how I stumbled into it**

â€™Twas the night before Christmas, and I was doing the least festive kind of work: staring at serialization code and asking â€œwaitâ€¦ why is that trusted?â€

Security research often looks dramatic from the outside. In reality, it is usually careful reading, small hypotheses, and a slow accumulation of â€œthatâ€™s oddâ€ moments.

This one started the way many do at Cyata: with a simple question we ask constantly as we assess AI stacks for real-world risk:

*Where are the trust boundaries in AI applications, and do developers actually know where those boundaries are?*

LangChain is a powerful framework, and like most modern frameworks it has to move complex structured data around: messages, tool calls, streaming events, traces, caches, and â€œrunnables.â€

Reviewing previous research, there was already extensive research on LangChain tooling and integrations, but very few findings on the core library.

I started the research working backwards. Finding interesting locations (sinks), then figuring out how an attacker might reach them. Deserialization was an obvious target.

It took me quite a while to find something meaningful. But after a while, I found that assuming an attacker-controlled deserialization primitive, I could trigger blind SSRF that could be leveraged to exfiltrate environment variables (to be detailed soon). Because the outcome was limited to secret exfil, instead of my primary RCE goal, I kept auditing the deserialization, and took my time.

The bug wasnâ€™t a piece of bad code, it was the absence of code. dumps() simply didnâ€™t escape user-controlled dictionaries containing â€˜lcâ€™Â  keys. A missing escape in the serialization path, not the deserialization.

![](https://cyata.ai/wp-content/uploads/2025/12/image-2-1024x342.png)

Itâ€™s much easier to spot something wrong than to notice something missing, especially when youâ€™re auditing load(), not dumps(). In one of the most scrutinized AI frameworks. For two and a half years.

From there, the investigation became a structured exercise:

1. Identify where **untrusted content** (mostly arbitrary dictionaries) gets serialized (LLM outputs, prompt injection, user input, external tools, retrieved docs).
2. Identify when that serialized data gets deserialized.
3. Identify what an attacker can achieve from arbitrary object instantiation.

At that point, the core finding was clear and actionable enough to responsibly report: there was an escaping gap in dumps() / dumpd() around â€˜lcâ€™ key dictionaries.

The advisory later captured what we often see in practice: fields like additional\_kwargs and response\_metadata can be influenced by LLM output and prompt injection, and those fields can get serialized-deserialized in many flows.

To the LangChain teamâ€™s credit: the response and follow-through were decisive, not just patching the bug but also hardening defaults that were too permissive for the world weâ€™re in now.

The LangChain project decided to award a $4,000 USD bounty for this finding. According to huntr, the platform where LangChain operated its bounty program, this would be the maximum amount ever awarded in the project, with bounties so far awarded up to $125.

## **Technical deep dive**

### **Background: the â€œlcâ€ marker and why it exists**

LangChain serializes certain objects using a structured dict format. The â€˜lcâ€™ key is used internally to indicate â€œthis is a LangChain-serialized structure,â€ not just arbitrary user data.

This is a common pattern, but it creates a security invariant: Any user-controlled data that could contain â€˜lcâ€™ must be treated carefully. Otherwise, an attacker can craft a dict that â€œlooks likeâ€ an internal object and trick the deserializer into giving it meaning.

The patch makes the intent explicit in the updated documentation: during serialization, **plain dicts that contain an â€˜lcâ€™ key are escaped** by wrapping them.

This prevents those dicts from being confused with actual LangChain serialized objects during deserialization.

### **The allowlist: what can be instantiated**

LangChainâ€™s load()/loads() functions donâ€™t instantiate arbitrary classes â€“ they check against an allowlist that controls which classes can be deserialized. By default, this allowlist includes classes from langchain\_core, langchain\_openai, langchain\_aws, and other ecosystem packages.

Hereâ€™s the catch: most classes on the allowlist have harmless constructors. Finding exploitable paths required digging through the ecosystem for classes that do something meaningful on instantiation. The ones I found are detailed below, but there may be others waiting to be discovered.

### **The exfiltration path**

LangChainâ€™s loads() function supports a *secret* type that resolves values from environment variables during deserialization. Before the patch, this feature *secrets\_from\_env* was enabled by default:

```
if (
Â Â Â Â value.get("lc") == 1
Â Â Â Â and value.get("type") == "secret"
Â Â Â Â and value.get("id") is not None
):
Â Â Â Â [key] = value["id"]
Â Â Â Â if key in self.secrets_map:
Â Â Â Â Â Â Â Â return self.secrets_map[key]
Â Â Â Â if self.secrets_from_env and key in os.environ and os.environ[key]:
Â Â Â Â Â Â Â  return os.environ[key] # <-- Returning env variable
Â    return None
```

If a deserialized object is returned to an attacker, for example message history inside the LLM context, that could leak environment variables.

But the more interesting path is an indirect prompt injection.Â Even an attacker who cannot see any LLM responses can exfiltrate secrets by instantiating the right class. *ChatBedrockConverse* from *langchain\_aws* is both in the default allowlist of *loads* and makes a GET request on construction.
The GET endpoint is attacker-controlled, and a specific HTTP header can be populated with an environment variable via the *secrets\_from\_env* feature.

![](https://cyata.ai/wp-content/uploads/2025/12/image-1.png)

This validator runs when *ChatBedrockConverse* is instantiated. The attacker controls *endpoint\_url*, triggering an outbound request. Combined with *secrets\_from\_env*, the *aws\_access\_key\_id* header can be populated with any environment variable â€“ not just AWS keys.

We are intentionally not publishing a copy-paste exploit here, to give time for security teams. In a few months, the Huntr website will publish them automatically.

### **Code Execution via jinja2 templates**

Among the classes in the default *loads()* allowlist is *PromptTemplate*. This class creates a prompt from a template, and one of the available template formats is Jinja2.

When a template is rendered with Jinja2, arbitrary Python code can run. We did not find a way to trigger this directly from the *loads()* function alone, but if a subsequent call to the deserialized object triggers rendering, code execution follows.

We suspect there may be paths to direct code execution from *loads()*, but we have not confirmed one yet. If you have a solid idea or a lead worth testing, weâ€™d love to hear from you â€“ this is exactly where the security community helps turn hypotheses into proof. ğŸ¤

Also worth noting: in past versions, the Chain class was also in the allowlist. That class has special features that might have enabled a flow to template rendering.

## **Who is affected? The practical checklist**

Your application is potentially exposed if it uses vulnerable langchain-core versions. Here are some of the most common vulnerable patterns (12 flows have been identified in total):

* astream\_events(version=â€v1â€³) (v1 uses the vulnerable serialization; v2 is not vulnerable)
* Runnable.astream\_log()
* dumps() / dumpd() on untrusted data, followed by load() / loads()
* Deserializing untrusted data with load() / loads()
* Internal serialization flows like RunnableWithMessageHistory, InMemoryVectorStore.load(), certain caches, pulling manifests from LangChain Hub (hub.pull), and other listed components in the advisory

That said, the systemâ€™s behavior is complex enough that itâ€™s risky to assume a quick code-path review will catch every reachable variant. **The safer course is to upgrade to a patched version** and not assume youâ€™re in the clear until you do.

Also, the advisory calls out what I consider the most important real-world point:

The most common attack vector is through LLM response fields like additional\_kwargs or response\_metadata, which can be controlled via prompt injection and then serialized/deserialized in streaming operations.

This is exactly the kind of â€œAI meets classic securityâ€ intersection where organizations get caught off guard. LLM output is an untrusted input. If your framework treats portions of that output as structured objects later, you must assume attackers will try to shape it.

## **Defensive guidance: how to respond in production**

### **1) Patch first (this is the fastest risk reduction)**

Upgrade langchain-core to a patched version. If you are using langchain, langchain-community, or other ecosystem packages, validate what version of langchain-core is actually installed in production environments.

### **2) Assume LLM outputs can be attacker-shaped**

Treat additional\_kwargs, response\_metadata, tool outputs, retrieved documents, and message history as untrusted unless proven otherwise. This is especially important if you stream logs/events and later rehydrate them with a loader.

### **3) Review deserialization features like secret resolution**

Even after upgrading, keep the principle: do not enable secret resolution from environment variables unless you trust the serialized input. The project changed defaults for a reason.

## **The LangChainJS parallel**

Based on my report, there is a closely related advisory in LangChainJS (GHSA-r399-636x-v7f6 / CVE-2025-68665) with similar mechanics: â€˜lcâ€™ marker confusion during serialization, enabling secret extraction and unsafe instantiation in certain configurations.

If your organization runs both Python and JavaScript LangChain stacks, treat this as a reminder that **the pattern travels across ecosystems**: marker-based serialization, untrusted model output, and later deserialization is a recurring risk shape.

## **Why this matters beyond LangChain**

We are entering a phase where agentic AI frameworks are becoming critical infrastructure inside production systems. Serialization formats, orchestration pipelines, tool execution, caches, and tracing are no longer â€œplumbingâ€ â€“ they are part of your security boundary.

This vulnerability is not â€œjust a bug in a library.â€ It is a case study in a bigger pattern:

* Your application might deserialize data it believes it produced safely.
* But that serialized output can contain fields influenced by untrusted sources (including LLM outputs shaped by prompt injection).
* A single reserved key used as an internal marker can become a pivot point into secrets and execution-adjacent behaviors.

At Cyata, our work is to help organizations build **visibility, risk assessment, control, and governance** around AI systems â€“ because if you canâ€™t quickly answer *where agents are running, which versions are deployed, and what data flows through it*, youâ€™re effectively flying blind when advisories like this land.

## **What this teaches us about AI governance**

If you are a security leader reading this, here is the uncomfortable truth:

Most organizations cannot currently answer, quickly and confidently:

* Where are we using agents?
* Which versions are deployed in production?
* Which services have access to sensitive secrets?
* Where do LLM outputs cross those boundaries?

That is not a â€œdeveloper problem.â€ It is a visibility and governance problem.

And that is where Cyata comes in.

## **How Cyata helps: visibility, risk assessment, control, governance**

At Cyata, we focus on a practical outcome: reducing AI and agent risk without slowing down builders. Vulnerabilities like this one are rarely â€œjust a patch.â€ They expose gaps in how teams discover where agents run, understand real trust boundaries, and enforce safer defaults across fast-moving frameworks.

### **Visibility**

Know what is running, where, and how it is wired.

Answer the first CVE question quickly: **are we exposed, and in which flows?**

Discover agent runtimes and integrations across environments (IDEs, CI, services, worker jobs, hosted agents).

Track frameworks, packages, and versions in use.

### **Risk assessment**

Prioritize what matters based on real-world blast radius, not just â€œlibrary present.â€

Support faster triage: what is internet-facing, what touches secrets, what runs with elevated credentials.

Identify the highest-risk paths: untrusted content flowing into privileged contexts (services with secrets, broad tool permissions, prod network access).

Highlight where â€œstructured fieldsâ€ can cross trust boundaries (metadata, tool outputs, streamed events, cached artifacts).

### **Control**

Reduce exposure even before every dependency is patched everywhere.

Encourage safer operational defaults: least privilege, isolation boundaries, and policy checks that scale across teams.

Enforce guardrails around risky patterns (for example: deserializing untrusted data, permissive object revival, unsafe streaming-to-cache-to-rehydrate flows).

Gate or restrict sensitive capabilities in untrusted contexts (for example: secret access via environment, high-privilege tool execution, or running risky code paths in privileged workers).

### **Governance**

Make â€œsafe agent usageâ€ repeatable, auditable, and hard to drift.

* Define policies for **approved frameworks, versions, and configurations**.
* Track and time-box exceptions with owners and rationale.
* Monitor for drift and risky feature usage over time, with an audit trail that supports security reviews and compliance.

When a Christmas-week advisory drops, the goal is not heroics â€“ itâ€™s calm, controlled response backed by real inventory and enforced guardrails.

## **Disclosure Timeline**

Report submitted via Huntr â€“ December 4th, 2025

Acknowledged by LangChain maintainers â€“ December 5th, 2025

Advisory and CVE published â€“ December 24th, 2025

The Control Plane â€¨for Agentic Identity

[Book a Demo](/book-a-demo)

[Subscribe](#newsletter)

### More insights

[Blog](https://cyata.ai/blog/)

[![](https://cyata.ai/wp-content/uploads/2025/12/cyata-cursor-1024x629.webp)](https://cyata.ai/blog/cyata-research-critical-flaw-in-cursor-mcp-installation/)

Dec 19, 2025
â€¢
2 min read

[### Cyata Research: Critical Flaw in Cursor MCP Installation](https://cyata.ai/blog/cyata-research-critical-flaw-in-cursor-mcp-installation/)

As originally published atÂ SiliconANGLE, a new report out today from artificial intelligenceâ€¦

![](https://cyata.ai/wp-content/uploads/2025/12/duncan-riley.png)

Written by
Duncan Riley

[Read more](https://cyata.ai/blog/cyata-research-critical-flaw-in-cursor-mcp-installation/)

[![](https://cyata.ai/wp-content/uploads/2025/12/speed-kills-why-your-ai-agents-are-exposing-iam-vulnerabilities-cover-1024x629.webp)](https://cyata.ai/blog/speed-kills-why-your-ai-agents-are-exposing-iam-vulnerabilities/)

Dec 18, 2025
â€¢
5 min read

[### Speed Kills: Why Your AI Agents Are Exposing IAM Vulnerabilities](https://cyata.ai/blog/speed-kills-why-your-ai-agents-are-exposing-iam-vulnerabilities/)

Security and IAM teams have spent the last decade building real-time Identityâ€¦

![](https://cyata.ai/wp-content/uploads/2025/12/Sushant-Chowdhary-300x300.webp)

Written by
Sushant Chowdhary, Engineering Manager at Ascension

[Read more](https://cyata.ai/blog/speed-kills-why-your-ai-agents-are-exposing-iam-vulnerabilities/)

[![](https://cyata.ai/wp-content/uploads/2025/11/google-antigravity-1024x629.webp)](https://cyata.ai/blog/google-antigravity-lands-cyata-already-has-it-on-the-map/)

Nov 19, 2025
â€¢
9 min read

[### Google Antigravity Lands. Cyata Already Has It On The Map.](https://cyata.ai/blog/google-antigravity-lands-cyata-already-has-it-on-the-map/)

Supported now in Cyata for discovery and active posture management. Yes, itâ€¦

![](https://cyata.ai/wp-content/uploads/2025/07/Avatar.svg)

Written by
Cyata Research Team

[Read more](https://cyata.ai/blog/google-antigravity-lands-cyata-already-has-it-on-the-map/)

![](https://cyata.ai/wp-content/themes/cyata/assets/build/images/shape-mobile.svg)

![](https://cyata.ai/wp-content/themes/cyata/assets/build/images/shape-desktop.svg)

Sign up for Cyataâ€™s Newsletter

Get early access, research, and updates from the leaders in Agentic Identity.

By submitting, you agree to ourÂ [Privacy Policy](https://cyata.ai/privacy-policy/)

[![](https://cyata.ai/wp-content/uploads/2025/06/cyata.svg)](https://cyata.ai/)

The Control Plane for
 Agentic Identity

[![](https://cyata.ai/wp-content/uploads/2025/07/linkedin.svg)](https://www.linkedin.com/company/cyata/)
[![](https://cyata.ai/wp-content/uploads/2025/07/x.svg)](https://x.com/TeamCyata)

![](https://cyata.ai/wp-content/uploads/2025/07/soc_2.svg)

![](https://cyata.ai/wp-content/uploads/2025/08/nvidia-inception-program-badge-rgb-1c-blk-for-screen-1.svg)

![](https://cyata.ai/wp-content/uploads/2025/10/OWASP_Tier.svg)

![](https://cyata.ai/wp-content/uploads/2025/10/logo-1.svg)

Explore

* [Blog](https://cyata.ai/blog/)

Company

* [About Us](https://cyata.ai/about-us/)
* [Partners](https://cyata.ai/partners/)
* [Careers](https://cyata.ai/careers/)
* [Book a Demo](https://cyata.ai/book-a-demo/)
* [Contact](https://cyata.ai/contact/)

![](https://cyata.ai/wp-content/uploads/2025/07/soc_2.svg)

![](https://cyata.ai/wp-content/uploads/2025/08/nvidia-inception-program-badge-rgb-1c-blk-for-screen-1.svg)

![](https://cyata.ai/wp-content/uploads/2025/10/OWASP_Tier.svg)

![](https://cyata.ai/wp-content/uploads/2025/10/logo-1.svg)

Â© 2025 Cyata. All rights reserved.

* [Privacy Policy](https://cyata.ai/privacy-policy/)
* [Terms of Use](https://cyata.ai/terms-of-use/)

![]()

![]()
